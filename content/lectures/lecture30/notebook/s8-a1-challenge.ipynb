{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import the necessary libraries\n",
                "import tensorflow as tf\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.model_selection import train_test_split\n",
                "import pandas as pd\n",
                "%matplotlib inline\n",
                "\n",
                "tf.keras.backend.clear_session()  # For easy reset of notebook state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read the file 'IRIS.csv'\n",
                "\n",
                "df = pd.read_csv('IRIS.csv')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Take a quick look at the dataset\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We use one-hot-encoding to encode the 'species' labels using pd.get_dummies\n",
                "\n",
                "one_hot_df = pd.get_dummies(df[___], prefix='species')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The predictor variables are all columns except species\n",
                "X = df.drop([___],axis=1).values\n",
                "\n",
                "# The response variable is the one-hot-encoded species values\n",
                "y = one_hot_df.values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We divide our data into test and train sets with 80% training size\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(___,___,train_size=___)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# To build the MLP, we will use the keras library\n",
                "\n",
                "model = tf.keras.models.Sequential(name='MLP')\n",
                "\n",
                "# To initialise our model we set some parameters \n",
                "# commonly defined in an MLP design\n",
                "\n",
                "# The number of nodes in a hidden layer\n",
                "n_hidden = ___\n",
                "\n",
                "# The number of nodes in the input layer (features)\n",
                "n_input = ___\n",
                "\n",
                "# The number of nodes in the output layer\n",
                "n_output = ___"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We add the first hidden layer with `n_hidden` number of neurons \n",
                "# and 'relu' activation\n",
                "model.add((tf.keras.layers.Dense(n_hidden,input_dim=___, activation = ___,name='hidden')))\n",
                "\n",
                "# The second layer is the final layer in our case, using 'softmax' on the output labels\n",
                "model.add(tf.keras.layers.Dense(n_output, activation = ___,name='output'))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Now we compile the model using 'categorical_crossentropy' loss, \n",
                "# optimizer as 'sgd' and 'accuracy' as a metric\n",
                "model.compile(optimizer=___,\n",
                "              loss=___,\n",
                "              metrics=[___])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# You can see an overview of the model you built using .summary()\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We fit the model, and save it to a variable 'history' that can be \n",
                "# accessed later to analyze the training profile\n",
                "# We also set validation_split=0.2 for 20% of training data to be \n",
                "# used for validation\n",
                "# verbose=0 means you will not see the output after every epoch. \n",
                "# Set verbose=1 to see it\n",
                "\n",
                "history = model.fit(___,___, epochs = ___, batch_size = 16,verbose=0,validation_split=___)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Here we plot the training and validation loss and accuracy\n",
                "\n",
                "fig, ax = plt.subplots(1,2,figsize = (16,4))\n",
                "ax[0].plot(history.history['loss'],'r',label = 'Training Loss')\n",
                "ax[0].plot(history.history['val_loss'],'b',label = 'Validation Loss')\n",
                "ax[1].plot(history.history['accuracy'],'r',label = 'Training Accuracy')\n",
                "ax[1].plot(history.history['val_accuracy'],'b',label = 'Validation Accuracy')\n",
                "ax[0].legend()\n",
                "ax[1].legend()\n",
                "ax[0].set_xlabel('Epochs')\n",
                "ax[1].set_xlabel('Epochs');\n",
                "ax[0].set_ylabel('Loss')\n",
                "ax[1].set_ylabel('Accuracy %');\n",
                "fig.suptitle('MLP Training', fontsize = 24)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_accuracy) ###\n",
                "# Once you have near-perfect validation accuracy, time to evaluate model performance on test set \n",
                "\n",
                "train_accuracy = model.evaluate(___,___)[1]\n",
                "test_accuracy = model.evaluate(___,___)[1]\n",
                "print(f'The training set accuracy for the model is {train_accuracy}\\\n",
                "    \\n The test set accuracy for the model is {test_accuracy}')"
            ]
        }
    ]
}
