{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "**Exercise: A.1- MLP using Keras**\n",
    "\n",
    "# Description\n",
    "\n",
    "The aim of this exercise is to come up with a simple Multi-layer perceptron classifier using tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/image.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used here is the Iris dataset, same as the one used for logistic regression classification. This dataset has several features such as sepal length, sepal width and so on, to predict which of the iris species that particular flower belongs to here. \n",
    "\n",
    "# Instructions:\n",
    "1. Read the csv file as a pandas dataframe.\n",
    "2. Assign the dependent and independent variables. The species is your dependent variable. All the other columns are your predictors.\n",
    "3. Split the dataset into train and validation sets.\n",
    "4. Define the network parameters for the MLP.\n",
    "5. Initialise the weights and biases of the network.\n",
    "6. Define the MLP model with input, hidden and output layers.\n",
    "7. Fit the model on the training data.\n",
    "8. Compute and print the train and validation accuracy.\n",
    "9. Compare the output of the MLP classifier with that of the logistic model you had earlier fit in `Session 5 Exercise B.1`. Find out which model performs better and why it does so?\n",
    "\n",
    "# Hints:\n",
    "\n",
    "\n",
    "<a href=\"https://keras.io/api/layers/merging_layers/add/\" target=\"_blank\">keras.add()</a> : To add a layer to the model\n",
    "\n",
    "\n",
    "<a href=\"https://keras.io/api/models/sequential/\" target=\"_blank\">keras.fit()</a> : Fit the model for the data\n",
    "\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/Model\" target=\"_blank\">model.evaluate()</a> : Evaluate model performance on predictors vs true values\n",
    "\n",
    "Note: This exercise is **auto-graded and you can try multiple attempts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file 'IRIS.csv'\n",
    "\n",
    "df = pd.read_csv('IRIS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a quick look at the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use one-hot-encoding to encode the 'species' labels using pd.get_dummies\n",
    "\n",
    "one_hot_df = pd.get_dummies(df[___], prefix='species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predictor variables are all columns except species\n",
    "X = df.drop([___],axis=1).values\n",
    "\n",
    "# The response variable is the one-hot-encoded species values\n",
    "y = one_hot_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We divide our data into test and train sets with 80% training size\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(___,___,train_size=___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build the MLP, we will use the keras library\n",
    "\n",
    "model = tf.keras.models.Sequential(name='MLP')\n",
    "\n",
    "# To initialise our model we set some parameters \n",
    "# commonly defined in an MLP design\n",
    "\n",
    "# The number of nodes in a hidden layer\n",
    "n_hidden = ___\n",
    "\n",
    "# The number of nodes in the input layer (features)\n",
    "n_input = ___\n",
    "\n",
    "# The number of nodes in the output layer\n",
    "n_output = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the first hidden layer with `n_hidden` number of neurons \n",
    "# and 'relu' activation\n",
    "model.add((tf.keras.layers.Dense(n_hidden,input_dim=___, activation = ___,name='hidden')))\n",
    "\n",
    "# The second layer is the final layer in our case, using 'softmax' on the output labels\n",
    "model.add(tf.keras.layers.Dense(n_output, activation = ___,name='output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we compile the model using 'categorical_crossentropy' loss, \n",
    "# optimizer as 'sgd' and 'accuracy' as a metric\n",
    "model.compile(optimizer=___,\n",
    "              loss=___,\n",
    "              metrics=[___])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see an overview of the model you built using .summary()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit the model, and save it to a variable 'history' that can be \n",
    "# accessed later to analyze the training profile\n",
    "# We also set validation_split=0.2 for 20% of training data to be \n",
    "# used for validation\n",
    "# verbose=0 means you will not see the output after every epoch. \n",
    "# Set verbose=1 to see it\n",
    "\n",
    "history = model.fit(___,___, epochs = ___, batch_size = 16,verbose=0,validation_split=___)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we plot the training and validation loss and accuracy\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize = (16,4))\n",
    "ax[0].plot(history.history['loss'],'r',label = 'Training Loss')\n",
    "ax[0].plot(history.history['val_loss'],'b',label = 'Validation Loss')\n",
    "ax[1].plot(history.history['accuracy'],'r',label = 'Training Accuracy')\n",
    "ax[1].plot(history.history['val_accuracy'],'b',label = 'Validation Accuracy')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[1].set_xlabel('Epochs');\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[1].set_ylabel('Accuracy %');\n",
    "fig.suptitle('MLP Training', fontsize = 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_accuracy) ###\n",
    "# Once you have near-perfect validation accuracy, time to evaluate model performance on test set \n",
    "\n",
    "train_accuracy = model.evaluate(___,___)[1]\n",
    "test_accuracy = model.evaluate(___,___)[1]\n",
    "print(f'The training set accuracy for the model is {train_accuracy}\\\n",
    "    \\n The test set accuracy for the model is {test_accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
