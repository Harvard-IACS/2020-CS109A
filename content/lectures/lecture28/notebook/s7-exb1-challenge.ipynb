{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Import library\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "from sklearn.datasets import load_iris\n",
                "from tensorflow.keras.utils import to_categorical\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "X shape: (150, 4)\ny shape: (150,)\n"
                }
            ],
            "source": [
                "#Load the iris data\n",
                "iris_data = load_iris()\n",
                "\n",
                "#Get the predictor and reponse variables\n",
                "X = iris_data.data\n",
                "y = iris_data.target\n",
                "\n",
                "#See the shape of the data\n",
                "print(f'X shape: {X.shape}')\n",
                "print(f'y shape: {y.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Y shape: (150, 3)\n"
                }
            ],
            "source": [
                "#One-hot encode target labels\n",
                "Y = to_categorical(y)\n",
                "print(f'Y shape: {Y.shape}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Load and inspect the pre-trained weights and biases. Compare their shapes to the NN diagram."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Load and inspect the pre-trained weights and biases\n",
                "weights = np.load('data/weights.npy', allow_pickle=True)\n",
                "\n",
                "# weights for hidden (1st) layer\n",
                "w1 = weights[0] \n",
                "\n",
                "# biases for hidden (1st) layer\n",
                "b1 = weights[1]\n",
                "\n",
                "# weights for output (2nd) layer\n",
                "w2 = weights[2]\n",
                "\n",
                "#biases for output (2nd) layer\n",
                "b2 = weights[3] "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "w1 - shape: (4, 3)\n[[-0.42714605 -0.72814226  0.37730372]\n [ 0.39002347 -0.73936987  0.7850246 ]\n [ 0.12336338 -0.7267647  -0.48210236]\n [ 0.20957732 -0.7505736  -1.3789996 ]]\n\nb1 - shape: (3,)\n[0.         0.         0.31270522]\n\nw2 - shape: (3, 3)\n[[ 0.7043929   0.13273811 -0.845736  ]\n [-0.8318007  -0.6977086   0.75894   ]\n [ 1.1978723   0.14868832 -0.473792  ]]\n\nb2 - shape: (3,)\n[-1.2774311   0.45491916  0.73040146]\n\n"
                }
            ],
            "source": [
                "#Compare their shapes to that in the NN diagram.\n",
                "for arr, name in zip([w1,b1,w2,b2], ['w1','b1','w2','b2']):\n",
                "    print(f'{name} - shape: {arr.shape}')\n",
                "    print(arr)\n",
                "    print()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For the first affine transformation we need to multiple the augmented input by the first weight matrix (i.e., layer).\n",
                "\n",
                "$$\n",
                "\\begin{bmatrix}\n",
                "1 \u0026 X_{11} \u0026 X_{12} \u0026 X_{13} \u0026 X_{14}\\\\\n",
                "1 \u0026 X_{21} \u0026 X_{22} \u0026 X_{23} \u0026 X_{24}\\\\\n",
                "\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \\\\\n",
                "1 \u0026 X_{n1} \u0026 X_{n2} \u0026 X_{n3} \u0026 X_{n4}\\\\\n",
                "\\end{bmatrix} \\begin{bmatrix}\n",
                "b_{1}^1 \u0026 b_{2}^1 \u0026 b_{3}^1\\\\\n",
                "W_{11}^1 \u0026 W_{12}^1 \u0026 W_{13}^1\\\\\n",
                "W_{21}^1 \u0026 W_{22}^1 \u0026 W_{23}^1\\\\\n",
                "W_{31}^1 \u0026 W_{32}^1 \u0026 W_{33}^1\\\\\n",
                "W_{41}^1 \u0026 W_{42}^1 \u0026 W_{43}^1\\\\\n",
                "\\end{bmatrix} =\n",
                "\\begin{bmatrix}\n",
                "z_{11}^1 \u0026 z_{12}^1 \u0026 z_{13}^1\\\\\n",
                "z_{21}^1 \u0026 z_{22}^1 \u0026 z_{23}^1\\\\\n",
                "\\vdots \u0026 \\vdots \u0026 \\vdots \\\\\n",
                "z_{n1}^1 \u0026 z_{n2}^1 \u0026 z_{n3}^1\\\\\n",
                "\\end{bmatrix}\n",
                "= \\textbf{Z}^{(1)}\n",
                "$$ \n",
                "\u003cspan style='color:gray'\u003eAbout the notation: superscript refers to the layer and subscript refers to the index in the particular matrix. So $W_{23}^1$ is the weight in the 1st layer connecting the 2nd input to 3rd hidden node. Compare this matrix representation to the slide image. Also note the bias terms and ones that have been added to 'augment' certain matrices. You could consider $b_1^1$ to be $W_{01}^1$.\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\n",
                "\u003cspan style='color:blue'\u003e1. Augment X with a column of ones to create `X_aug`\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\u003cspan style='color:blue'\u003e2. Create the first layer weight matrix `W1` by vertically stacking the bias vector `b1`on top of `w1` (consult `add_ones_col` for ideas. Don't forget your `Tab` and `Shift+Tab` tricks!)\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\u003cspan style='color:blue'\u003e3. Do the matrix multiplication to find `Z1`\u003c/span\u003e\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_ones_col(X):\n",
                "    '''Augment matrix with a column of ones'''\n",
                "    X_aug = np.hstack((np.ones((X.shape[0],1)), X))\n",
                "    return X_aug"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Use add_ones_col()\n",
                "X_aug = add_ones_col(___)\n",
                "\n",
                "#Use np.vstack to add biases to weight matrix\n",
                "W1 = np.vstack((___,___))\n",
                "\n",
                "#Use np.dot() to multiple X_aug and W1\n",
                "Z1 = np.dot(___,___) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we use our non-linearity\n",
                "$$\n",
                "\\textit{a}_{\\text{relu}}(\\textbf{Z}^{(1)})=\n",
                "\\begin{bmatrix}\n",
                "h_{11} \u0026 h_{12} \u0026 h_{13}\\\\\n",
                "h_{21} \u0026 h_{22} \u0026 h_{23}\\\\\n",
                "\\vdots \u0026 \\vdots \u0026 \\vdots \\\\\n",
                "h_{n1} \u0026 h_{n2} \u0026 h_{n3}\\\\\n",
                "\\end{bmatrix}= \\textbf{H}\n",
                "$$\n",
                "\n",
                "\n",
                "\n",
                "\u003cspan style='color:blue'\u003e1. Define the ReLU activation\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\n",
                "\u003cspan style='color:blue'\u003e2. use `plot_activation_func` to confirm implementation\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\n",
                "\u003cspan style='color:blue'\u003e3. Use relu on `Z1` to create `H`\u003c/span\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def relu(z: np.array) -\u003e np.array:\n",
                "    # hint: \n",
                "    #       relu(z) = 0 when z \u003c 0\n",
                "    #       otherwise relu(z) = z\n",
                "    # your code here\n",
                "    h = np.maximum(___,___) # np.maximum() will help\n",
                "    return h"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Helper code to plot the activation function\n",
                "def plot_activation_func(f, name):\n",
                "    lin_x = np.linspace(-10,10,200)\n",
                "    h = f(lin_x)\n",
                "    plt.plot(lin_x, h)\n",
                "    plt.xlabel('x')\n",
                "    plt.ylabel('y')\n",
                "    plt.title(f'{name} Activation Function')\n",
                "\n",
                "plot_activation_func(relu, name='RELU')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# use your relu activation function on Z1\n",
                "H = relu(___) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The next step is very similar to the first and so we've filled it in for you.\n",
                "\n",
                "$$\n",
                "\\begin{bmatrix}\n",
                "1 \u0026 h_{11} \u0026 h_{12} \u0026 h_{13}\\\\\n",
                "1 \u0026 h_{21} \u0026 h_{22} \u0026 h_{23}\\\\\n",
                "\\vdots \u0026 \\vdots \u0026 \\vdots \u0026 \\vdots \\\\\n",
                "1 \u0026 h_{n1} \u0026 h_{n2} \u0026 h_{n3}\\\\\n",
                "\\end{bmatrix}\n",
                "\\begin{bmatrix}\n",
                "b_{1}^{(2)} \u0026 b_{2}^2 \u0026 b_{3}^2\\\\\n",
                "W_{11}^2 \u0026 W_{12}^2 \u0026 W_{13}^2\\\\\n",
                "W_{21}^2 \u0026 W_{22}^2 \u0026 W_{23}^2\\\\\n",
                "W_{31}^2 \u0026 W_{32}^2 \u0026 W_{33}^2\\\\\n",
                "\\end{bmatrix}=\n",
                "\\begin{bmatrix}\n",
                "z_{11}^2 \u0026 z_{12}^2 \u0026 z_{13}^2\\\\\n",
                "z_{21}^2 \u0026 z_{22}^2 \u0026 z_{23}^2\\\\\n",
                "\\vdots \u0026 \\vdots \u0026 \\vdots \\\\\n",
                "z_{n1}^2 \u0026 z_{n2}^2 \u0026 z_{n3}^2\\\\\n",
                "\\end{bmatrix} = \\textbf{Z}^{(2)}\n",
                "$$\n",
                "\n",
                "\n",
                "\u003cspan style='color:blue'\u003e1. Augment `H` with ones to create `H_aug`\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\n",
                "\u003cspan style='color:blue'\u003e2. Combine `w2` and `b2` to create the output weight matric `W2`\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\n",
                "\u003cspan style='color:blue'\u003e3. Perform the matrix multiplication to produce `Z2`\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Use add_ones_col()\n",
                "H_aug = ___\n",
                "\n",
                "#Use np.vstack to add biases to weight matrix\n",
                "W2 = ___\n",
                "\n",
                "#Use np.dot()\n",
                "Z2 = np.dot(H_aug,W2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally we use the softmax activation on `Z2`. Now for each observation we have an output vector of length 3 which can be interpreted as a probability (they sum to 1).\n",
                "$$\n",
                "\\textit{a}_{\\text{softmax}}(\\textbf{Z}^2)=\n",
                "\\begin{bmatrix}\n",
                "\\hat{y}_{11} \u0026 \\hat{y}_{12} \u0026 \\hat{y}_{13}\\\\\n",
                "\\hat{y}_{21} \u0026 \\hat{y}_{22} \u0026 \\hat{y}_{23}\\\\\n",
                "\\vdots \u0026 \\vdots \u0026 \\vdots \\\\\n",
                "\\hat{y}_{n1} \u0026 \\hat{y}_{n2} \u0026 \\hat{y}_{n3}\\\\\n",
                "\\end{bmatrix}\n",
                "= \\hat{\\textbf{Y}}\n",
                "$$\n",
                "\n",
                "\u003cspan style='color:blue'\u003e1. Define softmax\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e\n",
                "\u003cspan style='color:blue'\u003e2. Use `softmax` on `Z2` to create `Y_hat`\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def softmax(z: np.array) -\u003e np.array:\n",
                "    '''\n",
                "    Input: z - 2D numpy array of logits \n",
                "           rows are observations, classes are columns \n",
                "    Returns: y_hat - 2D numpy array of probabilities\n",
                "             rows are observations, classes are columns \n",
                "    '''\n",
                "    # hint: we are summing across the columns\n",
                "\n",
                "    y_hat = np.exp(___)/np.sum(np.exp(___), axis=___, keepdims=True)\n",
                "    return y_hat"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Calling the softmax function\n",
                "Y_hat = softmax(___)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cspan style='color:blue'\u003eNow let's see how accuract the model's predictions are! Use `np.argmax` to collapse the columns of `Y_hat` to create `y_hat`, a vector of class labels like the original `y` before one-hot encoding.\u003c/span\u003e\u003cdiv\u003e\u003c/div\u003e"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_acc) ###\n",
                "\n",
                "# Compute the accuracy\n",
                "y_hat = np.argmax(___, axis=___)\n",
                "acc = sum(y == y_hat)/len(y)\n",
                "print(f'accuracy: {acc:.2%}')"
            ]
        }
    ]
}
