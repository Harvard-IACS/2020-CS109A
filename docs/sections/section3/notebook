{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "## Standard Section 3: Multiple Linear Regression and Polynomial Regression \n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2020**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Tanner<br/>\n",
    "**Section Leaders**: Marios Mattheakis, Sean Murphy, Yinyu Ji<br/>\n",
    "\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, our goal is to get you familiarized with Multiple Linear Regression. We have learned how to model data with kNN Regression and Simple Linear Regression and our goal now is to dive deep into Linear Regression.\n",
    "\n",
    "Specifically, we will: \n",
    "    \n",
    "- Load in the titanic dataset from seaborn\n",
    "- Learn a few ways to **visualize distributions** of variables using seaborn\n",
    "- Practice single variable OLS and how to **interpret coefficients** in linear regression\n",
    "- Practice multiple linear regression with **interaction** terms and **polynomial** regression terms\n",
    "- Learn about **bootstrapping** to generate confidence intervals\n",
    "- Understand the **assumptions** being made in a linear regression model\n",
    "- (Bonus 1): look at some cool plots to raise your exploratory data analysis (EDA) game\n",
    "- (Bonus 2): look at some example stats models code that produces equivalent results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme](../fig/meme.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data and Stats packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "# Modeling\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending Linear Regression\n",
    "\n",
    "## Working with the Titanic Dataset from Seaborn\n",
    "\n",
    "For our dataset, we'll be using the passenger list from the Titanic, which famously sank in 1912. Let's have a look at the data. Some descriptions of the data are at https://www.kaggle.com/c/titanic/data, and here's [how seaborn preprocessed it](https://github.com/mwaskom/seaborn-data/blob/master/process/titanic.py).\n",
    "\n",
    "The task is to build a regression model to **predict the fare**, based on different attributes.\n",
    "\n",
    "Let's keep a subset of the data, which includes the following variables: \n",
    "\n",
    "- age\n",
    "- sex\n",
    "- class\n",
    "- embark_town\n",
    "- alone\n",
    "- **fare** (the response variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from seaborn \n",
    "titanic = sns.load_dataset(\"titanic\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for null values\n",
    "chosen_vars = ['age', 'sex', 'class', 'embark_town', 'alone', 'fare']\n",
    "titanic = titanic[chosen_vars]\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the datatypes of each column and display the statistics (min, max, mean and any others) for all the numerical columns of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(titanic.dtypes)\n",
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all the non-null *rows* in the dataset. Is this always a good idea?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.dropna(axis=0)\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us visualize the response variable. A good visualization of the distribution of a variable will enable us to answer three kinds of questions:\n",
    "\n",
    "- What values are central or typical? (e.g., mean, median, modes)\n",
    "- What is the typical spread of values around those central values? (e.g., variance/stdev, skewness)\n",
    "- What are unusual or exceptional values (e.g., outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(24, 6))\n",
    "ax = ax.ravel()\n",
    "\n",
    "sns.distplot(titanic['fare'], ax=ax[0])\n",
    "ax[0].set_title('Seaborn distplot')\n",
    "ax[0].set_ylabel('Normalized frequencies')\n",
    "\n",
    "sns.violinplot(x='fare', data=titanic, ax=ax[1])\n",
    "ax[1].set_title('Seaborn violin plot')\n",
    "ax[1].set_ylabel('Frequencies')\n",
    "\n",
    "sns.boxplot(x='fare', data=titanic, ax=ax[2])\n",
    "ax[2].set_title('Seaborn box plot')\n",
    "ax[2].set_ylabel('Frequencies')\n",
    "fig.suptitle('Distribution of count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret these plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's split the data into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train, titanic_test = train_test_split(titanic, train_size=0.7, random_state=42)\n",
    "# important for avoiding the infamous SettingwithCopyWarning\n",
    "titanic_train = titanic_train.copy()\n",
    "titanic_test = titanic_test.copy()\n",
    "print(titanic_train.shape, titanic_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple One-Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's fit a simple model on the training data using LinearRegression from the sklearn library, predicting **fare** using **age**.\n",
    "\n",
    "We'll call this `model_1` and take a look at its coefficient and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign predictor and response variables\n",
    "X_train = titanic_train[['age']]\n",
    "y_train = titanic_train['fare']\n",
    "X_test = titanic_test[['age']]\n",
    "y_test = titanic_test['fare']\n",
    "\n",
    "model_1 = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the slope/coefficient and intercept values\n",
    "print(\"Coefficient of the model: \",model_1.coef_)\n",
    "print(\"Intercept of the model: \",model_1.intercept_)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = model_1.predict(X_test)\n",
    "\n",
    "# get R-squared\n",
    "print(\"R-squared of the model:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, you should be able to distinguish between three kinds of variables: \n",
    "\n",
    "1. Continuous variables: such as `fare` or `age`\n",
    "2. Categorical variables: such as `sex` or `alone`. There is no inherent ordering between the different values that these variables can take on. These are sometimes called nominal variables. Read more [here](https://stats.idre.ucla.edu/other/mult-pkg/whatstat/what-is-the-difference-between-categorical-ordinal-and-interval-variables/). \n",
    "3. Ordinal variables: such as `class` (first > second > third). There is some inherent ordering of the values in the variables, but the values are not continuous either. \n",
    "\n",
    "*Note*: While there is some inherent ordering in `class`, we will be treating it like a categorical variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now examine the `sex` column and see the value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a column `sex_male` that is 1 if the passenger is male, 0 if female. The value counts indicate that these are the two options in this particular dataset. Ensure that the datatype is `int`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train['sex_male'] = (titanic_train.sex == 'male').astype(int)\n",
    "titanic_train['sex_male'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we need a `sex_female` column, or a `sex_others` column? Why or why not?\n",
    "\n",
    "Now, let us look at `class` in greater detail. Let's one hot encode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train['class_Second'] = (titanic_train['class'] == 'Second').astype(int)\n",
    "titanic_train['class_Third'] = 1 * (titanic_train['class'] == 'Third') # just another way to do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for the test set\n",
    "titanic_test_orig = titanic_test.copy()\n",
    "\n",
    "titanic_test = pd.get_dummies(titanic_test, columns=['sex', 'class'], drop_first=True)\n",
    "titanic_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with binary predictors\n",
    "\n",
    "![male_female](../fig/male_female.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with More Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's fit a linear regression including the new sex and class variables, calling this model `model_2`.\n",
    "\n",
    "![image.png](../fig/multiple.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign predictor and response variables\n",
    "X_train = titanic_train[['age', 'sex_male', 'class_Second', 'class_Third']]\n",
    "y_train = titanic_train['fare']\n",
    "X_test = titanic_test[['age', 'sex_male', 'class_Second', 'class_Third']]\n",
    "y_test = titanic_test['fare']\n",
    "\n",
    "model_2 = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the coefficients and intercept values\n",
    "print(\"Coefficients of the model: \",model_2.coef_)\n",
    "print(\"Intercept of the model: \",model_2.intercept_)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = model_2.predict(X_test)\n",
    "\n",
    "# get R-squared\n",
    "print(\"R-squared of the model:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting These Results\n",
    "\n",
    "1. Which of the predictors do you think are important? Why?\n",
    "2. All else equal, what does being male do to the fare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering Variables: Exploring Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(y = \"fare\", x= \"age\", hue = \"sex_male\", data = titanic_train, height=7.27, aspect=11.7/8.27)\n",
    "ax = plt.gca()\n",
    "ax.set_title(\"Predicting Fare from Age + Age*Sex\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slopes seem to be different for male and female. What does that indicate?\n",
    "\n",
    "Let us now try to add an interaction effect into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seemed like sex interacted with age. Can we put that in our model?\n",
    "titanic_train['sex_male_X_age'] = titanic_train['age'] * titanic_train['sex_male']\n",
    "titanic_test['sex_male_X_age'] = titanic_test['age'] * titanic_test['sex_male']\n",
    "\n",
    "X_train = titanic_train[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age']]\n",
    "y_train = titanic_train['fare']\n",
    "X_test = titanic_test[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age']]\n",
    "y_test = titanic_test['fare']\n",
    "\n",
    "model_3 = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the coefficients and intercept values\n",
    "print(\"Coefficients of the model: \",model_3.coef_)\n",
    "print(\"Intercept of the model: \",model_3.intercept_)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = model_3.predict(X_test)\n",
    "\n",
    "# get R-squared\n",
    "print(\"R-squared of the model:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seemed like sex interacted with class. Can we put that in our model?\n",
    "titanic_train['sex_male_X_class_Second'] = titanic_train['age'] * titanic_train['class_Second']\n",
    "titanic_train['sex_male_X_class_Third'] = titanic_train['age'] * titanic_train['class_Third']\n",
    "titanic_test['sex_male_X_class_Second'] = titanic_test['age'] * titanic_test['class_Second']\n",
    "titanic_test['sex_male_X_class_Third'] = titanic_test['age'] * titanic_test['class_Third']\n",
    "\n",
    "X_train = titanic_train[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age', \n",
    "                         'sex_male_X_class_Second', 'sex_male_X_class_Third']]\n",
    "y_train = titanic_train['fare']\n",
    "X_test = titanic_test[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age',\n",
    "                      'sex_male_X_class_Second', 'sex_male_X_class_Third']]\n",
    "y_test = titanic_test['fare']\n",
    "\n",
    "model_4 = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the coefficients and intercept values\n",
    "print(\"Coefficients of the model: \",model_4.coef_)\n",
    "print(\"Intercept of the model: \",model_4.intercept_)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = model_4.predict(X_test)\n",
    "\n",
    "# get R-squared\n",
    "print(\"R-squared of the model:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happened to the `age` and `male` terms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Breakout Room 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boston Housing Prices Dataset**\n",
    "\n",
    "Data Description:\n",
    "- RM: Average number of rooms per dwelling\n",
    "- LSTAT: Percent of housing population classified as \"lower status\"\n",
    "- MEDV: Median value of owner-occupied homes in $1000's\n",
    "\n",
    "MEDV will be the response variable\n",
    "\n",
    "(For the curious: https://www.kaggle.com/c/boston-housing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv('../data/boston_housing.csv')\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect data by visualization (we can combine matplotlib with seaborn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16,4])\n",
    "plt.subplot(1,3,1)\n",
    "sns.scatterplot(x=\"RM\", y=\"MEDV\", data=boston)\n",
    "plt.subplot(1,3,2)\n",
    "sns.scatterplot(x=\"LSTAT\", y=\"MEDV\", data=boston)\n",
    "plt.subplot(1,3,3)\n",
    "sns.scatterplot(x=\"RM\", y=\"LSTAT\", data=boston);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data and perform single variable linear regression with the predictors RM and LSTAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_train, boston_test = train_test_split(boston, train_size=0.7, random_state=109)\n",
    "boston_train = boston_train.copy()\n",
    "boston_test = boston_test.copy()\n",
    "\n",
    "# Single variable linear regression with RM\n",
    "model_boston_0a = LinearRegression().fit(boston_train[[\"RM\"]], boston_train.MEDV)\n",
    "print(\"R^2 on training set of the model with RM:\",  r2_score(boston_train.MEDV, model_boston_0a.predict(boston_train[[\"RM\"]])) )\n",
    "print(\"R^2 on testing  set of the model with RM:\",  r2_score(boston_test.MEDV, model_boston_0a.predict(boston_test[[\"RM\"]])) )\n",
    "print('')\n",
    "\n",
    "# Single variable linear regression with RM\n",
    "model_boston_0b = LinearRegression().fit(boston_train[[\"LSTAT\"]], boston_train.MEDV)\n",
    "print(\"R^2 on training set of the model with LSTAT:\",  r2_score(boston_train.MEDV, model_boston_0b.predict(boston_train[[\"LSTAT\"]])) )\n",
    "print(\"R^2 on testing set  of the model with LSTAT:\",  r2_score(boston_test.MEDV, model_boston_0b.predict(boston_test[[\"LSTAT\"]])) )\n",
    "\n",
    "# Store R2 on testing for later\n",
    "R2_0a = r2_score(boston_test.MEDV, model_boston_0a.predict(boston_test[[\"RM\"]])) \n",
    "R2_0b = r2_score(boston_test.MEDV, model_boston_0b.predict(boston_test[[\"LSTAT\"]])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Breakout Room 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Learn about how to include interaction terms in your linear regression model using the sklearn library.  \n",
    "\n",
    "**Directions:**\n",
    "    1. Build an linear regression model using `RM` and `LSTAT` as predictors.\n",
    "        - As before, fit the model on the training data.\n",
    "        - Print the coefficients.\n",
    "        - Report the R^2 score on the test data. Did the performance improve?\n",
    "        - Does the multiple regression model perform better than the two single regression models?\n",
    "    2. Build a model with `LSTAT`, `RM`, and an interaction term between `LSTAT` and `RM`\n",
    "        - Print the coefficients.\n",
    "        - Does the interaction term improve R^2?\n",
    "        \n",
    "    Store R^2 on testing sets for later investigation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/breakout_1_sol.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineering Variables: Exploring Polynomial Regression \n",
    "\n",
    "![poly](../fig/poly.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we now believe that the fare also depends on the square of age. How would we include this term in our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.plot(titanic_train['age'], titanic_train['fare'], 'o')\n",
    "x = np.linspace(0,80,100)\n",
    "ax.plot(x, x, '-', label=r'$y=x$')\n",
    "ax.plot(x, 0.04*x**2, '-', label=r'$y=c x^2$')\n",
    "ax.set_title('Plotting Age (x) vs Fare (y)')\n",
    "ax.set_xlabel('Age (x)')\n",
    "ax.set_ylabel('Fare (y)')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model that predicts fare from all the predictors in `model_4` + the square of age. Show the summary of this model. Call it `model_5`. Remember to use the training data, `titanic_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train['age^2'] = titanic_train['age']**2\n",
    "titanic_test['age^2'] = titanic_test['age']**2\n",
    "\n",
    "X_train = titanic_train[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age', \n",
    "                             'sex_male_X_class_Second', 'sex_male_X_class_Third', 'age^2']]\n",
    "y_train = titanic_train['fare']\n",
    "X_test = titanic_test[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age', \n",
    "                             'sex_male_X_class_Second', 'sex_male_X_class_Third', 'age^2']]\n",
    "y_test = titanic_test['fare']\n",
    "\n",
    "model_5 = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the coefficients and intercept values\n",
    "print(\"Coefficients of the model: \",model_5.coef_)\n",
    "print(\"Intercept of the model: \",model_5.intercept_)\n",
    "\n",
    "# predict on test set\n",
    "y_test_pred = model_5.predict(X_test)\n",
    "\n",
    "# get R-squared\n",
    "print(\"R-squared of the model:\", r2_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time for Breakout Room 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: Learn about how to include polynomial terms in your model. \n",
    "\n",
    "**Directions:**\n",
    "\n",
    "In all the cases print the coefficients and report the R^2 in testing: \n",
    "\n",
    "    1. Build a polynomial regression model including the 2nd degree terms of both `LSTAT` and `RM` \n",
    "    2. Next, include the interaction term between `LSTAT` and `RM` in your model\n",
    "    3. Finally, include the 3rd degree terms of both `LSTAT` and `RM` in your model\n",
    "      \n",
    "Can you see any improvement? Reviewing the models you have built thus far, which one would you choose and why? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../solutions/breakout_2_sol.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance of previous models:\n",
    "print('R^2: ', R2_1, R2_1_inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good model is the model that gives good performance on the testing set while it does not use too many predictors (features). Less features makes a model more stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = [R2_0b, R2_0a,  R2_1, R2_1_inter, R2_2, R2_2_inter, R2_3_inter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[8,6])\n",
    "x= np.arange(0,7,1)\n",
    "plt.plot(x,R2,'-ob')\n",
    "\n",
    "xTick_label =  ['R2_0b', 'R2_0a',  'R2_1', 'R2_1_inter', 'R2_2', 'R2_2_inter', 'R2_3_inter'] \n",
    "plt.xticks(x,xTick_label, rotation ='vertical') \n",
    "plt.ylabel('$R^2$')\n",
    "plt.xlabel('Model')\n",
    "plt.title('Model Performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that including the interaction term the performance jumps from ~60% to ~72%. Including higher polynomial terms we do not notice a significant improvement. Hence, the best model is the first order polynomial with interaction term. \n",
    "\n",
    "Here, we do not suggest a method for selecting the best model, just introduce the concept. We will be covering cross validation in depth next section!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![linear regression](../fig/linear_regression.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer to this question can be found on closer examimation of $\\epsilon$. What is $\\epsilon$? It is assumed that $\\epsilon$ is normally distributed with a mean of 0 and variance $\\sigma^2$. But what does this tell us?\n",
    "\n",
    "1. Assumption 1: **Linearity** This is an implicit assumption as we claim that Y can be modeled through a linear combination of the predictors.\n",
    "3. Assumption 2: **Independence of observations** This comes from the randomness of $\\epsilon$.\n",
    "\n",
    "2. Assumption 3: **Constant variance of $\\epsilon$ errors** This means that if we plot our **residuals**, which are the differences between the true $Y$ and our predicted $\\hat{Y}$, they should look like they have constant variance and a mean of 0.\n",
    "4. Assumption 4: **Normality** We assume that the $\\epsilon$ is normally distributed, and we can show this in a histogram of the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model_5.predict(X_train)\n",
    "residuals = titanic_train['fare'] - y_hat\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "ax = ax.ravel()\n",
    "ax[0].set_title('Plot of Residuals')\n",
    "ax[0].scatter(y_hat, residuals, alpha=0.2)\n",
    "ax[0].set_xlabel(r'$\\hat{y}$')\n",
    "ax[0].set_xlabel('residuals')\n",
    "\n",
    "ax[1].set_title('Histogram of Residuals')\n",
    "ax[1].hist(residuals, alpha=0.7)\n",
    "ax[1].set_xlabel('residuals')\n",
    "ax[1].set_ylabel('frequency');\n",
    "\n",
    "# Mean of residuals\n",
    "print('Mean of residuals: {}'.format(np.mean(residuals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can you say about the assumptions of the model?\n",
    "\n",
    "Real data violate assumptions. So why use linear regression? Because linear regression has an analytical solution, so we guaranteed to find the optimal solutions and the solutions are computationally cheap to obtain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a confidence interval?\n",
    "\n",
    "A confidence interval is a range of values that is likely to include a parameter of interest with some degree of certainty or \"confidence.\"\n",
    "\n",
    "#### How do we interpret 95% confidence intervals?\n",
    "\n",
    "If we were to compute 95% confidence intervals for each of K repeated samples, we would expect 0.95\\*K of those confidence intervals to contain the true parameter of interest.\n",
    "\n",
    "#### What is bootstrapping?\n",
    "\n",
    "Bootstrapping is a procedure for resampling a dataset with replacement to produce a **distribution** of the value of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model we selected from above, let's do some bootstrapping to generate the confidence intervals for our coefficients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let's check out the coefficients of R2_1_inter\")\n",
    "print(\"Beta0\",\":\",model_boston_1_inter.intercept_)\n",
    "for i in range(3):\n",
    "    print(\"Beta\"+str(i+1),\":\",model_boston_1_inter.coef_[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bootstraps\n",
    "\n",
    "bootstrap = []\n",
    "numboot = 200\n",
    "\n",
    "for i in range(numboot):\n",
    "    boston_sampled = boston.sample(frac=1, replace=True)\n",
    "    boston_sampled[\"LSTAT*RM\"] = boston_sampled[\"LSTAT\"]*boston_sampled[\"RM\"]\n",
    "    model_boston_1_inter = LinearRegression().fit(boston_sampled[[\"LSTAT\", \"RM\", \"LSTAT*RM\"]], boston_sampled.MEDV)\n",
    "    bootstrap.append(model_boston_1_inter.coef_)\n",
    "\n",
    "bootstrap = np.array(bootstrap)\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize = (18,10))\n",
    "ax = ax.ravel()\n",
    "for i in range(3):\n",
    "    betavals = bootstrap[:,i]\n",
    "    betavals.sort()\n",
    "    x1 = np.percentile(betavals,2.5)\n",
    "    x2 = np.percentile(betavals,97.5)\n",
    "    x = np.linspace(x1,x2,500)\n",
    "    counts, bins = np.histogram(betavals)\n",
    "    y = counts.max()\n",
    "    ax[i].hist(bootstrap[:,i], bins = 10, color=\"gold\",alpha=0.5,edgecolor='black', linewidth=1)\n",
    "    ax[i].fill_between(x,y, color = 'cornflowerblue',alpha=0.3)\n",
    "    ax[i].set_ylabel(f'Distribution of beta {i}',fontsize=18)\n",
    "    ax[i].set_xlabel(f'Value of beta {i}',fontsize=18)\n",
    "    ax[i].axvline(x = np.mean(betavals), color='r')\n",
    "\n",
    "fig.delaxes(ax[3])\n",
    "fig.suptitle(f'95 % confidence interval of R2_1_inter Coefficients', fontsize = 24)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "### End of Standard Section\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Visual exploration of predictors'  correlations\n",
    "\n",
    "The dataset for this problem contains 10 simulated predictors and a response variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data \n",
    "data = pd.read_csv('../data/dataset3.txt')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this effect can be replicated using the scatter_matrix function in pandas plotting\n",
    "sns.pairplot(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictors x1, x2, x3 seem to be perfectly correlated while predictors x4, x5, x6, x7 seem correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: A Handy Matplotlib Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/XTzSuoR.png)\n",
    "source: http://matplotlib.org/faq/usage_faq.html\n",
    "\n",
    "See also [this](http://matplotlib.org/faq/usage_faq.html) matplotlib tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![violin plot](../fig/violin.png)\n",
    "\n",
    "See also [this](https://mode.com/blog/violin-plot-examples) violin plot tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using statsmodel OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: You've done this before: make a simple model using the OLS package from the statsmodels library predicting **fare** using **age** using the training data. Name your model `model_1` and display the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ca = sm.add_constant(titanic_train['age'])\n",
    "model_1 = OLS(titanic_train['fare'], age_ca).fit()\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = sm.OLS(titanic_train['fare'], \n",
    "                 sm.add_constant(titanic_train[['age', 'sex_male', 'class_Second', 'class_Third']])).fit()\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train['sex_male_X_age'] = titanic_train['age'] * titanic_train['sex_male']\n",
    "\n",
    "model_3 = sm.OLS(\n",
    "    titanic_train['fare'],\n",
    "    sm.add_constant(titanic_train[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age']])\n",
    ").fit()\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It seemed like gender interacted with age and class. Can we put that in our model?\n",
    "titanic_train['sex_male_X_class_Second'] = titanic_train['age'] * titanic_train['class_Second']\n",
    "titanic_train['sex_male_X_class_Third'] = titanic_train['age'] * titanic_train['class_Third']\n",
    "\n",
    "model_4 = sm.OLS(\n",
    "    titanic_train['fare'],\n",
    "    sm.add_constant(titanic_train[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age', \n",
    "                             'sex_male_X_class_Second', 'sex_male_X_class_Third']])\n",
    ").fit()\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train['age^2'] = titanic_train['age']**2\n",
    "\n",
    "model_5 = sm.OLS(\n",
    "    titanic_train['fare'],\n",
    "    sm.add_constant(titanic_train[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age', \n",
    "                             'sex_male_X_class_Second', 'sex_male_X_class_Third', 'age^2']])\n",
    ").fit()\n",
    "model_5.summary()\n",
    "\n",
    "predictors = sm.add_constant(titanic_train[['age', 'sex_male', 'class_Second', 'class_Third', 'sex_male_X_age', \n",
    "                                      'sex_male_X_class_Second', 'sex_male_X_class_Third', 'age^2']])\n",
    "y_hat = model_5.predict(predictors)\n",
    "residuals = titanic_train['fare'] - y_hat\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(16,5))\n",
    "ax = ax.ravel()\n",
    "ax[0].set_title('Plot of Residuals')\n",
    "ax[0].scatter(y_hat, residuals, alpha=0.2)\n",
    "ax[0].set_xlabel(r'$\\hat{y}$')\n",
    "ax[0].set_xlabel('residuals')\n",
    "\n",
    "ax[1].set_title('Histogram of Residuals')\n",
    "ax[1].hist(residuals, alpha=0.7)\n",
    "ax[1].set_xlabel('residuals')\n",
    "ax[1].set_ylabel('frequency');\n",
    "\n",
    "# Mean of residuals\n",
    "print('Mean of residuals: {}'.format(np.mean(residuals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
