var tipuesearch = {"pages":[{"title":"Calendar","text":"","tags":"pages","url":"pages/calendar.html"},{"title":"FAQ","text":"General What lectures may I attend? Registered students are free to attend any of the lecture times, regardless of which specific time they've registered for (e.g., you may attend 3pm lectures even if registered for 9am). Does the individual HW mean I have to submit on my own but can I still work with my HW partner? Individual CS109A HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with partner. You are allowed to use OHs to ask for clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at libraries documentation. Do I have access to the video recorded materials if I am not an Extension School student? Yes. All CS109A students have access to all video captured materials. If you have any issues accessing the video content, please send an email to our helpline with your name and HUID. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109a2020@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, you are welcome to audit this course. Send an email to cs109a2020@gmail.com to request full Canvas access. All auditors must agree to abide by the following rules: All auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Auditors may not attend lectures or section during the live stream. Students are randomly assigned to small groups for coding exercises and mixing auditors and students in this way is not ideal. We are investigating methods that would allow auditors to join, but this is the current policy. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, Jupyter Hub, and office hours. Projects Can I do a custom final project? The teaching staff may approve a custom project if: You have a group of at least 3 students interested in working on the project You have a clear problem statement outlined in a project proposal for approval by the professors You have access to the required data The data is not private and can be shared with members of the teaching staff who will be guiding and evaluating the project Quizzes & Exercises When are quizzes and exercises due? Quizzes and exercises are due before the next 9am lecture. I missed a quiz/exercise. Can I make it up? No. We will drop your lowest 25% of quizzes and your lowest 25% of exercises. This policy is to reduce stress and is in place so that missing a quiz or exercise on occasion should not affect your final grade.","tags":"pages","url":"pages/faq.html"},{"title":"Preparation","text":"In order to get the most out of CS109A, knowledge of multivariate calculus, probability theory, statistics, and some basic linear algebra (e.g., matrix operations, eigenvectors, etc.) is suggested. Below are some resources for self-assessment and review: Multivariate Calculus : multiple exams /w solutions Linear Algebra: multiple exams /w solutions 1 , 2 Probability: exams with solutions and problem sets with solutions Statistics: multiple pairs of exam questions and answers Q1 , A1 , Q2 , A2 , Q3 , A3 Here is a useful textbook for reviewing many of the above topics: Mathematics for Machine Learning Note: you can be successful in the course (assignments, quizzes, etc.) with the listed pre-requisites, but some of the material presented in lectures may be more easily understood with more background. TThis course dives right into core Machine Learning topics. For this, students are expected to be fluent in Python programming. You can familiarize yourself with Python by completing online tutorials. Additionally, there are free Basics Python courses that cover all necessary python topics. You are also expected to be able to manipulate data using pandas DataFrames, perform basic operations with numpy arrays, and make use of basic plotting functions (e.g. line chart, histogram, scatter plot, bar chart) from matplotlib. Python basics Throughout this course, we will be using Python as our primary programming language for exercises, labs, and homework. Thus, you must have basic Python programming knowledge. The following are the topics you need to cross off your checklist before the course begins: Variables, Datatypes, strings, file operations, Data structures such as lists, dictionaries, tuples and classes. Pandas Basics Most of the exercises you will encounter in this course exploit various datasets. Pandas is an open-source data analysis and manipulation tool, built on top of Python. In this course, we have provided the necessary support material and resources to work with Pandas. However, it is highly recommended that you get yourselves familiar with basic data manipulation using Pandas to ensure a smooth learning experience. Numpy Basics NumPy is a library for the Python programming language that provides support for large, multi-dimensional arrays and matrices and a large collection of high-level mathematical functions to operate on these data structures. Because of the extensive exercises provided in this course, it is important to use Numpy for efficient problem-solving to get identical results. Though this course aims to support individuals with no prior Numpy knowledge, you must go through the basics of this library to avoid any possible hiccups. Matplotlib Basics A large portion of this course uses different graphs and charts to explain topics and validate results. Matplotlib is a plotting library for Python. This library has been used to create all the graphs you will see throughout the course. Additionally, the exercises and homeworks are structured in a manner that integrates this library. Henceforth, it is highly recommended to get yourselves acquainted with Matplotlib Basics. For this course, we will be using Jupyter Notebooks. You can familiarize yourself with Jupyter notebooks by reading the following tutorials: A Beginner's Tutorial to Jupyter Notebooks Finally, we assume that students have a strong foundation in calculus, linear algebra, statistics, and probability. You should review these concepts before the course begins. Here is one useful resource: Mathematics for Machine Learning INTRODUCTION TO ONLINE TEACHING To make the most of this course, we highly recommend you follow the given guidelines: Ensure you have a good internet connection. As the course is entirely online, you must have strong internet to avoid missing out on the lecture. This course is designed to have healthy interactions to understand the material. Hence, it is vital to have a good audio system. You should be able to listen to lectures as well as respond to questions during one. It is highly recommended that each of you have your video turned on during the lectures. Make sure your face is clearly visible with ambient lighting. Before each lecture, ensure you have a programming ready device. You should be able to write and execute code during the lecture. Additionally, make sure your device settings allow screen sharing. Consistency is key. Attend all the lectures, sections and office hours (if needed). Make sure you are on time for all of them. If possible, please try to avoid the use of mobile phones to attend lectures. More info can be found here .","tags":"pages","url":"pages/preparation.html"},{"title":"Projects","text":"Project Guidelines ProjectGuidelines.pdf Project Choices A - Crime in Boston B - Brazil's COVID Response C - COVID in the US D - Mortality Prediction & Interpretation E - Predicting the 2020 Election F - EA Sports - FIFA G - Police Violence in the US","tags":"pages","url":"pages/projects.html"},{"title":"Schedule","text":"Date (Mon) Lecture (Mon) Lecture (Wed) Lecture (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 9/1 Lecture 1: What is Data Science? General introduction. Lecture 2: Data + RegEx R:HW0 9/7 No Lecture (Holiday) Lecture 3: Web Scraping + PANDAS Lecture 4: Advanced PANDAS R:HW1 - D:HW0 9/14 Lecture 5: kNN Regression and Linear Regression Lecture 6: Multi and Poly Regression Lecture 7: Modeling knn and Linear R with skleanr R:HW2 - D:HW1 9/21 Lecture 8: Basic Statistics for Data Science Lecture 9: Inference: Bootstrap and CI Lecture 10: Hypothesis Testing & Predictive CI Advanced Section 1: Linear Algebra and Hypothesis Testing R:HW3 - D:HW2 9/28 Lecture 11: Cross-Validation & Model Selection Lecture 12: Regularization: Ridge & Lasso Lecture 13: Estimation of regularization parameter; Hands on Advanced Section 2: Methods of regularization and their justifications Milestone 1 10/5 Lecture 14: Visualization for Communication Lecture 15: kNN classification and Logistic Regression I Lecture 16: Case Study 1 R: HW4 (Individual) - D: HW3 10/12 No Lecture (Holiday) Lecture 17: Logistic Regression II Lecture 18: Multi class Classification (introduce softmax) Advanced Section 3: Generalized Linear Models Milestone 2 10/19 Lecture 19: Dealing with missing data, imputation Lecture 20: PCA Lecture 21: PCA and missing with data; hands on Advanced Section 4: Mathematical Foundations of PCA R:HW5 - D:HW4 10/26 Lecture 22: Classification Trees Lecture 23: Regression Trees Bagging RF Lecture 24: Tuning hyperparameters R:HW6 - D:HW5 11/2 Lecture 25: Boosting Methods for Regression Lecture 26: Boosting Methods for Classification Lecture 27: Case Study 2 Advanced Section 5: Stacking and mixture of experts 11/9 Lecture 28: Neural Networks 1-Perceptron and MLP Lecture 29: Neural Networks 2- Anatomy of NN, design choices Lecture 30: Neural Netoworks 3- Design Choices II & Gradient Descent R:HW7 (Individual) - D:HW6 11/16 Lecture 31: Neural Networks 4 -Back Propagation, SGD Lecture 32: Regularization methods - Weight decay, data augmentation and dropout Lecture 33: Full worked example of regression and classification FFNN Advanced Section 6: Deeper into Solvers Milestone 3 11/23 Lecture 34: EthiCS No lecture (Thanksgiving) No lecture (Thanksgiving) R:HW8 - D:HW7 11/30 Lecture 35: Interpreting Prediction Models Lecture 36: Wrap-Up D: HW8 12/7 Reading Period 12/14 Finals Week","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"Introduction to Data Science (Fall 2020) CS 109a, AC 209a, Stat 121a, or CSCI E-109a Course Heads Pavlos Protopapas (SEAS), Kevin Rader (Statistics), & Chris Tanner (SEAS) Instructor: Eleni Kaxiras (SEAS) Lectures: Mon, Wed, Fri at 9am-10:15am and 3pm-4:15pm Sections: Fri 1:30-2:45 pm and Mon 8:30-9:45 pm. (identical material) [starts 9/11] Advanced Sections: Wed at 12pm [starts 9/23] Office Hours: (TBD) Format: Exclusively online, but we aim to foster enriching interactions and collaboration as much as possible. Prerequisites: You are expected to have programming experience at the level of CS 50 or above, and statistics knowledge at the level of Stat 100 or above (Stat 110 recommended). HW #0 is designed to test your knowledge on the prerequisites. Successful completion of this assignment will show that this course is suitable for you. HW #0 will not be graded but you are required to submit. Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course in data science. The course focuses on the analysis of messy, real-life data to perform predictions using statistical and machine learning methods. Throughout the semester, our content continuously centers around five key facets: 1. data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set; 2. data management ‐ accessing data quickly and reliably; 3. exploratory data analysis – generating hypotheses and building intuition; 4. prediction or statistical learning; and 5. communication – summarizing results through visualization, stories, and interpretable summaries. Only one of CS109a, AC209a, or STAT121a can be taken for credit. Students who have previously taken CS109, AC209, or STAT121 cannot take CS109A, AC 209A, or STAT121A for credit. Course Components The lectures will be live-streamed and can be accessed through the Zoom section on Canvas. Video recordings of the live stream will be made available within 24 hours after the event, and will be accessible from the Lecture Video section on Canvas. Lectures The class meets, virtually, three days a week for lectures (M, W, F). The same lecture will be given twice each day: once in the morning and again in the afternoon, to accommodate students in different time zones. Mondays and Wednesdays will be mostly lecture content with some hands-on coding, whereas Fridays will be the inverse (mostly hands-on coding). Attending and participating in lectures is a crucial component of learning the material presented in this course. What to expect A lecture will have the following pedagogy layout which will be repeated: Asynchronous pre-class exercises of approxmately 30 min. This will include, reading from the textbooks or other sources, watching videos to prepare you for the class. Approx. 10 minutes of Q&A regarding the pre-class exercises and/or review of homework and quiz questions. Live online instruction followed by a short Q/A session Hands-on exercises, on the ED platform. Sessions will help students develop the intuition for the core concepts, provide the necessary mathematical background, and provide guidance on technical details. Sessions will be accompanied by relevant examples to clarify key concepts and techniques. Quizzes At the end of each lecture, there will be a short, graded quiz that will cover the pre-class and in-class material; there will be no AC209a content in the quizzes. The quizzes will be available until the next lecture. 25% of the quizzes will be dropped from your grade. Exercises Lectures will include one or more coding exercises focused on the newly introduced material; there will be no AC209a content in the exercises. The exercises are short enough to be completed during the time allotted in lecture but they will remain available until the beginning of the following lecture to accomodate those who cannot attend in real time. 25% of the exercises will be dropped from your grade. Sections Lectures are supplemented by sections led by teaching fellows. There are two types of sections: Standard Sections : This will be a mix of review of material and practice problems similar to the HW. The material covered on Friday and Monday is identical. Advanced Sections The course will include advanced sections for 209a students and will cover a different topic per week. These are 75-min lectures and will cover advanced topics like the mathematical underpinnings of the methods seen in lecture and hands-on exercises, along with extensions of those methods. The material covered in the advanced sections is required for all AC209a students. Note: Sections are not held every week. Consult the course calendar for exact dates. Exams There are no exams in this course. Projects Students will work in groups of 2-4 to complete a final group project, due during the Exams period. See Calendar for specific dates. Homework Assignments There will be 9 graded homework assignments. Some of them will be due one week after being assigned, and some will be due two weeks after being assigned. You have the option to work and submit in pairs for all the assignments except HW4 and HW7, which you will do individually. You will be working in Jupyter Notebooks, which you can run in your own environment or in the SEAS JupyterHub cloud. Instructions for Setting up Your Environment Instructions for Using JupyterHub On weeks with new assignments, the assignments will be released by Wednesday 3pm. Standard assignments are graded out of 5 points. AC209a students will have additional homework content for most assignments worth 1 point. Instructor Office Hours Pavlos : (TBD) Kevin : (TBD) Chris : (TBD) Eleni : (TBD) Participation Students are expected to be actively engaged with the course. This includes: Attending and participating in lectures (or the follow-up session later in the day) Making use of resources such as office hours and sections Participating in the Ed discussion forum — both through asking thoughtful questions and by answering the questions of others Despite being remote, we aim to make this course as interactive, stimulating, and fun as always, and we rely on each of you to contribute your awesome uniqueness. Recommended Textbook An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani. The book is available here: Free electronic version : http://www-bcf.usc.edu/~gareth/ISL/ (Links to an external site). HOLLIS : http://link.springer.com.ezp-prod1.hul.harvard.edu/book/10.1007%2F978-1-4614-7138-7 Amazon: https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370 (Links to an external site) . Deep Learning, Vol. 1 & 2: From Basics to Practice by Andrew Glassner Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville (MIT Press, 2016) Course Policies Getting Help For questions about homework, course content, package installation, JupyterHub, and after you have tried to troubleshoot yourselves, the process to get help is: 1. Post the question in Ed and get a response from your peers. Note that in Ed questions are visible to everyone. The teaching staff monitors the posts. 2. Go to Office Hours ; this is the best way to get direct help. 3. For private matters send an email to the Helpline: cs109a2020@gmail.com . The Helpline is monitored by the teaching staff. 4. For personal and confidential matters send an email to the instructors . Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit individual assignments, include the name of each other in the designated area of the submission. if you work with a fellow student and want to submit the same assignment, you need to form a group prior to the submission. Details in the assignment. Remember, not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator (e.g., not entirely from Google search results) you are welcome to take ideas from code presented in lecture or section, but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Wrongly Submitted Assignments There are no late days in homework submission. We will accept late submissions only for medical (if accompanied by a doctor's note) or other official University-excused reasons. To submit after Canvas has closed or to ask for an extension , send an email to the Helpline with subject line \"Submit HW1: Reason=the flu\" replacing 'HW1' with the name of the current assignment and \"the flu\" with your reason. You need to attach the note from your medical provider otherwise we will not accept the request. Email the instructors if you have other University-excused reasons. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Grading Guidelines Homework will be graded based on: 1. How correct your code is (the Notebook cells should run, we are not troubleshooting code) 2. How you have interpreted the results — we want text not just code. It should be a report. 3. How well you present the results. The scale is 0 to 5 for each assignment. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading , The points we take off are based on a grading rubric that is being applied uniformly to all submissions. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release . Zoom Expectations: Despite being remote and distributed, we want everyone to participate and engage with fellow classmates and staff. To this end, we request for everyone to leave his/her/their video on during class. We fully understand that some may be uncomfortable or unable to do so, for various personal reasons. We respect your decision and empathize for any situation that may arise. Nonetheless, if possible, we strongly ask for you to please leave your video on. Many are getting accustomed to Zoom for the first time. While it is understandable to have some glitches, mistakes, and faux pas, some mistakes can be largely disruptive to such a large audience like that of our class. Please familiarize yourself to Zoom before the semester begins. In particular, please: keep your video on (see above) keep your mic muted by default, until you speak be mindful of overtly distracting actions that your video may cause (this isn't a large issue, but it's akin to students' laptops distracting those who sit nearby) be mindful of your mic settings after returning to the main room from a break-out room, as it tends to un-mute each person's mic. Communication from Staff to Students Class announcements will be through Ed . All homework and will be posted and submitted through Canvas . Quizzes are completed on Ed as well as all feedback forms. NOTE: make sure you adjust your account settings so you can receive emails from Canvas. Submitting an assignment Please consult Homework Policies & Submission Instructions [Update link] Course Grade Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Homework 0 1% Paired Homework (7) 42% Individual Homework (2) 20% Quizzes 6% Exercises 6% Project 25% Total 100% Software We will be using Jupyter Notebooks, Python 3, and various python modules. You can access the notebook viewer either on your own machine by installing the Anaconda platform (Links to an external site) which includes Jupyter/IPython as well all packages that will be required for the course, or by using the SEAS JupyterHub from Canvas. Details in class. Auditing the Class If you would like to audit the class, please send an email to the Helpline indicating who you are and why you want to audit the class. You need a HUID to be included to Canvas. Please note that auditors may not submit assignments for grading or make use of other limited student resources such as office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109A we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. Accommodations for Students with Disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve.","tags":"pages","url":"pages/syllabus.html"},{"title":"Lecture 36: Wrap-Up Review","text":"Slides PDF | Lecture 36: Wrap-Up Review PPTX | Lecture 36: Wrap-Up Review","tags":"lectures","url":"lectures/lecture36/"},{"title":"Lecture 35: Interpreting Prediction Models","text":"Slides PDF | Lecture 35: Interpreting Prediction Models Exercises Lecture 35: Exercise [Notebook]","tags":"lectures","url":"lectures/lecture35/"},{"title":"Lecture 33: Full Example of Regression & Classification FFNN","text":"Slides PDF | Lecture 33: Review of NNs PPTX | Lecture 33: Review of NNs PDF | Lecture 33: Dropout PPTX | Lecture 33: Dropout Notebook Public Link to Google Colab Notebook for Lecture 33 Public Link to kanye_shoe.jpg, a photo of a very important shoe Exercises Lecture 33: Exercise 1 [Notebook]","tags":"lectures","url":"lectures/lecture33/"},{"title":"S-Section 10:  Feed Forward Neural Networks: Regularization and Adam optimizer","text":"Jupyter Notebooks S-Section 10: Feed Forward Neural Networks: Regularization and Adam optimizer","tags":"sections","url":"sections/section10/"},{"title":"Advanced Section 6: Deep into Solvers","text":"Slides A-Section 6: Deep into Solvers [PDF] A-Section 6: Deep into Solvers [PPTX]","tags":"a-sections","url":"a-sections/a-section6/"},{"title":"Lecture 32: Regularization methods - Weight decay, data augmentation and dropout","text":"Slides PDF | Lecture 32: Optimizers PPTX | Lecture 32: Optimizers PDF | Lecture 32: NN Regularization A PPTX | Lecture 32: NN Regularization A PDF | Lecture 32: NN Regularization B PPTX | Lecture 32: NN Regularization B Exercises Lecture 32: Regularization using L1 and L2 Norm [Notebook] Lecture 32: Early Stopping [Notebook]","tags":"lectures","url":"lectures/lecture32/"},{"title":"Lecture 31: Neural Networks 4 -Back Propagation, SGD","text":"Slides PDF | Lecture 31: Stochastic Gradient Descent PPTX | Lecture 31: Stochastic Gradient Descent PDF | Lecture 31: Back Propagation PPTX | Lecture 31: Back Propagation PDF | Lecture 31: Optimizers PPTX | Lecture 31: Optimizers Exercises Lecture 31: Back-propagation by hand [Notebook] Lecture 31: Back-propagation by chain rule [Notebook]","tags":"lectures","url":"lectures/lecture31/"},{"title":"Lecture 30: Neural Networks 3: Design Choices II & Gradient Descent","text":"Slides PDF | Lecture 30: Design Layers PPTX | Lecture 30: Design Layers PDF | Lecture 30: Gradient Descent PPTX | Lecture 30: Gradient Descent Exercises Lecture 30: A.1- MLP using Keras [Notebook] Lecture 30: B.1 - Gradient Descent [Notebook]","tags":"lectures","url":"lectures/lecture30/"},{"title":"S-Section 09: Feed forward neural networks","text":"Jupyter Notebooks S-Section 9: Feed forward neural networks","tags":"sections","url":"sections/section9/"},{"title":"Lecture 29:Neural Networks 2 - Anatomy of NN & Design Choices","text":"Slides PDF | Lecture 29: Anatomy of NN PPTX | Lecture 29: Anatomy of NN PDF | Lecture 29: Design Layers PPTX | Lecture 29: Design Layers Exercises Lecture 29: A.1 - Constructing an MLP [Notebook]","tags":"lectures","url":"lectures/lecture29/"},{"title":"Lecture 28: Neural Networks 1 - Perceptron & MLP","text":"Slides PDF | Lecture 1: Perceptron & MLP A PPTX | Lecture 1: Perceptron & MLP A PDF | Lecture 1: Perceptron & MLP B PPTX | Lecture 1: Perceptron & MLP B Exercises Lecture 28: A.1 - Build a Single Neuron by Hand [Notebook] Lecture 28: B.1 - MLP by Hand [Notebook]","tags":"lectures","url":"lectures/lecture28/"},{"title":"Lecture 27: Case Study 2","text":"Slides PDF | Lecture 27: Case Study 2 PPTX | Lecture 27: Case Study 2 Exercises Lecture 27: 1 Example Notebooks [Notebook] Lecture 27: 2 Example Notebooks [Notebook]","tags":"lectures","url":"lectures/lecture27/"},{"title":"S-Section 08: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost","text":"Jupyter Notebooks S-Section 8: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost","tags":"sections","url":"sections/section8/"},{"title":"Advanced Section 5: Stacking and mixture of experts","text":"Slides A-Section 5: Stacking [PDF]","tags":"a-sections","url":"a-sections/a-section5/"},{"title":"Lecture 26: Boosting Methods for Classification","text":"Slides PDF | Lecture 26: Gradient Boosting PPTX | Lecture 26: Gradient Boosting PDF | Lecture 26: AdaBoositng PPTX | Lecture 26: AdaBoositng Exercises Lecture 26: Regression with Boosting [Notebook] Lecture 26: Boosting Classification [Notebook]","tags":"lectures","url":"lectures/lecture26/"},{"title":"Lecture 25: Boosting Methods for Regression","text":"Slides PDF | Lecture 25: RF Feature Importance PPTX | Lecture 25: RF Feature Importance PDF | Lecture 25: Boositng PPTX | Lecture 25: Boositng Exercises Lecture 25: Feature Importance [Notebook] Lecture 25: Random Forest vs SMOTE Classification [Notebook]","tags":"lectures","url":"lectures/lecture25/"},{"title":"Lecture 24: Tuning Hyperparameters","text":"Slides PDF | Lecture 24: Tuning Hyperparameters PPTX | Lecture 24: Tuning Hyperparameters Exercises Lecture 24: Bagging vs Random Forest (Tree correlation) [Notebook] Lecture 24: Hyperparameter tuning [Notebook]","tags":"lectures","url":"lectures/lecture24/"},{"title":"S-Section 07: Bagging and Random Forest","text":"Jupyter Notebooks S-Section 7: Bagging and Random Forest","tags":"sections","url":"sections/section7/"},{"title":"Lecture 23: Regression Trees, Bagging, and RF","text":"Slides PDF | Lecture 23: Bagging PPTX | Lecture 23: Bagging Exercises Lecture 23: Regression with Bagging [Notebook] Lecture 23: Bagging Classification with Decision Boundary [Notebook]","tags":"lectures","url":"lectures/lecture23/"},{"title":"Lecture 21: PCA & Missing Data","text":"See lecture 20 slides Exercises Lecture 21: Multi-class Classification and PCA [Notebook]","tags":"lectures","url":"lectures/lecture21/"},{"title":"S-Section 06: PCA and Logistic Regression","text":"Jupyter Notebooks S-Section 6: PCA and Logistic Regression","tags":"sections","url":"sections/section6/"},{"title":"Advanced Section 4: Mathematical Foundations  of PCA","text":"Slides A-Section 4: PCA [PDF] A-Section 4: PCA [PPTX] Notes A-Section 4: PCA (Notes) [PDF]","tags":"a-sections","url":"a-sections/a-section4/"},{"title":"Lecture 20: PCA","text":"Slides PDF | Lecture 20: PCA Exercises Lecture 20: PCA [Notebook]","tags":"lectures","url":"lectures/lecture20/"},{"title":"Lecture 22: Classification Trees","text":"Slides PDF | Lecture 22: Classification Trees Exercises Lecture 22: Decision Tree Classification [Notebook]","tags":"lectures","url":"lectures/lecture22/"},{"title":"Lecture 19: Missing Data","text":"Slides PDF | Lecture 19: Missing Data Exercises Lecture 19: 1 - Dealing with Missingness [Notebook]","tags":"lectures","url":"lectures/lecture19/"},{"title":"Lecture 18: Multiclass Logistic Regression","text":"Slides Logistic Regression [PDF] Logistic Regression [PDF] Annotated Slides Logistic Regression [PDF] <9AM> Logistic Regression [PDF] <3PM> Exercises Lecture 18: 1 - Basic Multi-classification [Notebook] Lecture 18: 2 [Not Graded!] - A Walkthrough Example [Notebook]","tags":"lectures","url":"lectures/lecture18/"},{"title":"S-Section 05:  Logistic Regression, Multiple Logistic Regression, and KNN-classification","text":"Jupyter Notebooks S-Section 5: Logistic Regression, Multiple Logistic Regression, and KNN-classification","tags":"sections","url":"sections/section5/"},{"title":"Advanced Section 3: Generalized Linear Models","text":"Slides A-Section 3: Generalized Linear Models (Slides) [PPTX] A-Section 3: Generalized Linear Models (Slides) [PDF] Notes A-Section 3: Generalized Linear Models (Notes) [PDF]","tags":"a-sections","url":"a-sections/a-section3/"},{"title":"Lecture 17:  kNN classification and Logistic Regression II","text":"Slides Logistic Regression [PDF] Logistic Regression [PPTX] Exercises Lecture 17: 1 - Regularization and Decision Boundaries in Logistic Regression [Notebook] Lecture 17: 2 [Not Graded!] - Confusion Matrices & ROC Curves [Notebook]","tags":"lectures","url":"lectures/lecture17/"},{"title":"Lecture 16:  Case Study","text":"Slides Case Study [PDF] Case Study [PPTX] Exercises Lecture 16: 1 - Exploration, Wrangling, and Defining a Question [Notebook] Lecture 16: 2 - Redefining and Scoping [Notebook]","tags":"lectures","url":"lectures/lecture16/"},{"title":"Lecture 15:  kNN classification and Logistic Regression I","text":"Slides Logistic Regression [PDF] Logistic Regression [PPTX] Exercises Lecture 15: 1 - Guesstimating Beta values for Logistic Regression [Notebook] Lecture 15: 2 - Simple k-NN Classification and Logistic Regression [Notebook]","tags":"lectures","url":"lectures/lecture15/"},{"title":"Lecture 14: Visualization","text":"Slides Visualization 1 [PDF] Visualization 2 [PDF] Visualization 3 [PDF] Visualization 4 [PDF] Visualization 5 [PDF] Visualization 6 [PDF] Visualization 7 [PDF] Visualization 8 [PDF] Visualization 9 [PDF] Visualization 10 [PDF] Visualization 11 [PDF] Visualization 12 [PDF] Visualization [PPTX] Exercises Lecture 14: Visualization [Notebook]","tags":"lectures","url":"lectures/lecture14/"},{"title":"Lecture 13: Thinking critically about models, data, and debugging","text":"Slides Models, Data, and Debugging [PDF] Models, Data, and Debugging [PPTX] Exercises Lecture 13: 1 - Debugging [Notebook]","tags":"lectures","url":"lectures/lecture13/"},{"title":"S-Section 04: Regularization and Model Selection","text":"Jupyter Notebooks S-Section 4: Regularization and Model Selection","tags":"sections","url":"sections/section4/"},{"title":"Advanced Section 2: Methods of regularization and their justifications","text":"Slides A-Section 1: Linear Algebra and Hypothesis Testing (slides) [PDF] A-Section 1: Linear Algebra and Hypothesis Testing (slides) [PPTX] Notes A-Section 1: Linear Algebra and Hypothesis Testing (notes) [PDF]","tags":"a-sections","url":"a-sections/a-section2/"},{"title":"Lecture 12: Estimation of the Regularization Coefficients using CV and comparison","text":"Slides Ridge & Lasso - Hyperparameters [PDF] Ridge & Lasso - Hyperparameters [PPTX] Ridge & Lasso - Comparison [PDF] Ridge & Lasso - Comparison [PPTX] Exercises Lecture 12: 1 - Regularization with Cross-validation [Notebook] Lecture 12: 2 - Variation of coefficients [Notebook]","tags":"lectures","url":"lectures/lecture12/"},{"title":"Lecture 11: Regularization","text":"Slides Bias/Variance Trade-off [PDF] Lasso & Ridge [PDF] Exercises Lecture 11: 1 - Bias Variance Tradeoff [Notebook] Lecture 11: 2 - Simple Lasso and Ridge Regularization [Notebook] Lecture 11: 3 - Hyper-parameter Tuning for Ridge Regression [Notebook]","tags":"lectures","url":"lectures/lecture11/"},{"title":"Lecture 10: Hypothesis Testing and Predictive CI","text":"Slides Hypothesis Testing [PDF] Hypothesis Testing [PPTX] Predictive CI [PDF] Predictive CI [PPTX] Exercises Lecture 10: 1 - Hypothesis testing [Notebook] Lecture 10: 2 - Prediction CI [Notebook] Lecture 10: Lecture 10 Additional Code: Multiple and Polynomial Regression (September 26, 2019 version) [Notebook]","tags":"lectures","url":"lectures/lecture10/"},{"title":"S-Section 03: Multiple Linear and Polynomial Regression","text":"Jupyter Notebooks S-Section 3: Multiple Linear and Polynomial Regression","tags":"sections","url":"sections/section3/"},{"title":"Advanced Section 1: Linear Algebra and Hypothesis Testing","text":"Slides A-Section 1: Linear Algebra and Hypothesis Testing [PPTX] A-Section 1: Linear Algebra and Hypothesis Testing [PDF]","tags":"a-sections","url":"a-sections/a-section1/"},{"title":"Lecture 9: Inference in Linear Regression","text":"Slides Into to Inference [PDF] Intro to Inference [PPTX] Inference & Bootstrap [PDF] Inference & Bootstrap [PPTX] Exercises Lecture 9: A.1 - Beta values for data from Random Universe [Notebook] Lecture 9: B.1 - Beta values for Data from Random Universe using Bootstrap [Notebook] Lecture 9: B.2 - Confidence Interval for Beta value [Notebook]","tags":"lectures","url":"lectures/lecture09/"},{"title":"Lecture 8: Probability","text":"Slides Probability [PDF] Probability [PPTX] Exercises Lecture 8: 1 - Normal Distributions and Likelihood [Notebook] Lecture 8: 2 - Linear Regression in Statsmodels [Notebook]","tags":"lectures","url":"lectures/lecture08/"},{"title":"Lecture 7: Model Selection","text":"Slides Model Selection [PDF] Model Selection [PPTX] Exercises Lecture 7: A.1 - Linear and Polynomial Regression with Residual Analysis [Notebook] Lecture 7: A.2 - Multi-collinearity vs Model Predictions [Notebook] Lecture 7: B.1 - Best Degree of Polynomial with Train and Validation sets [Notebook] Lecture 7: B.2 - Best Degree of Polynomial using Cross-validation [Notebook]","tags":"lectures","url":"lectures/lecture07/"},{"title":"S-Section 02: kNN and Linear Regression","text":"Jupyter Notebooks S-Section 2: kNN and Linear Regression","tags":"sections","url":"sections/section2/"},{"title":"Lecture 6: Linear Regression","text":"Slides Linear Regression [PDF] Multi & Poly Regression Regression [PDF] Exercises Lecture 6: A.1 - Guesstimate the β values [Notebook] Lecture 6: A.2 - MSE for varying β1 values [Notebook] Lecture 6: A.3 - Linear Regression using sklearn [Notebook] Lecture 6: B.1 - Simple Multi-linear Regression [Notebook]","tags":"lectures","url":"lectures/lecture06/"},{"title":"Lecture 5: kNN & Linear Regression","text":"Slides kNN Regression (Part A) Slides [PDF] kNN Regression (Part A) Slides [PPTX] kNN Regression (Part B) Slides [PDF] kNN Regression (Part B) Slides [PPTX] Linear Regression (Part C) Slides [PDF] Linear Regression (Part C) Slides [PPTX] Exercises Lecture 5: A.1 - Plotting the data [Notebook] Lecture 5: A.2 - Simple kNN regression [Notebook] Lecture 5: B.1 - Finding best k in kNN regression [Notebook]","tags":"lectures","url":"lectures/lecture05/"},{"title":"Lecture 4: Advanced PANDAS","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 4: EDA with PANDAS [Notebook] Lecture 4: EDA and Advanced PANDAS [Notebook] Lecture 4: Data Collection, Parsing, and Quick Analyses [Notebook]","tags":"lectures","url":"lectures/lecture04/"},{"title":"S-Section 01: Introduction to Web Scraping","text":"Jupyter Notebooks S-Section 1: Introduction to Web Scraping","tags":"sections","url":"sections/section1/"},{"title":"Lecture 3: Web Scraping + PANDAS","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 3: Web Scraping and Parsing Intro [Notebook] Lecture 3: PANDAS Intro [Notebook]","tags":"lectures","url":"lectures/lecture03/"},{"title":"Lecture 2: Data + RegEx","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 2: Data + RegEx [Notebook]","tags":"lectures","url":"lectures/lecture02/"},{"title":"Lecture 1: Introduction","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 1: Data Science Demo Exercises 1 Notebook Lecture 1: Data Science Demo Exercises 2 [Notebook]","tags":"lectures","url":"lectures/lecture01/"},{"title":"CS109a: Introduction to Data Science","text":"Fall 2020 Pavlos Protopapas , Kevin A. Rader , and Chris Tanner Additional Instructor: Eleni Kaxiras Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course to data science. We will focus on the analysis of data to perform predictions using statistical and machine learning methods. Topics include data scraping, data management, data visualization, regression and classification methods, and deep neural networks. You will get ample practice through weekly homework assignments. The class material integrates the five key facets of an investigation using data: 1. data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set 2. data management ‐ accessing data quickly and reliably 3. exploratory data analysis – generating hypotheses and building intuition 4. prediction or statistical learning 5. communication – summarizing results through visualization, stories, and interpretable summaries Only one of CS 109a, AC 209a, or Stat 121a can be taken for credit. Students who have previously taken CS 109, AC 209, or Stat 121 cannot take CS 109a, AC 209a, or Stat 121a for credit. Important Dates: Tuesday 9/8 - HW1 released Wednesday 9/9 - HW0 due at 11:59pm EST Thursday 9/10 - 'Study Break' at 8:30pm EST Friday 9/11 - 'Study Break' at 10:15am EST Friday 9/11 - Sections start (1:30pm EST) See syllabus/calendar for weekly times Helpline: cs109a2020@gmail.com Lectures: Mon , Wed , & Fri 9:00‐10:15 am & 3:00-4:14 pm (identical material in a single day) Sections: Fri 1:30-2:45 pm & Mon 8:30-9:45 pm (identical material) [starts 9/11] Advanced Sections: Wed 12pm [starts 9/23] Office Hours: TBD Course material can be viewed in the public GitHub repository . Previous Material 2019 2018 2017 2015 2014 . 2013","tags":"pages","url":"pages/cs109a-introduction-to-data-science/"}]}