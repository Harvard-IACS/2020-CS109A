var tipuesearch = {"pages":[{"title":"Calendar","text":"","tags":"pages","url":"pages/calendar.html"},{"title":"FAQ","text":"General What lectures may I attend? Registered students are free to attend any of the lecture times, regardless of which specific time they've registered for (e.g., you may attend 3pm lectures even if registered for 9am). Does the individual HW mean I have to submit on my own but can I still work with my HW partner? Individual CS109A HW means you are supposed to work on your own, without any human intervention, assistance, or input. You are not allowed to work with partner. You are allowed to use OHs to ask for clarification questions, but we may not be able to answer all of your questions or help with your coding. You are allowed to use all of your materials from the course up to the HW, look at problem resolution online, and look at libraries documentation. Do I have access to the video recorded materials if I am not an Extension School student? Yes. All CS109A students have access to all video captured materials. If you have any issues accessing the video content, please send an email to our helpline with your name and HUID. Where should I send my questions? Use Ed for anying related to the course content or assignments. All other concerns should be sent to the course helpline: cs109a2020@gmail.com Auditing Can I audit this course? What are the rules for auditing? Yes, you are welcome to audit this course. Send an email to cs109a2020@gmail.com to request full Canvas access. All auditors must agree to abide by the following rules: All auditors are held to the same standard of academic honesty as our registered students. Please do not share homeworks or solutions with anyone. Violations will be reported to the Harvard Administrative Board. Auditors are not permitted to take the course for credit in the future. Auditors may not attend lectures or section during the live stream. Students are randomly assigned to small groups for coding exercises and mixing auditors and students in this way is not ideal. We are investigating methods that would allow auditors to join, but this is the current policy. Audiors should not submit HWs or participate in projects. Auditors should refrain from using any course and TF resources that are designed for our registered students like Ed, Jupyter Hub, and office hours. Projects Can I do a custom final project? The teaching staff may approve a custom project if: You have a group of at least 3 students interested in working on the project You have a clear problem statement outlined in a project proposal for approval by the professors You have access to the required data The data is not private and can be shared with members of the teaching staff who will be guiding and evaluating the project Quizzes & Exercises When are quizzes and exercises due? Quizzes and exercises are due before the next 9am lecture. I missed a quiz/exercise. Can I make it up? No. We will drop your lowest 25% of quizzes and your lowest 25% of exercises. This policy is to reduce stress and is in place so that missing a quiz or exercise on occasion should not affect your final grade.","tags":"pages","url":"pages/faq.html"},{"title":"Preparation","text":"In order to get the most out of CS109A, knowledge of multivariate calculus, probability theory, statistics, and some basic linear algebra (e.g., matrix operations, eigenvectors, etc.) is suggested. Below are some resources for self-assessment and review: Multivariate Calculus : multiple exams /w solutions Linear Algebra: multiple exams /w solutions 1 , 2 Probability: exams with solutions and problem sets with solutions Statistics: multiple pairs of exam questions and answers Q1 , A1 , Q2 , A2 , Q3 , A3 Here is a useful textbook for reviewing many of the above topics: Mathematics for Machine Learning Note: you can be successful in the course (assignments, quizzes, etc.) with the listed pre-requisites, but some of the material presented in lectures may be more easily understood with more background. TThis course dives right into core Machine Learning topics. For this, students are expected to be fluent in Python programming. You can familiarize yourself with Python by completing online tutorials. Additionally, there are free Basics Python courses that cover all necessary python topics. You are also expected to be able to manipulate data using pandas DataFrames, perform basic operations with numpy arrays, and make use of basic plotting functions (e.g. line chart, histogram, scatter plot, bar chart) from matplotlib. Python basics Throughout this course, we will be using Python as our primary programming language for exercises, labs, and homework. Thus, you must have basic Python programming knowledge. The following are the topics you need to cross off your checklist before the course begins: Variables, Datatypes, strings, file operations, Data structures such as lists, dictionaries, tuples and classes. Pandas Basics Most of the exercises you will encounter in this course exploit various datasets. Pandas is an open-source data analysis and manipulation tool, built on top of Python. In this course, we have provided the necessary support material and resources to work with Pandas. However, it is highly recommended that you get yourselves familiar with basic data manipulation using Pandas to ensure a smooth learning experience. Numpy Basics NumPy is a library for the Python programming language that provides support for large, multi-dimensional arrays and matrices and a large collection of high-level mathematical functions to operate on these data structures. Because of the extensive exercises provided in this course, it is important to use Numpy for efficient problem-solving to get identical results. Though this course aims to support individuals with no prior Numpy knowledge, you must go through the basics of this library to avoid any possible hiccups. Matplotlib Basics A large portion of this course uses different graphs and charts to explain topics and validate results. Matplotlib is a plotting library for Python. This library has been used to create all the graphs you will see throughout the course. Additionally, the exercises and homeworks are structured in a manner that integrates this library. Henceforth, it is highly recommended to get yourselves acquainted with Matplotlib Basics. For this course, we will be using Jupyter Notebooks. You can familiarize yourself with Jupyter notebooks by reading the following tutorials: A Beginner's Tutorial to Jupyter Notebooks Finally, we assume that students have a strong foundation in calculus, linear algebra, statistics, and probability. You should review these concepts before the course begins. Here is one useful resource: Mathematics for Machine Learning INTRODUCTION TO ONLINE TEACHING To make the most of this course, we highly recommend you follow the given guidelines: Ensure you have a good internet connection. As the course is entirely online, you must have strong internet to avoid missing out on the lecture. This course is designed to have healthy interactions to understand the material. Hence, it is vital to have a good audio system. You should be able to listen to lectures as well as respond to questions during one. It is highly recommended that each of you have your video turned on during the lectures. Make sure your face is clearly visible with ambient lighting. Before each lecture, ensure you have a programming ready device. You should be able to write and execute code during the lecture. Additionally, make sure your device settings allow screen sharing. Consistency is key. Attend all the lectures, sections and office hours (if needed). Make sure you are on time for all of them. If possible, please try to avoid the use of mobile phones to attend lectures. More info can be found here .","tags":"pages","url":"pages/preparation.html"},{"title":"Projects","text":"Project Guidelines ProjectGuidelines.pdf Project Choices A - Crime in Boston B - Brazil's COVID Response C - COVID in the US D - Mortality Prediction & Interpretation E - Predicting the 2020 Election F - EA Sports - FIFA G - Police Violence in the US","tags":"pages","url":"pages/projects.html"},{"title":"Schedule","text":"Date (Mon) Lecture (Mon) Lecture (Wed) Lecture (Fri) Advanced Section (Wed) Assignment (R:Released Wed - D:Due Wed) 9/1 Lecture 1: What is Data Science? General introduction. Lecture 2: Data + RegEx R:HW0 9/7 No Lecture (Holiday) Lecture 3: Web Scraping + PANDAS Lecture 4: Advanced PANDAS R:HW1 - D:HW0 9/14 Lecture 5: kNN Regression and Linear Regression Lecture 6: Multi and Poly Regression Lecture 7: Modeling knn and Linear R with skleanr R:HW2 - D:HW1 9/21 Lecture 8: Basic Statistics for Data Science Lecture 9: Inference: Bootstrap and CI Lecture 10: Hypothesis Testing & Predictive CI Advanced Section 1: Linear Algebra and Hypothesis Testing R:HW3 - D:HW2 9/28 Lecture 11: Cross-Validation & Model Selection Lecture 12: Regularization: Ridge & Lasso Lecture 13: Estimation of regularization parameter; Hands on Advanced Section 2: Methods of regularization and their justifications Milestone 1 10/5 Lecture 14: Visualization for Communication Lecture 15: kNN classification and Logistic Regression I Lecture 16: Case Study 1 R: HW4 (Individual) - D: HW3 10/12 No Lecture (Holiday) Lecture 17: Logistic Regression II Lecture 18: Multi class Classification (introduce softmax) Advanced Section 3: Generalized Linear Models Milestone 2 10/19 Lecture 19: Dealing with missing data, imputation Lecture 20: PCA Lecture 21: PCA and missing with data; hands on Advanced Section 4: Mathematical Foundations of PCA R:HW5 - D:HW4 10/26 Lecture 22: Classification Trees Lecture 23: Regression Trees Bagging RF Lecture 24: Tuning hyperparameters R:HW6 - D:HW5 11/2 Lecture 25: Boosting Methods for Regression Lecture 26: Boosting Methods for Classification Lecture 27: Case Study 2 Advanced Section 5: Stacking and mixture of experts 11/9 Lecture 28: Neural Networks 1-Perceptron and MLP Lecture 29: Neural Networks 2- Anatomy of NN, design choices Lecture 30: Neural Netoworks 3- Design Choices II & Gradient Descent R:HW7 (Individual) - D:HW6 11/16 Lecture 31: Neural Networks 4 -Back Propagation, SGD Lecture 32: Regularization methods - Weight decay, data augmentation and dropout Lecture 33: Full worked example of regression and classification FFNN Advanced Section 6: Deeper into Solvers Milestone 3 11/23 Lecture 34: EthiCS No lecture (Thanksgiving) No lecture (Thanksgiving) R:HW8 - D:HW7 11/30 Lecture 35: Interpreting Prediction Models Lecture 36: Wrap-Up D: HW8 12/7 Reading Period 12/14 Finals Week","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"Introduction to Data Science (Fall 2020) CS 109a, AC 209a, Stat 121a, or CSCI E-109a Course Heads Pavlos Protopapas (SEAS), Kevin Rader (Statistics), & Chris Tanner (SEAS) Instructor: Eleni Kaxiras (SEAS) Lectures: Mon, Wed, Fri at 9am-10:15am and 3pm-4:15pm Sections: Fri 1:30-2:45 pm and Mon 8:30-9:45 pm. (identical material) [starts 9/11] Advanced Sections: Wed at 12pm [starts 9/23] Office Hours: (TBD) Format: Exclusively online, but we aim to foster enriching interactions and collaboration as much as possible. Prerequisites: You are expected to have programming experience at the level of CS 50 or above, and statistics knowledge at the level of Stat 100 or above (Stat 110 recommended). HW #0 is designed to test your knowledge on the prerequisites. Successful completion of this assignment will show that this course is suitable for you. HW #0 will not be graded but you are required to submit. Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course in data science. The course focuses on the analysis of messy, real-life data to perform predictions using statistical and machine learning methods. Throughout the semester, our content continuously centers around five key facets: 1. data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set; 2. data management ‐ accessing data quickly and reliably; 3. exploratory data analysis – generating hypotheses and building intuition; 4. prediction or statistical learning; and 5. communication – summarizing results through visualization, stories, and interpretable summaries. Only one of CS109a, AC209a, or STAT121a can be taken for credit. Students who have previously taken CS109, AC209, or STAT121 cannot take CS109A, AC 209A, or STAT121A for credit. Course Components The lectures will be live-streamed and can be accessed through the Zoom section on Canvas. Video recordings of the live stream will be made available within 24 hours after the event, and will be accessible from the Lecture Video section on Canvas. Lectures The class meets, virtually, three days a week for lectures (M, W, F). The same lecture will be given twice each day: once in the morning and again in the afternoon, to accommodate students in different time zones. Mondays and Wednesdays will be mostly lecture content with some hands-on coding, whereas Fridays will be the inverse (mostly hands-on coding). Attending and participating in lectures is a crucial component of learning the material presented in this course. What to expect A lecture will have the following pedagogy layout which will be repeated: Asynchronous pre-class exercises of approxmately 30 min. This will include, reading from the textbooks or other sources, watching videos to prepare you for the class. Approx. 10 minutes of Q&A regarding the pre-class exercises and/or review of homework and quiz questions. Live online instruction followed by a short Q/A session Hands-on exercises, on the ED platform. Sessions will help students develop the intuition for the core concepts, provide the necessary mathematical background, and provide guidance on technical details. Sessions will be accompanied by relevant examples to clarify key concepts and techniques. Quizzes At the end of each lecture, there will be a short, graded quiz that will cover the pre-class and in-class material; there will be no AC209a content in the quizzes. The quizzes will be available until the next lecture. 25% of the quizzes will be dropped from your grade. Exercises Lectures will include one or more coding exercises focused on the newly introduced material; there will be no AC209a content in the exercises. The exercises are short enough to be completed during the time allotted in lecture but they will remain available until the beginning of the following lecture to accomodate those who cannot attend in real time. 25% of the exercises will be dropped from your grade. Sections Lectures are supplemented by sections led by teaching fellows. There are two types of sections: Standard Sections : This will be a mix of review of material and practice problems similar to the HW. The material covered on Friday and Monday is identical. Advanced Sections The course will include advanced sections for 209a students and will cover a different topic per week. These are 75-min lectures and will cover advanced topics like the mathematical underpinnings of the methods seen in lecture and hands-on exercises, along with extensions of those methods. The material covered in the advanced sections is required for all AC209a students. Note: Sections are not held every week. Consult the course calendar for exact dates. Exams There are no exams in this course. Projects Students will work in groups of 2-4 to complete a final group project, due during the Exams period. See Calendar for specific dates. Homework Assignments There will be 9 graded homework assignments. Some of them will be due one week after being assigned, and some will be due two weeks after being assigned. You have the option to work and submit in pairs for all the assignments except HW4 and HW7, which you will do individually. You will be working in Jupyter Notebooks, which you can run in your own environment or in the SEAS JupyterHub cloud. Instructions for Setting up Your Environment Instructions for Using JupyterHub On weeks with new assignments, the assignments will be released by Wednesday 3pm. Standard assignments are graded out of 5 points. AC209a students will have additional homework content for most assignments worth 1 point. Instructor Office Hours Pavlos : (TBD) Kevin : (TBD) Chris : (TBD) Eleni : (TBD) Participation Students are expected to be actively engaged with the course. This includes: Attending and participating in lectures (or the follow-up session later in the day) Making use of resources such as office hours and sections Participating in the Ed discussion forum — both through asking thoughtful questions and by answering the questions of others Despite being remote, we aim to make this course as interactive, stimulating, and fun as always, and we rely on each of you to contribute your awesome uniqueness. Recommended Textbook An Introduction to Statistical Learning by James, Witten, Hastie, Tibshirani. The book is available here: Free electronic version : http://www-bcf.usc.edu/~gareth/ISL/ (Links to an external site). HOLLIS : http://link.springer.com.ezp-prod1.hul.harvard.edu/book/10.1007%2F978-1-4614-7138-7 Amazon: https://www.amazon.com/Introduction-Statistical-Learning-Applications-Statistics/dp/1461471370 (Links to an external site) . Deep Learning, Vol. 1 & 2: From Basics to Practice by Andrew Glassner Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville (MIT Press, 2016) Course Policies Getting Help For questions about homework, course content, package installation, JupyterHub, and after you have tried to troubleshoot yourselves, the process to get help is: 1. Post the question in Ed and get a response from your peers. Note that in Ed questions are visible to everyone. The teaching staff monitors the posts. 2. Go to Office Hours ; this is the best way to get direct help. 3. For private matters send an email to the Helpline: cs109a2020@gmail.com . The Helpline is monitored by the teaching staff. 4. For personal and confidential matters send an email to the instructors . Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit individual assignments, include the name of each other in the designated area of the submission. if you work with a fellow student and want to submit the same assignment, you need to form a group prior to the submission. Details in the assignment. Remember, not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator (e.g., not entirely from Google search results) you are welcome to take ideas from code presented in lecture or section, but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Wrongly Submitted Assignments There are no late days in homework submission. We will accept late submissions only for medical (if accompanied by a doctor's note) or other official University-excused reasons. To submit after Canvas has closed or to ask for an extension , send an email to the Helpline with subject line \"Submit HW1: Reason=the flu\" replacing 'HW1' with the name of the current assignment and \"the flu\" with your reason. You need to attach the note from your medical provider otherwise we will not accept the request. Email the instructors if you have other University-excused reasons. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Grading Guidelines Homework will be graded based on: 1. How correct your code is (the Notebook cells should run, we are not troubleshooting code) 2. How you have interpreted the results — we want text not just code. It should be a report. 3. How well you present the results. The scale is 0 to 5 for each assignment. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading , The points we take off are based on a grading rubric that is being applied uniformly to all submissions. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release . Zoom Expectations: Despite being remote and distributed, we want everyone to participate and engage with fellow classmates and staff. To this end, we request for everyone to leave his/her/their video on during class. We fully understand that some may be uncomfortable or unable to do so, for various personal reasons. We respect your decision and empathize for any situation that may arise. Nonetheless, if possible, we strongly ask for you to please leave your video on. Many are getting accustomed to Zoom for the first time. While it is understandable to have some glitches, mistakes, and faux pas, some mistakes can be largely disruptive to such a large audience like that of our class. Please familiarize yourself to Zoom before the semester begins. In particular, please: keep your video on (see above) keep your mic muted by default, until you speak be mindful of overtly distracting actions that your video may cause (this isn't a large issue, but it's akin to students' laptops distracting those who sit nearby) be mindful of your mic settings after returning to the main room from a break-out room, as it tends to un-mute each person's mic. Communication from Staff to Students Class announcements will be through Ed . All homework and will be posted and submitted through Canvas . Quizzes are completed on Ed as well as all feedback forms. NOTE: make sure you adjust your account settings so you can receive emails from Canvas. Submitting an assignment Please consult Homework Policies & Submission Instructions [Update link] Course Grade Your final score for the course will be computed using the following weights: Assignment Final Grade Weight Homework 0 1% Paired Homework (7) 42% Individual Homework (2) 20% Quizzes 6% Exercises 6% Project 25% Total 100% Software We will be using Jupyter Notebooks, Python 3, and various python modules. You can access the notebook viewer either on your own machine by installing the Anaconda platform (Links to an external site) which includes Jupyter/IPython as well all packages that will be required for the course, or by using the SEAS JupyterHub from Canvas. Details in class. Auditing the Class If you would like to audit the class, please send an email to the Helpline indicating who you are and why you want to audit the class. You need a HUID to be included to Canvas. Please note that auditors may not submit assignments for grading or make use of other limited student resources such as office hours. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109A we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. Accommodations for Students with Disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Diversity and Inclusion Statement Data Science and Computer Science have historically been representative of only a small sliver of the population. This is despite the contributions of a diverse group of early pioneers - see Ada Lovelace, Dorothy Vaughan, and Grace Hopper for just a few examples. As educators, we aim to build a diverse, inclusive, and representative community offering opportunities in data science to those who have been historically marginalized. We will encourage learning that advances ethical data science, exposes bias in the way data science is used, and advances research into fair and responsible data science. We need your help to create a learning environment that supports a diversity of thoughts, perspectives, and experiences, and honors your identities (including but not limited to race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please do not hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to us making a general announcement to the class, if necessary, to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion. We (like many people) are still learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. As a participant in course discussions, you are expected to respect your classmates' diverse backgrounds and perspectives. Our course will discuss diversity, inclusion, and ethics in data science. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions for how we can improve.","tags":"pages","url":"pages/syllabus.html"},{"title":"Lecture 36: Wrap-Up Review","text":"Slides PDF | Lecture 36: Wrap-Up Review PPTX | Lecture 36: Wrap-Up Review","tags":"lectures","url":"lectures/lecture36/"},{"title":"Lecture 35: Interpreting Prediction Models","text":"Slides PDF | Lecture 35: Interpreting Prediction Models Exercises Lecture 35: Exercise [Notebook]","tags":"lectures","url":"lectures/lecture35/"},{"title":"Lecture 33: Full Example of Regression & Classification FFNN","text":"CS-109A Introduction to Data Science Lecture 33 Notebook: Training a FFN Harvard University Fall 2020 Instructors: Pavlos Protopapas, Kevin Rader, Chris Tanner Authors: Eleni Kaxiras, David Sondak, and Pavlos Protopapas. (student edition) In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT CELLS import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: import numpy as np import matplotlib.pyplot as plt import matplotlib.image as mpimg import numpy as np import pandas as pd from keras.preprocessing.image import ImageDataGenerator , array_to_img , img_to_array , load_img from sklearn.preprocessing import StandardScaler % matplotlib inline from PIL import Image , ImageOps Using TensorFlow backend. In [3]: from __future__ import absolute_import , division , print_function , unicode_literals # TensorFlow and tf.keras import tensorflow as tf tf . keras . backend . clear_session () # For easy reset of notebook state. print ( tf . __version__ ) # You should see a 2.0.0 here! 2.0.0 In [4]: # set the seed for reproducability seed = 7 np . random . seed ( seed ) Tensorflow 2.0: All references to Keras should be written as tf.keras . For example: model = tf.keras.models.Sequential([ tf.keras.layers.Flatten(input_shape=(28, 28)), tf.keras.layers.Dense(128, activation='relu'), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10, activation='softmax') ]) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) tf.keras.models.Sequential tf.keras.layers.Dense, tf.keras.layers.Activation, tf.keras.layers.Dropout, tf.keras.layers.Flatten, tf.keras.layers.Reshape tf.keras.optimizers.SGD tf.keras.preprocessing.image.ImageDataGenerator tf.keras.regularizers tf.keras.datasets.mnist You could avoid the long names by using from tensorflow import keras from tensorflow.keras import layers These imports do not work on some systems, however, because they pick up previous versions of keras and tensorflow . Tensors We can think of tensors as multidimensional arrays of real numerical values; their job is to generalize matrices to multiple dimensions. scalar = just a number = rank 0 tensor ($a$ ∈ $F$,) vector = 1D array = rank 1 tensor ( $x = (\\;x_1,...,x_i\\;)⊤$ ∈ $F&#94;n$ ) matrix = 2D array = rank 2 tensor ( $\\textbf{X} = [a_{ij}] ∈ F&#94;{m×n}$ ) 3D array = rank 3 tensor ( $\\mathscr{X} =[t_{i,j,k}]∈F&#94;{m×n×l}$ ) First you build the network The input layer : our dataset. The internal architecture or hidden layers the number of layers, the activation functions, the learnable parameters and other hyperparameters) The output layer : what we want from the network, a probability for belonging in a class (classification) or a number (regression). Load and pre-process the data Define the layers of the model. Compile the model. ... and then you train it! Fit the model to the train set (also using a validation set). Save the model. Evaluate the model on the test set. We learn a lot by studying history: metric traceplots. Regularize the model. Now let's use the Network to predict on the test set. Try our model on a sandal from the Kanye West collection! 1. Load the data Fashion MNIST Fashion-MNIST is a dataset of clothing article images (created by Zalando ), consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28 x 28 grayscale image, associated with a label from 10 classes . The creators intend Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. Each pixel is 8 bits so its value ranges from 0 to 255. These images do not have a channel dimension because they are B&W. Let's load and look at it! In [0]: # get the data from keras fashion_mnist = tf . keras . datasets . fashion_mnist # load the data splitted in train and test! ( x_train , y_train ),( x_test , y_test ) = fashion_mnist . load_data () print ( x_train . shape , y_train . shape , ' \\n\\n ' , x_train [ 56 ][: 2 ], ' \\n\\n ' , set ( y_train )) In [0]: # checking the min and max of x_train and x_test print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) In [0]: # normalize the data by dividing with pixel intensity # (each pixel is 8 bits so its value ranges from 0 to 255) x_train , x_test = x_train / 255.0 , x_test / 255.0 print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) In [0]: # inspect a single image array print ( x_train [ 45 ] . shape ) print ( x_train [ 45 ][: 2 ][: 2 ]) In [0]: # Give names to classes for clarity class_names = [ 'T-shirt/top' , 'Trouser' , 'Pullover' , 'Dress' , 'Coat' , 'Sandal' , 'Shirt' , 'Sneaker' , 'Bag' , 'Ankle boot' ] # plot plt . figure ( figsize = ( 10 , 10 )) for i in range ( 25 ): plt . subplot ( 5 , 5 , i + 1 ) plt . xticks ([]) plt . yticks ([]) plt . grid ( False ) plt . imshow ( x_train [ i ], cmap = plt . cm . binary ) plt . xlabel ( class_names [ y_train [ i ]]) plt . show () 2. Define the layers of the model. In [0]: model = tf . keras . Sequential () model . add ( tf . keras . layers . Flatten ( input_shape = ( 28 , 28 ))) model . add ( tf . keras . layers . Dense ( 154 , activation = 'relu' )) model . add ( tf . keras . layers . Dense ( 154 , activation = 'relu' )) model . add ( tf . keras . layers . Dense ( 154 , activation = 'relu' )) model . add ( tf . keras . layers . Dense ( 64 , activation = 'relu' )) model . add ( tf . keras . layers . Dense ( 10 , activation = 'softmax' )) 3. Compile the model In [0]: loss_fn = tf . keras . losses . SparseCategoricalCrossentropy () optimizer = tf . keras . optimizers . Adam () model . compile ( optimizer = optimizer , loss = loss_fn , metrics = [ 'accuracy' ]) In [0]: model . summary () In [0]: tf . keras . utils . plot_model ( model , #to_file='model.png', # if you want to save the image show_shapes = True , # True to see more details show_layer_names = True , rankdir = 'TB' , expand_nested = True , dpi = 150 ) Everything you wanted to know about a Keras Model and were afraid to ask 4. Fit the model to the train set (also using a validation set) This is the part that takes the longest. ep·och noun: epoch; plural noun: epochs. A period of time in history or a person's life, typically one marked by notable events or particular characteristics. Examples: \"the Victorian epoch\", \"my Neural Netwok's epochs\". In [0]: %%time # Fit the model <--- always a good idea to time it! history = model . fit ( x_train , y_train , validation_split = 0.33 , epochs = 50 , verbose = 2 ) Save the model You can save the model so you do not have .fit everytime you reset the kernel in the notebook. Network training is expensive! For more details on this see https://www.tensorflow.org/guide/keras/save_and_serialize In [0]: # save the model so you do not have to run the code everytime model . save ( 'fashion_model.h5' ) # Recreate the exact same model purely from the file #model = tf.keras.models.load_model('fashion_model.h5') 5. Evaluate the model on the test set. In [0]: test_loss , test_accuracy = model . evaluate ( x_test , y_test , verbose = 0 ) print ( f 'Test accuracy= { test_accuracy } ' ) 6. We learn a lot by studying history: metric traceplots. You can learn a lot about neural networks by observing how they perform while training. The networks's performance is stored in a variable named history which can be plotted. In [0]: print ( history . history . keys ()) In [0]: # plot accuracy and loss for the test set fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) ax [ 0 ] . plot ( history . history [ 'accuracy' ]) ax [ 0 ] . plot ( history . history [ 'val_accuracy' ]) ax [ 0 ] . set_title ( 'Model accuracy' ) ax [ 0 ] . set_ylabel ( 'accuracy' ) ax [ 0 ] . set_xlabel ( 'epoch' ) ax [ 0 ] . legend ([ 'train' , 'val' ], loc = 'best' ) ax [ 1 ] . plot ( history . history [ 'loss' ]) ax [ 1 ] . plot ( history . history [ 'val_loss' ]) ax [ 1 ] . set_title ( 'Model loss' ) ax [ 1 ] . set_ylabel ( 'loss' ) ax [ 1 ] . set_xlabel ( 'epoch' ) ax [ 1 ] . legend ([ 'train' , 'val' ], loc = 'best' ) What do you observe in these traceplots? Breakout Room Activity 7. Regularization Let's try adding a regularizer in our model. Name your new model model_regular . For more see tf.keras regularizers . Norm penalties: model_regular.add(tf.keras.layers.Dense(154, activation='relu', kernel_regularizer= tf.keras.regularizers.l2(l=0.001))) Early stopping via tf.keras.callbacks . Callbacks provide a way to interact with the model while it's training and inforce some decisions automatically. Callbacks need to be instantiated and are added to the .fit() function via the callbacks argument. # callbacks: watch validation loss and be \"patient\" for 10 epochs of no improvement es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=20) model_regular.fit(x_train, y_train, validation_split=0.2, epochs=50, verbose=1, callbacks=[es]) Dropout model_regular.add(tf.keras.layers.Dropout(0.2)) Remove layers Note: Name your new model model_regular for the rest of the code to work. In [0]: # your code here # define the model In [0]: # compile the model In [0]: # fit the model In [0]: # evaluate on test set test_loss_regular , test_accuracy_regular = model_regular . evaluate ( x_test , y_test , verbose = 0 ) print ( f 'Test accuracy: \\n Baseline model= { test_accuracy : .4f } \\n Regularized model= { test_accuracy_regular : .4f } ' ) In [0]: # plot accuracy and loss for the test set fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) ax [ 0 ] . plot ( history_regular . history [ 'accuracy' ]) ax [ 0 ] . plot ( history_regular . history [ 'val_accuracy' ]) ax [ 0 ] . set_title ( 'Model accuracy' ) ax [ 0 ] . set_ylabel ( 'accuracy' ) ax [ 0 ] . set_xlabel ( 'epoch' ) ax [ 0 ] . legend ([ 'train' , 'val' ], loc = 'best' ) ax [ 1 ] . plot ( history_regular . history [ 'loss' ]) ax [ 1 ] . plot ( history_regular . history [ 'val_loss' ]) ax [ 1 ] . set_title ( 'Model loss' ) ax [ 1 ] . set_ylabel ( 'loss' ) ax [ 1 ] . set_xlabel ( 'epoch' ) ax [ 1 ] . legend ([ 'train' , 'val' ], loc = 'best' ) In class discussion : How far did you get with regularization? 7. Now let's predict in the test set In [0]: predictions = model_regular . predict ( x_test ) In [0]: # choose a specific item to predict (one out of the 60000 samples) item = 6 In [0]: predictions [ item ] In [0]: np . argmax ( predictions [ item ]), class_names [ np . argmax ( predictions [ item ])] Let's see if our network predicted right! Is the first item what was predicted? In [0]: plt . figure () plt . imshow ( x_test [ item ], cmap = plt . cm . binary ) plt . xlabel ( class_names [ y_test [ item ]]) plt . colorbar () In [0]: # code source: https://www.tensorflow.org/tutorials/keras/classification def plot_image ( i , predictions_array , true_label , img ): predictions_array , true_label , img = predictions_array , true_label [ i ], img [ i ] plt . grid ( False ) plt . xticks ([]) plt . yticks ([]) plt . imshow ( img , cmap = plt . cm . binary ) predicted_label = np . argmax ( predictions_array ) if predicted_label == true_label : color = 'blue' else : color = 'red' plt . xlabel ( \" {} {:2.0f} % ( {} )\" . format ( class_names [ predicted_label ], 100 * np . max ( predictions_array ), class_names [ true_label ]), color = color ) def plot_value_array ( i , predictions_array , true_label ): predictions_array , true_label = predictions_array , true_label [ i ] plt . grid ( False ) plt . xticks ( range ( 10 )) plt . yticks ([]) thisplot = plt . bar ( range ( 10 ), predictions_array , color = \"#777777\" ) plt . ylim ([ 0 , 1 ]) predicted_label = np . argmax ( predictions_array ) thisplot [ predicted_label ] . set_color ( 'red' ) thisplot [ true_label ] . set_color ( 'blue' ) In [0]: i = item plt . figure ( figsize = ( 6 , 3 )) plt . subplot ( 1 , 2 , 1 ) plot_image ( i , predictions [ i ], y_test , x_test ) plt . subplot ( 1 , 2 , 2 ) plot_value_array ( i , predictions [ i ], y_test ) plt . show () 8. Try our model on a sandal from the Kanye West collection! The true usefullness of a NN is to be able to classigy unseen data and not only on the test set. Let's see if our network can generalize beyond the MNIST fashion dataset. Let's give it a trendy shoe and see what it predicts. This image is not part of the test set, it was downloaded from the internet. In class discussion : What kinds of images can our model predict? Bias measures how much the network's output, averaged over all possible data sets differs from the true function. Variance measures how much the network output varies between datasets. Is generalization the pillar of Intelligence? In [5]: # Let'see the tensor shape shoe = Image . open ( 'kanye_shoe.jpg' ) imgplot = plt . imshow ( shoe ) In [0]: # Resize image to 28x28 shoe = shoe . resize (( 28 , 28 )) imgplot = plt . imshow ( shoe ) In [0]: shoe = ImageOps . mirror ( shoe ) imgplot = plt . imshow ( shoe ) In [0]: # Delete the other 2 channels to make image B&W. shoe_data = np . array ( shoe ) # cast into munpy array shoe_data = shoe_data [:,:, 0 ] print ( shoe_data . shape ) imgplot = plt . imshow ( shoe_data , cmap = plt . cm . binary ) tf.keras models are optimized to make predictions on a batch, or collection, of examples at once. Accordingly, even though you're using a single image, you need to add it to a list: In [0]: # Add the image to a batch where it's the only member. shoe_batch = ( np . expand_dims ( shoe_data , 0 )) print ( shoe_batch . shape ) In [0]: predictions_single = model_regular . predict ( shoe_batch ) print ( predictions_single [ 0 ]) print ( np . argmax ( predictions_single [ 0 ]), class_names [ np . argmax ( predictions_single [ 0 ])]) In [0]: shoe_data = np . ones ( shoe_data . shape ) * 255 - shoe_data plt . figure () plt . imshow ( shoe_data , cmap = plt . cm . binary ) plt . xlabel ( 'a cool shoe' ) plt . colorbar () In [0]: # Add the image to a batch where it's the only member. shoe_batch = ( np . expand_dims ( shoe_data , 0 )) print ( shoe_batch . shape ) In [0]: predictions_single = model_regular . predict ( shoe_batch ) print ( predictions_single [ 0 ]) print ( np . argmax ( predictions_single [ 0 ]), class_names [ np . argmax ( predictions_single [ 0 ])]) Data augementation Data augmentation generates more training data by applying a series of random transformations that yield belivable images. Be careful of transformations that result in unlikely images. In [0]: datagen = ImageDataGenerator ( rotation_range = 40 , width_shift_range = 0.2 , height_shift_range = 0.2 , rescale = 1. / 255 , shear_range = 0.2 , zoom_range = 0.2 , horizontal_flip = True , fill_mode = 'nearest' ) In [0]: shoe_augm = ( np . expand_dims ( shoe_data , 0 )) shoe_augm = ( np . expand_dims ( shoe_augm , 3 )) print ( shoe_augm . shape ) In [0]: img = shoe_augm img . shape In [0]: from keras.preprocessing import image # the .flow() command below generates batches of randomly transformed images # and saves the results to the `preview/` directory (this directory must exist) i = 0 for batch in datagen . flow ( shoe_augm , batch_size = 1 , save_to_dir = 'preview' , save_prefix = 'shoe' , save_format = 'jpeg' ): plt . figure ( i ) imgplot = plt . imshow ( image . array_to_img ( batch [ 0 ]), cmap = plt . cm . binary ) i += 1 if i % 6 == 0 : break # otherwise the generator would loop indefinitely plt . show ()","tags":"labs","url":"labs/lecture-33/notebook/"},{"title":"Lecture 33: Full Example of Regression & Classification FFNN","text":"Slides PDF | Lecture 33: Review of NNs PPTX | Lecture 33: Review of NNs PDF | Lecture 33: Dropout PPTX | Lecture 33: Dropout Notebook Public Link to Google Colab Notebook for Lecture 33 Public Link to kanye_shoe.jpg, a photo of a very important shoe Exercises Lecture 33: Exercise 1 [Notebook]","tags":"lectures","url":"lectures/lecture33/"},{"title":"S-Section 10: Feed Forward Neural Networks: Regularization and Adam optimizer","text":"CS109A Introduction to Data Science Standard Section 10: Feed Forward Neural Networks: Regularization and Adam optimizer Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Henry Jin, Hayden Joy In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: In this section is we will learn methods for improving the network optimization. Essentially, the goal is to become familiar with regularization methods and with the optimizers used in training neural networks (NNs). Specifically, we will: Use NNs to solve a regression task where polynomial regression fails Fit noise data and observe underfitting and overfitting Learn about early-stopping and regularization: $L_1$, $L_2$, and dropout Explore the Adam optimizer Improve the classifier network used in section 9 Import packages In [2]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras import optimizers from tensorflow.keras import regularizers from tensorflow.keras import callbacks from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error , r2_score from sklearn.preprocessing import PolynomialFeatures import copy import operator from numpy.random import seed seed ( 123 ) In [3]: tf . random . set_seed ( 256 ) Helper functions: For plotting train set, test set, neural network predictions For plotting the training and validation loss functions In [4]: def array_exists ( arr ): return hasattr ( arr , 'shape' ) #check if the numpy array exists def reshape_if_exists ( arr ): if array_exists ( arr ): return arr [ 'x' ] . values . reshape ( - 1 , 1 ), arr [ 'y' ] . values . reshape ( - 1 , 1 ) else : return None , None def reshape_and_extract_sets ( train_set , test_set ): \"\"\" Extracts x_train, y_train, x_test and y_test and reshapes them for using with keras. \"\"\" x_train , y_train = reshape_if_exists ( train_set ) x_test , y_test = reshape_if_exists ( test_set ) return x_train , y_train , x_test , y_test def plot_sets ( train_set = None , test_set = None , NN_model = None , title = None ): \"\"\" plots the train set, test set, and Neural network model's predictions if entered Arguments: train_set : The training set points to plot test_set : The test set points to plot NN_model : if you enter a model this function will plot the predictions title : if you enter a title, it will display on the plot. \"\"\" x_train , y_train , x_test , y_test = reshape_and_extract_sets ( train_set , test_set ) plt . figure ( figsize = ( 12 , 5 )) if array_exists ( train_set ): plt . plot ( x_train , y_train , 'og' , alpha = 0.8 , label = 'train data' ) if array_exists ( test_set ): plt . plot ( x_test , y_test , '&#94;r' , alpha = 0.8 , label = 'test data' ) # if the neural network model was provided, plot the predictions. if type ( NN_model ) != type ( None ): xx = np . linspace ( np . min ( x_train ), np . max ( x_train ), num = 1000 ) NN_preds = NN_model . predict ( xx . reshape ( - 1 , 1 )) plt . plot ( xx , NN_preds , '-b' , linewidth = 2 , label = 'FFNN' , alpha = 0.7 ) NN_preds_test = NN_model . predict ( x_test . reshape ( - 1 , 1 )) r2 = r2_score ( NN_preds_test , y_test ) if title : if NN_model : plt . title ( title + \", $R&#94;2$ score = {} \" . format ( round ( r2 , 4 ))) else : plt . title ( title ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y(x)\" ) plt . legend () if title : if \"Lorentz\" in title or \"lorentz\" in title : lorentz_labels () plt . show () def plot_loss ( model_history , rolling = None , title = \"Loss vs Epoch \" ): \"\"\" Arguments: model_history : the nueral network model history to plot title : if you want a title other than the default plot it. rolling : this will plot a rolling average of the loss (purely for visualization purposes) \"\"\" plt . figure ( figsize = ( 12 , 5 )) train_loss = model_history . history [ 'loss' ] val_loss = model_history . history [ 'val_loss' ] set_colors = { \"train\" : sns . color_palette ()[ 0 ], \"val\" : sns . color_palette ()[ 1 ]} if rolling : alphas = [ 0.45 , 0.35 ] else : alphas = [ 0.8 , 0.6 ] plt . loglog ( train_loss , linewidth = 3 , label = 'Training' , alpha = alphas [ 0 ], color = set_colors [ \"train\" ]) plt . loglog ( val_loss , linewidth = 3 , label = 'Validation' , color = set_colors [ \"val\" ], alpha = alphas [ 1 ]) if rolling : plt . plot ( pd . Series ( train_loss ) . rolling ( rolling ) . mean (), linewidth = 4 , label = 'Train loss rolling avg.' , color = set_colors [ \"train\" ]) plt . plot ( pd . Series ( val_loss ) . rolling ( rolling ) . mean (), linewidth = 4 , label = 'Val loss rolling avg.' , color = set_colors [ \"val\" ]) plt . title ( title ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . legend () plt . show () def lorentz_labels (): \"\"\" chage axis labels and set ylim for lorentz problem. \"\"\" plt . xlabel ( '$\\omega$' ) plt . ylabel ( '$\\epsilon$' ) plt . legend () plt . ylim ([ - 9 , 9 ]); 1. Regression: Neural Nets VS Polynomial Regression Let's fit a difficult function where polynomial regression fails. The dielectric function of many optical materials depends on the frequency and is given by the Lorentz model as: $$ \\varepsilon(\\omega) = 1 - \\frac{\\omega_0&#94;2}{\\omega_0&#94;2-\\omega&#94;2 +i\\omega\\Gamma},$$ where $\\omega$ is the frequency, $\\omega_0$ is the resonance frequency of the bound electrons, and $\\Gamma$ is the electron damping. In many situations, we measure the real part of the dielectric function in the lab and then we fit these observations. Let's assume that we perform an experiment and the observations came from a Lorentz model. Lorentz model In [5]: lor_set = pd . read_csv ( '../data/lorentz_set.csv' ) lor_train , lor_test = train_test_split ( lor_set , train_size = 0.7 , random_state = 109 ) x_train , y_train , x_test , y_test = reshape_and_extract_sets ( lor_train , lor_test ) plot_sets ( lor_train , lor_test , title = \"Lorentz equation data\" ) In [6]: x_train [:, 0 ] . shape Out[6]: (89,) Using polynomial regression to fit the data In [7]: x = copy . copy ( x_train ) y = copy . copy ( y_train ) xTest = copy . copy ( x_test ) yTest = copy . copy ( y_test ) polynomial_features = PolynomialFeatures ( degree = 25 ) x_poly = polynomial_features . fit_transform ( x ) x_poly_test = polynomial_features . fit_transform ( xTest ) model = LinearRegression () model . fit ( x_poly , y ) y_poly_train = model . predict ( x_poly ) y_poly_test = model . predict ( x_poly_test ) mse_train_poly = mean_squared_error ( y , y_poly_train ) mse_test_poly = mean_squared_error ( yTest , y_poly_test ) print ( 'MSE on training set: ' , mse_train_poly ) print ( 'MSE on testing set: ' , mse_test_poly ) y_poly_pred = model . predict ( x_poly ) plt . figure ( figsize = ( 12 , 5 )) plt . plot ( x , y , 'ob' , label = 'train data' ) # sort the values of x before line plot sort_axis = operator . itemgetter ( 0 ) sorted_zip = sorted ( zip ( x , y_poly_train ), key = sort_axis ) x , y_poly_train = zip ( * sorted_zip ) plt . plot ( x , y_poly_train , color = 'm' , linewidth = 2 , label = 'polynomial model train' ) plt . title ( \"The Lorentz Equation: Polynomial Fit, $R&#94;2$ score = {} \" . format ( round ( r2_score ( y , y_poly_pred ), 4 ))) lorentz_labels () MSE on training set: 1.1882431815719614 MSE on testing set: 1.6884109651402155 Using a Neural Network Design the Network In [8]: model_1 = models . Sequential ( name = 'LorentzModel' ) # hidden layer model_1 . add ( layers . Dense ( 50 , activation = 'tanh' , input_shape = ( 1 ,))) #second hidden layer model_1 . add ( layers . Dense ( 50 , activation = 'tanh' )) # output layer, one neuron model_1 . add ( layers . Dense ( 1 , activation = 'linear' )) model_1 . summary () Model: \"LorentzModel\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 50) 100 _________________________________________________________________ dense_1 (Dense) (None, 50) 2550 _________________________________________________________________ dense_2 (Dense) (None, 1) 51 ================================================================= Total params: 2,701 Trainable params: 2,701 Non-trainable params: 0 _________________________________________________________________ Select a solver and train the NN In [9]: %%time # optimizer = optimizers.SGD(lr=0.01, momentum=0.9) optimizer = optimizers . Adam ( lr = 0.01 ) model_1 . compile ( loss = 'MSE' , optimizer = optimizer ) history_1 = model_1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 1000 , batch_size = 32 , verbose = 0 ) CPU times: user 35.4 s, sys: 1.91 s, total: 37.3 s Wall time: 36.5 s Plot the training and validation loss In [10]: # Use the helper function plot_loss ( history_1 , rolling = 30 ) Visualize the model prediction In [11]: # Use the helper function plot_sets ( lor_train , lor_test , NN_model = model_1 , title = \"Lorentz vanilla FFNN\" ) 2. Noisy data, underfitting and overfitting In real-data always contain some noise Hence, in a real experiment, the observations will follow a form of $$ \\varepsilon(\\omega) = 1 - \\frac{\\omega_0&#94;2}{\\omega_0&#94;2-\\omega&#94;2 +i\\omega\\Gamma} + \\epsilon,$$ where, $\\epsilon$ is Gaussian (white) noise. Our goal is to discover the underlying law, namely the Lorentz model, by using neural networks. Read, split, and plot the data: In [62]: lor_set_n = pd . read_csv ( '../data/lorentz_noise_set2.csv' ) lor_train_n , lor_test_n = train_test_split ( lor_set_n , train_size = 0.7 , random_state = 109 ) x_train , y_train , x_test , y_test = reshape_and_extract_sets ( lor_train_n , lor_test_n ) plot_sets ( lor_train_n , lor_test_n , title = \"Noisy Lorentz Data\" ) Discover the underlying function Define some hyperparameter In [13]: n_neurons = 50 optimizer = optimizers . Adam ( lr = 0.01 ) In [14]: model_2 = models . Sequential ( name = 'noiseLorentzModel' ) # first hidden layer model_2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , input_shape = ( 1 ,))) # second hidden layer model_2 . add ( layers . Dense ( n_neurons , activation = 'tanh' )) # output layer, one neuron model_2 . add ( layers . Dense ( 1 , activation = 'linear' )) model_2 . summary () Model: \"noiseLorentzModel\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_3 (Dense) (None, 50) 100 _________________________________________________________________ dense_4 (Dense) (None, 50) 2550 _________________________________________________________________ dense_5 (Dense) (None, 1) 51 ================================================================= Total params: 2,701 Trainable params: 2,701 Non-trainable params: 0 _________________________________________________________________ In [15]: %%time model_2 . compile ( loss = 'MSE' , optimizer = optimizer ) history_2 = model_2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 500 , batch_size = 32 , verbose = 0 ) CPU times: user 18.8 s, sys: 1.01 s, total: 19.8 s Wall time: 19.3 s In [16]: plot_loss ( history_2 , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2 , title = \"Noisy Lorentz: Vanilla FFNN\" ) Underfitting We use the same architecture but we train less In [17]: %%time model_2_uf = models . Sequential ( name = 'noiseLorentzModel_underFitting' ) # first hidden layer model_2_uf . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , input_shape = ( 1 ,))) # second hidden layer model_2_uf . add ( layers . Dense ( n_neurons , activation = 'tanh' )) # output layer, one neuron model_2_uf . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_uf . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_uf = model_2_uf . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = 80 , batch_size = 32 , verbose = 0 ) CPU times: user 4.1 s, sys: 181 ms, total: 4.28 s Wall time: 4.2 s In [18]: plot_loss ( history_2_uf ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_uf , title = \"Noisy Lorentz FFNN underfit\" ) Overfitting Train for long time. Change a hyperparameter: In [19]: epochs_max = 2000 In [20]: model_2_of = models . Sequential ( name = 'noiseLorentzModel_overFitting' ) # first hidden layer model_2_of . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , input_shape = ( 1 ,))) # second hidden layer model_2_of . add ( layers . Dense ( n_neurons , activation = 'tanh' )) # output layer, one neuron model_2_of . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_of . compile ( loss = 'MSE' , optimizer = optimizer ) In [21]: %%time history_2_of = model_2_of . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) CPU times: user 1min 10s, sys: 3.72 s, total: 1min 13s Wall time: 1min 13s In [22]: plot_loss ( history_2_of , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_of , title = \"Noisy Lorentz FFNN overfit\" ) Regularization The easiest way to avoid overfitting is the early-stopping method, namely stop the training when the validation loss is minimum. A more systematic method is given by regularization . Early-stopping does not change the model but regularization does change the model since we change the loss function. Two common regularization methods are the so-called $L_1$ and $L_2$. $L_1$ tries to minimize the number of network parameters and reduces the model complexity . In other words, it is trying to have as many zero parameters as it is possible. $L_1$ wants the smallest number of parameters . $L_2$ tries to minimize the value of all the parameters and gives a more stable network. So, it does not care about the number of the non-zero parameters but it cares about their values. $L_2$ wants the smallest values of parameters . Warning! In the extreme limit of too large regularization coefficients both $L_1$ and $L_2$ lead to zero parameters. Over-regularization yields underfitting . Dropout Another method of regularizing a neural network is called 'dropout'. In dropout, we randomly 'drop' some neurons of the neural network with probability p during training. This forces the network to distribute the weights across its neurons, preventing it from becoming overly dependent on any particular subset of neurons. Early stopping In [23]: %%time callback = callbacks . EarlyStopping ( monitor = 'val_loss' , patience = 55 ) model_2_estop = models . Sequential ( name = 'noiseLorentzModel_earlyStopping' ) # first hidden layer model_2_estop . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , input_shape = ( 1 ,))) # second hidden layer model_2_estop . add ( layers . Dense ( n_neurons , activation = 'tanh' )) # output layer, one neuron model_2_estop . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_estop . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_estop = model_2_estop . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , callbacks = [ callback ], batch_size = 32 , verbose = 0 ) CPU times: user 9.34 s, sys: 389 ms, total: 9.73 s Wall time: 9.29 s In [24]: plot_loss ( history_2_estop , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_estop , title = \"Noisy Lorentz FFNN early stoping\" ) $L_1$ Regularizer In [25]: kernel_weight = 0.005 bias_weight = 0.005 model_2_l1 = models . Sequential ( name = 'noiseLorentzModel_l1' ) # first hidden layer model_2_l1 . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l1 ( kernel_weight ), bias_regularizer = regularizers . l1 ( bias_weight ) )) # second hidden layer model_2_l1 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l1 ( kernel_weight ))) # output layer, one neuron model_2_l1 . add ( layers . Dense ( 1 , activation = 'linear' )) In [26]: %%time model_2_l1 . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_l1 = model_2_l1 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) CPU times: user 1min 14s, sys: 3.97 s, total: 1min 18s Wall time: 1min 16s In [27]: plot_loss ( history_2_l1 , rolling = 30 ) l1_args = { \"NN_model\" : model_2_l1 , \"title\" : \"Noisy Lorentz FFNN L1 regularization\" } plot_sets ( lor_train_n , lor_test_n , ** l1_args ) $L_2$ Regularizer In [28]: kernel_weight = 0.005 bias_weight = 0.005 model_2_l2 = models . Sequential ( name = 'noiseLorentzModel_l2' ) # first hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight ) )) # second hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( kernel_weight ))) # output layer, one neuron model_2_l2 . add ( layers . Dense ( 1 , activation = 'linear' )) In [29]: %%time model_2_l2 . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_l2 = model_2_l2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) CPU times: user 1min 16s, sys: 4.27 s, total: 1min 20s Wall time: 1min 20s In [30]: plot_loss ( history_2_l2 , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_l2 , title = \"Noisy Lorentz FFNN L2 regularization\" ) Regularizing with Dropout In [31]: kernel_weight = 0.005 bias_weight = 0.005 model_drop = models . Sequential ( name = 'noiseLorentzModel_drop' ) model_drop . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l1 ( kernel_weight ), bias_regularizer = regularizers . l1 ( bias_weight ) )) # dropout for first hidden layer model_drop . add ( layers . Dropout ( 0.3 )) # hidden layer model_drop . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l1 ( kernel_weight ) )) # output layer, one neuron model_drop . add ( layers . Dense ( 1 , activation = 'linear' )) In [32]: %%time model_drop . compile ( loss = 'MSE' , optimizer = optimizer ) history_drop = model_drop . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) CPU times: user 1min 21s, sys: 4.39 s, total: 1min 26s Wall time: 1min 26s In [33]: plot_loss ( history_drop , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_drop , title = \"Lorentz FFNN w/ Dropout\" ) Underfitting through Regularization In [34]: kernel_weight_Large = . 1 bias_weight_Large = . 1 model_2_l2_uf = models . Sequential ( name = 'noiseLorentzModel_l2_uf' ) # first hidden layer model_2_l2_uf . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l2 ( kernel_weight_Large ), bias_regularizer = regularizers . l2 ( bias_weight_Large ) )) # second hidden layer model_2_l2_uf . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( kernel_weight_Large ))) # output layer, one neuron model_2_l2_uf . add ( layers . Dense ( 1 , activation = 'linear' )) In [35]: %%time model_2_l2_uf . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_l2_uf = model_2_l2_uf . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) CPU times: user 1min 18s, sys: 4.34 s, total: 1min 23s Wall time: 1min 22s In [36]: plot_loss ( history_2_l2_uf ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_l2_uf , title = \"Noisy Lorentz FFNN reg. underfit\" ) Inspect the weights In [37]: weights = model_2_l2_uf . layers [ 1 ] . get_weights ()[ 0 ] plt . hist ( weights . flatten (), bins = 50 ); Break Out Room 1 Prevent the training from overfitting Overfit the given data below. Plot the loss for the overfit neural network Regularize all the layers of this model using either L1 or L2. Find a good set of regularization coefficients Plot the losses for the regularized neural network Note: Use the functions we have declared above for plotting the loss and the neural network output In [38]: r1_set = pd . read_csv ( '../data/breakRoom1.csv' ) r1_train , r1_test = train_test_split ( r1_set , train_size = 0.7 , random_state = 109 ) x_train1 , y_train1 , x_test1 , y_test1 = reshape_and_extract_sets ( r1_train , r1_test ) plot_sets ( r1_train , r1_test ) In [39]: epochs_max1 = 2000 n_neurons1 = 50 model_br1 = models . Sequential ( name = 'br1_overfit' ) # first hidden layer model_br1 . add ( layers . Dense ( n_neurons1 , activation = 'tanh' , kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , input_shape = ( 1 ,))) # second hidden layer model_br1 . add ( layers . Dense ( n_neurons1 , activation = 'tanh' )) # third hidden layer model_br1 . add ( layers . Dense ( n_neurons1 , activation = 'tanh' )) # output layer, one neuron model_br1 . add ( layers . Dense ( 1 , activation = 'linear' )) adam = optimizers . Adam ( lr = 0.01 ) model_br1 . compile ( loss = 'MSE' , optimizer = adam ) In [40]: %%time history_br1 = model_br1 . fit ( x_train1 , y_train1 , validation_data = ( x_test1 , y_test1 ), epochs = epochs_max1 , batch_size = 32 , verbose = 0 ) CPU times: user 1min 21s, sys: 4.4 s, total: 1min 26s Wall time: 1min 25s In [41]: plot_loss ( history_br1 ) plot_sets ( r1_train , r1_test , NN_model = model_br1 , title = \"BreakRoom1_noRegularization\" ) Regularize the network In [42]: ## Your code here In [15]: # %load '../solutions/sol_1.py' Explore the Adam optimizer We explore method to improve the training. Learning rate Momentum Number of minibatches Learning Rate Baseline model In [44]: %%time kernel_weight = 0.003 bias_weight = 0.003 model_2_l2 = models . Sequential ( name = 'noiseLorentzModel_l2_baseline' ) # first hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight ) )) # second hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( kernel_weight ))) # output layer, one neuron model_2_l2 . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_l2 . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_l2 = model_2_l2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) plot_loss ( history_2_l2 , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_l2 , title = \"Noisy Lorentz FFNN L2 regularization\" ) CPU times: user 1min 34s, sys: 5.07 s, total: 1min 40s Wall time: 1min 45s Too large of a learning rate In [45]: optimizer_large_lr = optimizers . Adam ( lr = 0.3 ) kernel_weight = 0.003 bias_weight = 0.003 model_2_l2 = models . Sequential ( name = 'noiseLorentzModel_l2_large_lr' ) # first hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight ) )) # second hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( kernel_weight ))) # output layer, one neuron model_2_l2 . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_l2 . compile ( loss = 'MSE' , optimizer = optimizer_large_lr ) history_2_l2 = model_2_l2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) plot_loss ( history_2_l2 , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_l2 , title = \"Noisy Lorentz FFNN L2 regularization\" ) Too small learning rate In [63]: optimizer_small_lr = optimizers . Adam ( lr = 0.001 ) kernel_weight = 0.003 bias_weight = 0.003 model_2_l2 = models . Sequential ( name = 'noiseLorentzModel_l2_small_lr' ) # first hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight ) )) # second hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( kernel_weight ))) # output layer, one neuron model_2_l2 . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_l2 . compile ( loss = 'MSE' , optimizer = optimizer_small_lr ) history_2_l2 = model_2_l2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = 32 , verbose = 0 ) plot_loss ( history_2_l2 , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_l2 , title = \"Noisy Lorentz FFNN L2 regularization\" ) Batching Very large batch size In [47]: %%time batch_size = lor_train . shape [ 0 ] kernel_weight = 0.003 bias_weight = 0.003 model_2_l2 = models . Sequential ( name = 'noiseLorentzModel_l2_large_batch' ) # first hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight ) )) # second hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( kernel_weight ))) # output layer, one neuron model_2_l2 . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_l2 . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_l2 = model_2_l2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = batch_size , verbose = 0 ) plot_loss ( history_2_l2 , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_l2 , title = \"Noisy Lorentz FFNN L2 regularization\" ) Very small batch size (too slow) check the training time, it is more than 3 times slower than the network with batch size 32 In [48]: %%time batch_size = 4 kernel_weight = 0.003 bias_weight = 0.003 model_2_l2 = models . Sequential ( name = 'noiseLorentzModel_l2_large_batch' ) # first hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , input_shape = ( 1 ,), kernel_initializer = 'random_normal' , bias_initializer = 'random_normal' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight ) )) # second hidden layer model_2_l2 . add ( layers . Dense ( n_neurons , activation = 'tanh' , kernel_regularizer = regularizers . l2 ( kernel_weight ))) # output layer, one neuron model_2_l2 . add ( layers . Dense ( 1 , activation = 'linear' )) model_2_l2 . compile ( loss = 'MSE' , optimizer = optimizer ) history_2_l2 = model_2_l2 . fit ( x_train , y_train , validation_data = ( x_test , y_test ), epochs = epochs_max , batch_size = batch_size , verbose = 0 ) plot_loss ( history_2_l2 , rolling = 30 ) plot_sets ( lor_train_n , lor_test_n , NN_model = model_2_l2 , title = \"Noisy Lorentz FFNN L2 regularization\" ) CPU times: user 4min 56s, sys: 18.4 s, total: 5min 14s Wall time: 4min 44s Classification Task Wine Dataset Here, we will work with the wine dataset which we saw in Section 5. First, we overfit the data with a neural network. In [5]: def get_rolling_avg ( arr , rolling = 10 ): return pd . Series ( arr ) . rolling ( rolling ) . mean () def plot_accuracy_loss_rolling ( model_history , rollNum = 10 ): plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plt . semilogx ( get_rolling_avg ( model_history . history [ 'accuracy' ], rollNum ), label = 'train_acc' , linewidth = 4 ) plt . semilogx ( get_rolling_avg ( model_history . history [ 'val_accuracy' ], rollNum ), label = 'val_acc' , linewidth = 4 , alpha =. 7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Rolling Accuracy' ) plt . legend () plt . subplot ( 1 , 2 , 2 ) plt . loglog ( get_rolling_avg ( model_history . history [ 'loss' ], rollNum ), label = 'train_loss' , linewidth = 4 ) plt . loglog ( get_rolling_avg ( model_history . history [ 'val_loss' ], rollNum ), label = 'val_loss' , linewidth = 4 , alpha =. 7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Rolling Loss' ) plt . legend () plt . show () # print('Accuracy in temodel_history.history['val_accuracy'][-1]) In [50]: from sklearn.datasets import load_wine dataset_wine = load_wine () X_wine = pd . DataFrame ( data = dataset_wine . data , columns = dataset_wine . feature_names ) y_wine = pd . DataFrame ( data = dataset_wine . target , columns = [ 'class' ]) print ( X_wine . shape ) print ( y_wine . shape ) full_df_wine = pd . concat ([ X_wine , y_wine ], axis = 1 ) full_df_wine . head () (178, 13) (178, 1) Out[50]: alcohol malic_acid ash alcalinity_of_ash magnesium total_phenols flavanoids nonflavanoid_phenols proanthocyanins color_intensity hue od280/od315_of_diluted_wines proline class 0 14.23 1.71 2.43 15.6 127.0 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065.0 0 1 13.20 1.78 2.14 11.2 100.0 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050.0 0 2 13.16 2.36 2.67 18.6 101.0 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185.0 0 3 14.37 1.95 2.50 16.8 113.0 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480.0 0 4 13.24 2.59 2.87 21.0 118.0 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735.0 0 In [51]: %%time X_train_wine , X_test_wine , y_train_wine , y_test_wine = train_test_split ( X_wine , y_wine , test_size = 0.6 , random_state = 41 ) model_wine_ofit = models . Sequential ([ layers . Input ( shape = ( 13 ,)), layers . Dense ( 32 , activation = 'relu' ), layers . Dense ( 32 , activation = 'relu' ), layers . Dense ( 3 , activation = 'softmax' ) ]) model_wine_ofit . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = optimizers . Adam ( 0.005 ), metrics = [ 'accuracy' ], ) wine_trained_ofit = model_wine_ofit . fit ( x = X_train_wine . to_numpy (), y = y_train_wine . to_numpy (), verbose = 0 , epochs = 1000 , validation_data = ( X_test_wine . to_numpy (), y_test_wine . to_numpy ()), ) CPU times: user 43.1 s, sys: 2.48 s, total: 45.6 s Wall time: 42.6 s In [52]: plot_accuracy_loss_rolling ( wine_trained_ofit , rollNum = 50 ) Add L2 regularization In [53]: kernel_weight = 0.01 bias_weight = 0.01 model_wine_l2 = models . Sequential ([ layers . Input ( shape = ( 13 ,)), layers . Dense ( 32 , activation = 'relu' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight )), layers . Dense ( 32 , activation = 'relu' , kernel_regularizer = regularizers . l2 ( kernel_weight ), bias_regularizer = regularizers . l2 ( bias_weight )), layers . Dense ( 3 , activation = 'softmax' ) ]) model_wine_l2 . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = optimizers . Adam ( 0.005 ), metrics = [ 'accuracy' ], ) wine_trained_l2 = model_wine_l2 . fit ( x = X_train_wine . to_numpy (), y = y_train_wine . to_numpy (), verbose = 0 , epochs = 1000 , validation_data = ( X_test_wine . to_numpy (), y_test_wine . to_numpy ()), ) In [54]: plot_accuracy_loss_rolling ( wine_trained_l2 , rollNum = 50 ) Break Out Room 2 Recall from Section 9 that we had overfit to the Iris dataset Run the given code below to overfit to the Iris dataset (this is the same as in Section 9) Regularize the neural network on the Iris dataset. Tip: The regularization coefficient should be in the range 0.01 - 0.05 Plot the accuracy and losses Note: Use the functions we've declared above for plotting the loss and the neural network output In [6]: from sklearn import datasets iris_data = datasets . load_iris () X = pd . DataFrame ( data = iris_data . data , columns = iris_data . feature_names ) y = pd . DataFrame ( data = iris_data . target , columns = [ 'species' ]) In [12]: %%time # Increase the size of the testing set to encourage overfitting X_train_iris , X_test_iris , y_train_iris , y_test_iris = train_test_split ( X , y , test_size = 0.6 , random_state = 41 ) model_iris_ofit = models . Sequential ([ layers . Input ( shape = ( 4 ,)), layers . Dense ( 32 , activation = 'relu' ), layers . Dense ( 32 , activation = 'relu' ), layers . Dense ( 3 , activation = 'softmax' ) ]) model_iris_ofit . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = optimizers . Adam ( 0.005 ), metrics = [ 'accuracy' ], ) ################## # TRAIN THE MODEL ################## iris_trained_ofit = model_iris_ofit . fit ( x = X_train_iris . to_numpy (), y = y_train_iris . to_numpy (), verbose = 0 , epochs = 1000 , validation_data = ( X_test_iris . to_numpy (), y_test_iris . to_numpy ()), ) CPU times: user 42.4 s, sys: 2.26 s, total: 44.6 s Wall time: 45.9 s In [13]: plot_accuracy_loss_rolling ( iris_trained_ofit ) Now regularize the network to reduce overfitting to Iris dataset In [58]: #your code here In [16]: # %load '../solutions/sol_2.py' In [17]: # %load '../solutions/sol_2b.py' End of Section","tags":"sections","url":"sections/sec_10/"},{"title":"S-Section 10:  Feed Forward Neural Networks: Regularization and Adam optimizer","text":"Jupyter Notebooks S-Section 10: Feed Forward Neural Networks: Regularization and Adam optimizer","tags":"sections","url":"sections/section10/"},{"title":"Advanced Section 6: Deep into Solvers","text":"Slides A-Section 6: Deep into Solvers [PDF] A-Section 6: Deep into Solvers [PPTX]","tags":"a-sections","url":"a-sections/a-section6/"},{"title":"Lecture 32: Regularization methods - Weight decay, data augmentation and dropout","text":"In [1]: # Import the necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt import warnings warnings . filterwarnings ( \"ignore\" ) import tensorflow as tf np . random . seed ( 0 ) tf . random . set_seed ( 0 ) from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras import optimizers from tensorflow.keras.models import load_model from tensorflow.keras import regularizers from sklearn.metrics import mean_squared_error from tensorflow.keras.models import load_model from sklearn.model_selection import train_test_split % matplotlib inline In [2]: # Use the helper code below to generate the data # Defines the number of data points to generate num_points = 30 # Generate predictor points (x) between 0 and 5 x = np . linspace ( 0 , 5 , num_points ) # Generate the response variable (y) using the predictor points y = x * np . sin ( x ) + np . random . normal ( loc = 0 , scale = 1 , size = num_points ) # Generate data of the true function y = x*sin(x) # x_b will be used for all predictions below x_b = np . linspace ( 0 , 5 , 100 ) y_b = x_b * np . sin ( x_b ) In [3]: # Split the data into train and test sets with .33 and random_state = 42 x_train , x_test , y_train , y_test = train_test_split ( x , y , test_size = 0.33 , random_state = 42 ) In [4]: # Helper code to plot the generated data # Plot the train data plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . plot ( x_train , y_train , '.' , label = 'Train data' , markersize = 15 , color = '#FF9A98' ) # Plot the test data plt . plot ( x_test , y_test , '.' , label = 'Test data' , markersize = 15 , color = '#75B594' ) # Plot the true data plt . plot ( x_b , y_b , '-' , label = 'True function' , linewidth = 3 , color = '#5E5E5E' ) # Set the axes labels plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . legend () plt . show () Begin with an unregularized NN. Same as the previous exercise In [5]: # Building an unregularized NN. # Initialise the NN, give it an appropriate name for the ease of reading # The FCNN has 5 layers, each with 100 nodes model_1 = models . Sequential ( name = 'Unregularized' ) # Add 5 hidden layers with 100 neurons each model_1 . add ( layers . Dense ( 100 , activation = 'tanh' , input_shape = ( 1 ,))) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) # Add the output layer with one neuron model_1 . add ( layers . Dense ( 1 , activation = 'linear' )) # View the model summary model_1 . summary () In [7]: # Load with the weights already provided for the unregularized network model_1 . load_weights ( 'weights.h5' ) # Compile the model model_1 . compile ( loss = 'MSE' , optimizer = optimizers . Adam ( learning_rate = 0.001 )) In [8]: # Use the model above to predict for x_b (used exclusively for plotting) y_pred = model_1 . predict ( x_b ) # Use the model above to predict on the test data y_pred_test = model_1 . predict ( x_test ) # Compute the MSE on the test data mse = mean_squared_error ( y_test , y_pred_test ) In [11]: # Use the helper code to plot the predicted data plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . plot ( x_b , y_pred , label = 'Unregularized model' , color = '#5E5E5E' , linewidth = 3 ) plt . plot ( x_train , y_train , '.' , label = 'Train data' , markersize = 15 , color = '#FF9A98' ) plt . plot ( x_test , y_test , '.' , label = 'Test data' , markersize = 15 , color = '#75B594' ) plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . legend () plt . show () Implement previous NN with early stopping For early stopping we build the same network but then we implement early stopping using callbacks. In [0]: # Building an unregularized NN with early stopping. # Initialise the NN, give it an appropriate name for the ease of reading # The FCNN has 5 layers, each with 100 nodes model_2 = models . Sequential ( name = 'EarlyStopping' ) # Add 5 hidden layers with 100 neurons each # tanh is the activation for the first layer # relu is the activation for all other layers model_2 . add ( layers . Dense ( 100 , activation = 'tanh' , input_shape = ( 1 ,))) model_2 . add ( layers . Dense ( 100 , activation = 'relu' )) model_2 . add ( layers . Dense ( 100 , activation = 'relu' )) model_2 . add ( layers . Dense ( 100 , activation = 'relu' )) model_2 . add ( layers . Dense ( 100 , activation = 'relu' )) # Add the output layer with one neuron model_2 . add ( layers . Dense ( 1 , activation = 'linear' )) # View the model summary model_2 . summary () In [0]: # Use the keras early stopping callback with patience=10 while monitoring the loss callback = ___ # Compile the model with MSE as loss and Adam optimizer with learning rate as 0.001 ___ # Save the history about the model after fitting on the train data # Use 0.2 validation split with 1500 epochs and batch size of 10 # Use the callback for early stopping here history_2 = ___ In [0]: # Helper function to plot the data # Plot the MSE of the model plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . title ( \"Early stop model\" ) plt . semilogy ( history_2 . history [ 'loss' ], label = 'Train Loss' , color = '#FF9A98' , linewidth = 2 ) plt . semilogy ( history_2 . history [ 'val_loss' ], label = 'Validation Loss' , color = '#75B594' , linewidth = 2 ) plt . legend () # Set the axes labels plt . xlabel ( 'Epochs' ) plt . ylabel ( 'Log MSE Loss' ) plt . legend () plt . show () In [0]: # Use the early stop implemented model above to predict for x_b (used exclusively for plotting) y_early_stop_pred = ___ # Use the model above to predict on the test data y_earl_stop_pred_test = ___ # Compute the test MSE by predicting on the test data mse_es = ___ In [0]: # Use the helper code to plot the predicted data # Plotting the predicted data using the L2 regularized model plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . plot ( x_b , y_early_stop_pred , label = 'Early stop regularized model' , color = 'black' , linewidth = 2 ) # Plotting the predicted data using the unregularized model plt . plot ( x_b , y_pred , label = 'Unregularized model' , color = '#005493' , linewidth = 2 ) # Plotting the training data plt . plot ( x_train , y_train , '.' , label = 'Train data' , markersize = 15 , color = '#FF9A98' ) # Plotting the testing data plt . plot ( x_test , y_test , '.' , label = 'Test data' , markersize = 15 , color = '#75B594' ) # Set the axes labels plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . legend () plt . show () Mindchow 🍲 After marking change the patience parameter once to 2 and once to 100 in the early stopping callback with the same data. Do you notice any change? While value is more efficient?","tags":"labs","url":"labs/lecture-32/notebook/"},{"title":"Lecture 32: Regularization methods - Weight decay, data augmentation and dropout","text":"In [2]: # Import the libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt import warnings warnings . filterwarnings ( \"ignore\" ) import tensorflow as tf from tensorflow.keras import layers from tensorflow.keras import models from sklearn.metrics import mean_squared_error from tensorflow.keras import optimizers from tensorflow.keras import regularizers from sklearn.model_selection import train_test_split np . random . seed ( 0 ) tf . random . set_seed ( 0 ) % matplotlib inline In [3]: # Use the helper code below to generate the data # Defines the number of data points to generate num_points = 30 # Generate predictor points (x) between 0 and 5 x = np . linspace ( 0 , 5 , num_points ) # Generate the response variable (y) using the predictor points y = x * np . sin ( x ) + np . random . normal ( loc = 0 , scale = 1 , size = num_points ) # Generate data of the true function y = x*sin(x) # x_b will be used for all predictions below x_b = np . linspace ( 0 , 5 , 100 ) y_b = x_b * np . sin ( x_b ) In [4]: # Split the data into train and test sets with .33 and random_state = 42 x_train , x_test , y_train , y_test = train_test_split ( x , y , test_size = 0.33 , random_state = 42 ) In [5]: # Plot the train data plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . plot ( x_train , y_train , '.' , label = 'Train data' , markersize = 15 , color = '#FF9A98' ) # Plot the test data plt . plot ( x_test , y_test , '.' , label = 'Test data' , markersize = 15 , color = '#75B594' ) # Plot the true data plt . plot ( x_b , y_b , '-' , label = 'True function' , linewidth = 3 , color = '#5E5E5E' ) # Set the axes labels plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . legend () plt . show () Begin with an unregularized NN In [7]: # Building an unregularized NN. # Initialise the NN, give it an appropriate name for the ease of reading # The FCNN has 5 layers, each with 100 nodes model_1 = models . Sequential ( name = 'Unregularized' ) # Add 5 hidden layers with 100 neurons each model_1 . add ( layers . Dense ( 100 , activation = 'tanh' , input_shape = ( 100 ,))) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) model_1 . add ( layers . Dense ( 100 , activation = 'relu' )) # Add the output layer with one neuron model_1 . add ( layers . Dense ( 1 , activation = 'linear' )) # View the model summary model_1 . summary () Model: \"Unregularized\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_6 (Dense) (None, 100) 10100 _________________________________________________________________ dense_7 (Dense) (None, 100) 10100 _________________________________________________________________ dense_8 (Dense) (None, 100) 10100 _________________________________________________________________ dense_9 (Dense) (None, 100) 10100 _________________________________________________________________ dense_10 (Dense) (None, 100) 10100 _________________________________________________________________ dense_11 (Dense) (None, 1) 101 ================================================================= Total params: 50,601 Trainable params: 50,601 Non-trainable params: 0 _________________________________________________________________ In [0]: # Compile the model with MSE as loss and Adam optimizer with learning rate as 0.001 model_1 . compile ( ___ ) # Save the history about the model after fitting on the train data # Use 0.2 as the validation split with 1500 epochs and batch size of 10 history_1 = ___ In [0]: # Helper function to plot the data # Plot the MSE of the model plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . title ( \"Unregularized model\" ) plt . semilogy ( history_1 . history [ 'loss' ], label = 'Train Loss' , color = '#FF9A98' ) plt . semilogy ( history_1 . history [ 'val_loss' ], label = 'Validation Loss' , color = '#75B594' ) plt . legend () # Set the axes labels plt . xlabel ( 'Epochs' ) plt . ylabel ( 'Log MSE Loss' ) plt . legend () plt . show () In [0]: # Use the model above to predict for x_b (used exclusively for plotting) y_pred = ___ # Use the model above to predict for x_text y_pred_test = ___ # Compute the test MSE mse = ___ In [0]: # Use the helper code to plot the predicted data plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . plot ( x_b , y_pred , label = 'Unregularized model' , color = '#5E5E5E' , linewidth = 3 ) plt . plot ( x_train , y_train , '.' , label = 'Train data' , markersize = 15 , color = '#FF9A98' ) plt . plot ( x_test , y_test , '.' , label = 'Test data' , markersize = 15 , color = '#75B594' ) plt . legend () plt . show () Regularize the NN with L2 regularization The previous graph indicates that there is overfitting We now initialise a NN with L2 regularization In [0]: # We will now regularise the NN with L2 regularization # Initialise the NN, give it a different name for the ease of reading model_2 = models . ___ # Add L2 regularization to the model # Use L2 regularization parameter value of 0.1 ___ # Add 5 hidden layers with 100 neurons each # relu is the activation for all other layers # Don't forget to include the regularization parameter from above model_2 . add ( layers . Dense ( ___ , kernel_regularizer = ___ , activation = ___ , input_shape = ___ ) ___ ___ ___ ___ # Add the output layer with one neuron and linear activation model_2 . add ( layers . Dense ( ___ )) In [0]: # Compile the model with MSE as loss and Adam optimizer with learning rate as 0.001 model_2 . compile ( ___ ) # Save the history about the model after fitting on the train data # Use 0.2 as the validation split with 1500 epochs and batch size of 10 history_2 = ___ In [0]: # Helper code to plot the MSE of the L2 regularized model plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . title ( \"L2 Regularized model\" ) plt . semilogy ( history_2 . history [ 'loss' ], label = 'Train Loss' , color = '#FF9A98' ) plt . semilogy ( history_2 . history [ 'val_loss' ], label = 'Validation Loss' , color = '#75B594' ) plt . xlabel ( 'Epochs' ) plt . ylabel ( 'MSE Loss' ) plt . legend () plt . show () In [0]: # Use the regularised model above to predict for x_b (used exclusively for plotting) y_l2_regularized_pred = ___ # Use the regularised model above to predict for x_text y_l2_pred_test = ___ # Compute the test MSE by predicting on the test data mse_l2 = ___ In [0]: # Use the helper code to plot the predicted data # Plotting the predicted data using the L2 regularized model plt . rcParams [ \"figure.figsize\" ] = ( 10 , 8 ) plt . plot ( x_b , y_l2_regularized_pred , label = 'L2 regularized model' , color = '#5E5E5E' , linewidth = 3 ) # Plotting the predicted data using the unregularized model plt . plot ( x_b , y_pred , label = 'Unregularized model' , color = '#005493' , linewidth = 2 ) # Plotting the training data plt . plot ( x_train , y_train , '.' , label = 'Train data' , markersize = 15 , color = '#FF9A98' ) # Plotting the testing data plt . plot ( x_test , y_test , '.' , label = 'Test data' , markersize = 15 , color = '#75B594' ) # Set the axes labels plt . xlabel ( 'X' ) plt . ylabel ( 'Y' ) plt . legend () plt . show () Mindchow 🍲 Try the same with L1 regularization. Compare the three neural networks models. Which one best reduces overfitting? Why do you think so? Your answer here","tags":"labs","url":"labs/lecture-32/notebook-2/"},{"title":"Lecture 32: Regularization methods - Weight decay, data augmentation and dropout","text":"Slides PDF | Lecture 32: Optimizers PPTX | Lecture 32: Optimizers PDF | Lecture 32: NN Regularization A PPTX | Lecture 32: NN Regularization A PDF | Lecture 32: NN Regularization B PPTX | Lecture 32: NN Regularization B Exercises Lecture 32: Regularization using L1 and L2 Norm [Notebook] Lecture 32: Early Stopping [Notebook]","tags":"lectures","url":"lectures/lecture32/"},{"title":"Lecture 31: Neural Networks 4 -Back Propagation, SGD","text":"In [1]: # Import the necessary libraries import pandas as pd import matplotlib.pyplot as plt % matplotlib inline import numpy as np In [48]: # Read the file 'backprop.csv' df = pd . read_csv ( 'backprop.csv' ) In [49]: #Generating the predictor and response data x = df . x . values . reshape ( - 1 , 1 ) y = df . y . values In [50]: # Initialize the weights and use the same random seed as the previous exercise i.e. 310 np . random . seed ( 310 ) W = [ np . random . randn ( 1 , 1 ), np . random . randn ( 1 , 1 )] In [25]: # Function to compute the activation function def A ( x ): return ___ # Function to compute the derivative of the activation function def der_A ( x ): return ___ In [47]: # Defining a simple neural network we used in the previous exercise def neural_network ( W , x ): # Computing the first affine a1 = np . dot ( x , W [ 0 ]) # Defining sin() as the activation function fa1 = A ( a1 ) # Computing the second affine a2 = np . dot ( fa1 , W [ 1 ]) # Defining sin() as the activation function y = A ( a2 ) return a1 , a2 , y In [51]: #Use the helper code below to plot the true data and the predictions of your neural network fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) ax . plot ( x , y , label = 'True Function' , color = 'darkblue' , linewidth = 2 ) ax . plot ( x , neural_network ( W , x )[ 2 ], label = 'Neural Network Predictions' , color = '#9FC131FF' , linewidth = 2 ) ax . set_xlabel ( '$x$' , fontsize = 14 ) ax . set_ylabel ( '$y$' , fontsize = 14 ) ax . legend ( fontsize = 14 , loc = 'best' ); In [26]: # Function to compute the partial derivate of a (particular neuron) with respect to corresponding weight w def dadw ( x , firstweight = 0 ): ''' The derivative of the 'a' wrt the preceding weight is just the activation of the previous neuron Note, account for the case where the input layer has no activation layers associated with it. i.e return x if its the first weight ''' if firstweight == 1 : return ___ return ___ In [27]: # Function to compute the partial derivate of h with respect to a def dhda ( a ): ''' This is the derivative of the output of the activation function wrt the affine transformation. Return the derivative of the activation of the affine transformation ''' return ___ In [0]: # Function to compute the partial derivate of y with respect to a def dyda ( a ): ''' This is the derivative of the output of the neural network wrt the affine transformation. Return the derivative of the activation of the affine transformation ''' return ___ In [0]: # Function to compute the partial derivate of a with respect to h def dadh ( w ): return ___ In [28]: # Function to compute the partial derivate of loss with respect to y def dldy ( y_pred , y ): ''' Since our loss function is the MSE, The partial derivative of L wrt y will be 2*(y_pred - y), for all predictions and response ''' return ___ In [46]: # Function to compute the partial derivate of loss with respect to w def dldw ( W , x ): ''' Now, combine the functions from above and find the derivative wrt weights. These will be for all the points, hence take a mean of all values for each partial derivative and return as a list of 2 values ''' dldw2 = ____ dldw1 = ____ return [ np . mean ( dldw2 ), np . mean ( dldw1 )] In [52]: ### edTest(test_gradient) ### # Get the predicted response, and the two activations of the network a1 , a2 , y_pred = neural_network ( W , x ) # Compute the gradient of the loss function with respect to the weights using function defined above gradW = dldw ( W , x ) In [0]: # Print the list of your gradients below print ( f 'The derivatives of w1 w2 wrt L are { gradW } ' ) Mindchow 🍲 Compare your computed partial derivatives wrt the previous exercise. Are they the same? This example was just for a simple case of 1 neuron in 1 hidden layer. How could we generalize this idea to compute partial derivatives of all the weights?","tags":"labs","url":"labs/lecture-31/notebook/"},{"title":"Lecture 31: Neural Networks 4 -Back Propagation, SGD","text":"In [1]: # import required libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error % matplotlib inline In [2]: # get the data from the file `backprop.csv` df = pd . read_csv ( 'backprop.csv' ) # The input needs to be a 2D array so since we have a single # column (1000,) we need to reshape to a 2D array (1000,1) x = df . x . values . reshape ( - 1 , 1 ) y = df . y . values In [3]: # Designing the simple neural network def neural_network ( W , x ): # W is a list of the two weights (w1,w2) of your neural network # x is the input to the neural network ''' Compute a1, a2, and y a1 is the dot product of the input and weight To compute a2, first use the activation function on a1, then multiply by w2 Finally, use the activation function on a2 to compute y Return all three values which you will use to compute derivatives later ''' a1 = np . dot ( ___ , ___ ) fa1 = np . sin ( __ ) a2 = np . dot ( ___ , ___ ) y = np . sin ( __ ) return a1 , a2 , y In [4]: # Initialize the weights, but keep the random seed as 310 for reproducable results np . random . seed ( 310 ) W = [ np . random . randn ( 1 , 1 ), np . random . randn ( 1 , 1 )] In [0]: # Plot the predictor and response variables fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) # plot the true x and y values ax . plot ( x , y , label = 'True function' , color = 'darkblue' , linewidth = 2 ) # plot the x values with the network predictions ax . plot ( x , neural_network ( W , x )[ 2 ], label = 'Neural net predictions' , color = '#9FC131FF' , linewidth = 2 ) # Set the x and y labels ax . set_xlabel ( '$x$' , fontsize = 14 ) ax . set_ylabel ( '$y$' , fontsize = 14 ) ax . legend ( fontsize = 14 ); In [0]: ### edTest(test_nn_mse) ### # You can use the mean_squared_error function to find the MSE of your predictions with true function values y_pred = ___ mse = mean_squared_error ( y , y_pred ) print ( f 'The MSE of the neural network predictions wrt true function is { mse : .2f } ' ) Single update In [5]: # Here we will update the weights only once # Get the predicted response, and the two a's of the network a1 , a2 , y_pred = neural_network ( W , x ) # Compute the gradient of the loss function with respect to weight 2 # Use pen and paper to calculate these derivatives before coding them dldw2 = ___ # Now compute the gradient of the loss function with respect to weight 1 dldw1 = ___ # combine the two in a list dldw = [ np . mean ( dldw1 ), np . mean ( dldw2 )] --------------------------------------------------------------------------- NameError Traceback (most recent call last) in 3 # Ge the predicted response, and the two activations of the network 4 ----> 5 a1 , a2 , y_pred = neural_network ( W , x ) 6 7 # Compute the gradient of the loss function with respect to weight 2 in neural_network (W, x) 13 ___ 14 ---> 15 return a1 , a2 , y NameError : name 'a1' is not defined In [0]: # In the update step, make sure to update the weights with their gradients Wnew = [ i - j for i , j in zip ( W , dldw )] In [0]: # Plot the predictor and response variables fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) ax . plot ( x , y , label = 'True function' , color = 'darkblue' , linewidth = 2 ) ax . plot ( x , neural_network ( Wnew , x )[ 2 ], label = 'Neural net predictions' , color = '#9FC131FF' , linewidth = 2 ) ax . set_xlabel ( '$x$' , fontsize = 14 ) ax . set_ylabel ( '$y$' , fontsize = 14 ) ax . legend ( fontsize = 14 ); In [0]: ### edTest(test_one_update_mse) ### # Compute the new MSE after one update and print it y_pred = ___ mse_update = ___ print ( f 'The MSE of the new neural network predictions with true function is { mse_update : .2f } as compared to { mse : .2f } from before ' ) Several updates In principle, only a single update will never be sufficient to improve model predictions. In the below segment, use the method from above, and update the weight 300 times before plotting predictions. Does your MSE decrease? In [0]: # Reinitialize the weights to start again np . random . seed ( 310 ) W = [ np . random . randn ( 1 , 1 ), np . random . randn ( 1 , 1 )] In [0]: # Unlike the previous step, this time we will set a learning rate of 0.01 to avoid drastic updates and run the above code for 10000 loops lmb = 0.01 for i in range ( 300 ): a1 , a2 , y_pred = ___ # Remember to use np.mean dldw2 = ___ dldw1 = ___ W [ 0 ] = W [ 0 ] - lmb * dldw1 W [ 1 ] = W [ 1 ] - lmb * dldw2 In [0]: # Plot your results and calculate the MSE # Plot the predictor and response variables fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) ax . plot ( x , y , label = 'True function' , color = 'darkblue' , linewidth = 2 ) ax . plot ( x , neural_network ( W , x )[ 2 ], label = 'Neural net predictions' , color = '#9FC131FF' , linewidth = 2 ) ax . set_xlabel ( '$x$' , fontsize = 14 ) ax . set_ylabel ( '$y$' , fontsize = 14 ) ax . legend ( fontsize = 14 ); In [0]: ### edTest(test_mse) ### # We again compute the MSE and compare it with the original predictions y_pred = ___ mse_final = mean_squared_error ( y , y_pred ) print ( f 'The final MSE is { mse_final : .2f } as compared to { mse : .2f } from before ' ) Mindchow 🍲 If you notice, your predicted values are off by approximately 0.5, from the actual values. After marking, go back to your neural network and add a bias correction to your predictions of 0.5. i.e y = np.sin(a2) + 0.5 and rerun your code. Does your code fit better? And does your $MSE$ reduce? In [0]:","tags":"labs","url":"labs/lecture-31/notebook-2/"},{"title":"Lecture 31: Neural Networks 4 -Back Propagation, SGD","text":"Slides PDF | Lecture 31: Stochastic Gradient Descent PPTX | Lecture 31: Stochastic Gradient Descent PDF | Lecture 31: Back Propagation PPTX | Lecture 31: Back Propagation PDF | Lecture 31: Optimizers PPTX | Lecture 31: Optimizers Exercises Lecture 31: Back-propagation by hand [Notebook] Lecture 31: Back-propagation by chain rule [Notebook]","tags":"lectures","url":"lectures/lecture31/"},{"title":"Lecture 30: Neural Networks 3: Design Choices II & Gradient Descent","text":"In [1]: import matplotlib.pyplot as plt import numpy as np % matplotlib inline Here is our function of interest and its derivative. In [2]: def f ( x ): return np . cos ( 3 * np . pi * x ) / x def der_f ( x ): '''derivative of f(x)''' return - ( 3 * np . pi * x * np . sin ( 3 * np . pi * x ) + np . cos ( 3 * np . pi * x )) / x ** 2 # the part of the function we will focus on FUNC_RANGE = ( 0.1 , 3 ) x = np . linspace ( * FUNC_RANGE , 200 ) fig , ax = plt . subplots ( figsize = ( 4 , 3 )) plt . plot ( x , f ( x )) plt . xlim ( x . min (), x . max ()); plt . xlabel ( 'x' ) plt . ylabel ( 'y' ); These 2 functions are just here to help us visualize the gradient descent. You should inspect them later to see how the work. In [3]: def get_tangent_line ( x , x_range =. 5 ): '''returns information about the tangent line of f(x) at a given x Returns: x: np.array - x-values in the tangent line segment y: np.array - y-values in tangent line segment m: float - slope of tangent line''' y = f ( x ) m = der_f ( x ) # get tangent line points # slope point form: y-y_1 = m(x-x_1) # y = m(x-x_1)+y_1 x1 , y1 = x , y x = np . linspace ( x1 - x_range / 2 , x1 + x_range / 2 , 50 ) y = m * ( x - x1 ) + y1 return x , y , m def plot_it ( cur_x , title = '' , ax = plt ): '''plots the point cur_x on the curve f(x) as well as the tangent line on f(x) where x=cur_x''' y = f ( x ) ax . plot ( x , y ) ax . scatter ( cur_x , f ( cur_x ), c = 'r' , s = 80 , alpha = 1 ); x_tan , y_tan , der = get_tangent_line ( cur_x ) ax . plot ( x_tan , y_tan , ls = '--' , c = 'r' ) # indicate if our location is outside the x range if cur_x > x . max (): ax . axvline ( x . max (), c = 'r' , lw = 3 ) ax . arrow ( x . max () / 1.6 , y . max () / 2 , x . max () / 5 , 0 , color = 'r' , head_width =. 25 ) if cur_x < x . min (): ax . axvline ( x . min (), c = 'r' , lw = 3 ) ax . arrow ( x . max () / 2.5 , y . max () / 2 , - x . max () / 5 , 0 , color = 'r' , head_width =. 25 ) ax . set_xlim ( x . min (), x . max ()) ax . set_ylim ( - 3.5 , 3.5 ) ax . set_title ( title ) Set a learning rate between 1 and 0.001. Then fill in the blanks to: Find the derivative, delta , of f(x) where x = cur_x Update the current value of x, cur_x Create the boolean expression has_converged that ends the algorithm if True You can experiment with how different values for learning_rate and max_iter affect your results. In [4]: ### edTest(test_convergence) ### converged = False max_iter = __ # Play with figsize=(25,20) to make more visible plots fig , axs = plt . subplots ( max_iter // 5 , 5 , figsize = ( 25 , 20 ), sharey = True ) cur_x = 0.75 # initial value of x learning_rate = __ # controls how large our update steps are epsilon = 0.0025 # minimum update magnitude for i , ax in enumerate ( axs . ravel ()): plot_it ( cur_x , title = f \" { i } step { '' if i == 1 else 's' } \" , ax = ax ) prev_x = cur_x # remember what x was delta = __ # find derivative (Hint: use der_f()) cur_x = __ # update current x-value (Hint: use learning_rate & delta) # stop algorithm if we've converged # boolean expression (Hint: last update size & epsilon) has_converged = __ if has_converged : converged = True # hide unused subplots for ax in axs . ravel ()[ i + 1 :]: ax . axis ( 'off' ) break plt . tight_layout () if not converged : print ( 'Did not converge!' ) else : converged = True print ( 'Converged to a local minimum!' ) Did you get $x$ to converge to a local minimum? Did it converge at all? If not, how would you describe the problem? What might we do to address this issue? Mindchow 🍲 Why does the algorithm not find the first local minimum which is actually the global minimum? Try changing the initial value of cur_x to see if you can have the function converge to this global minimum your answer here","tags":"labs","url":"labs/lecture-30/notebook/"},{"title":"Lecture 30: Neural Networks 3: Design Choices II & Gradient Descent","text":"In [0]: # Import the necessary libraries import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split import pandas as pd % matplotlib inline tf . keras . backend . clear_session () # For easy reset of notebook state. In [0]: # Read the file 'IRIS.csv' df = pd . read_csv ( 'IRIS.csv' ) In [0]: # Take a quick look at the dataset df . head () In [0]: # We use one-hot-encoding to encode the 'species' labels using pd.get_dummies one_hot_df = pd . get_dummies ( df [ ___ ], prefix = 'species' ) In [0]: # The predictor variables are all columns except species X = df . drop ([ ___ ], axis = 1 ) . values # The response variable is the one-hot-encoded species values y = one_hot_df . values In [0]: # We divide our data into test and train sets with 80% training size X_train , X_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = ___ ) In [0]: # To build the MLP, we will use the keras library model = tf . keras . models . Sequential ( name = 'MLP' ) # To initialise our model we set some parameters # commonly defined in an MLP design # The number of nodes in a hidden layer n_hidden = ___ # The number of nodes in the input layer (features) n_input = ___ # The number of nodes in the output layer n_output = ___ In [0]: # We add the first hidden layer with `n_hidden` number of neurons # and 'relu' activation model . add (( tf . keras . layers . Dense ( n_hidden , input_dim = ___ , activation = ___ , name = 'hidden' ))) # The second layer is the final layer in our case, using 'softmax' on the output labels model . add ( tf . keras . layers . Dense ( n_output , activation = ___ , name = 'output' )) In [0]: # Now we compile the model using 'categorical_crossentropy' loss, # optimizer as 'sgd' and 'accuracy' as a metric model . compile ( optimizer = ___ , loss = ___ , metrics = [ ___ ]) In [0]: # You can see an overview of the model you built using .summary() model . summary () In [0]: # We fit the model, and save it to a variable 'history' that can be # accessed later to analyze the training profile # We also set validation_split=0.2 for 20% of training data to be # used for validation # verbose=0 means you will not see the output after every epoch. # Set verbose=1 to see it history = model . fit ( ___ , ___ , epochs = ___ , batch_size = 16 , verbose = 0 , validation_split = ___ ) In [0]: # Here we plot the training and validation loss and accuracy fig , ax = plt . subplots ( 1 , 2 , figsize = ( 16 , 4 )) ax [ 0 ] . plot ( history . history [ 'loss' ], 'r' , label = 'Training Loss' ) ax [ 0 ] . plot ( history . history [ 'val_loss' ], 'b' , label = 'Validation Loss' ) ax [ 1 ] . plot ( history . history [ 'accuracy' ], 'r' , label = 'Training Accuracy' ) ax [ 1 ] . plot ( history . history [ 'val_accuracy' ], 'b' , label = 'Validation Accuracy' ) ax [ 0 ] . legend () ax [ 1 ] . legend () ax [ 0 ] . set_xlabel ( 'Epochs' ) ax [ 1 ] . set_xlabel ( 'Epochs' ); ax [ 0 ] . set_ylabel ( 'Loss' ) ax [ 1 ] . set_ylabel ( 'Accuracy %' ); fig . suptitle ( 'MLP Training' , fontsize = 24 ) In [0]: ### edTest(test_accuracy) ### # Once you have near-perfect validation accuracy, time to evaluate model performance on test set train_accuracy = model . evaluate ( ___ , ___ )[ 1 ] test_accuracy = model . evaluate ( ___ , ___ )[ 1 ] print ( f 'The training set accuracy for the model is { train_accuracy } \\ \\n The test set accuracy for the model is { test_accuracy } ' )","tags":"labs","url":"labs/lecture-30/notebook-2/"},{"title":"Lecture 30: Neural Networks 3: Design Choices II & Gradient Descent","text":"Slides PDF | Lecture 30: Design Layers PPTX | Lecture 30: Design Layers PDF | Lecture 30: Gradient Descent PPTX | Lecture 30: Gradient Descent Exercises Lecture 30: A.1- MLP using Keras [Notebook] Lecture 30: B.1 - Gradient Descent [Notebook]","tags":"lectures","url":"lectures/lecture30/"},{"title":"S-Section 09: Feed forward neural networks","text":"CS109A Introduction to Data Science Standard Section 9: Feed Forward Neural Networks Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Henry Jin, Hayden Joy In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: The goal of this section is to become familiar with a basic Artificial Neural Network architecture, the Feed-Forward Neural Network (FFNN). Specifically, we will: Quickly review the FFNN anatomy . Design a simple FFNN from scratch (using numpy) and fit toy datasets. Quantify the prediction (fit) by using sklearn's mean square error metric. FFNN is a universal approximator . Understand this property by inspecting the functions generated by an FFNN. Forward propagation with TensorFlow and Keras: Design the previous and more complex architectures. Back propagation with TensorFlow and Keras: Train the networks, that is, find the optimal weights. Train an FFNN for image classification: MNIST and Iris datasets are explored. Import packages and check the version of your TensorFlow, it should be the version 2+ In [3]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt # import seaborn as sns import tensorflow as tf from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split In [4]: print ( tf . __version__ ) 2.0.0 IMPORTANT: Unless you have the \"TF version 2.2\" or higher, try the following pip install --upgrade pip pip install tensorflow OR conda install tensorflow 1. Review of the ANN anatomy Input, Hidden, and Output layers The forward pass through an FFNN is a sequence of linear (affine) and nonlinear operations (activation). 2. Design a Feed Forward neural network Let's create a simple FFNN with one input , one hidden layer with arbitrary number of hidden neurons, and one linear neuron for the output layer. The purpose here is to become familiar with the forward propagation. Define the ReLU and Sigmoid nonlinear functions. These are two commonly used activation functions. Create an FFNN with one hidden neuron and become familiar with the activation function. Break Room : Load the toyDataSet_1.csv and fit (manually tuning the weights). This is a simple regression problem with one input and one output. Write a function for the forward pass of a single input/output FFNN with a single hidden layer of arbitrary number of neurons. Tune the weights randomly and inspect the generated functions. Is this network a universal approximator ? Define activation functions Rectified Linear Unit (ReLU) function is defined as $$g(x)=\\max(0,x)$$ Sigmoid function is defined as $$\\sigma(x)=\\frac{1}{1+e&#94;{-z}}$$ In [5]: def g ( z : float ) -> float : return np . maximum ( 0 , z ) # or # g = lambda z: np.maximum(0, z) def sig ( z : float ) -> float : return 1 / ( 1 + np . exp ( - z )) Build an FFNN with one hidden neuron. No training here ReLU activation In [7]: # input vector x_train = np . linspace ( - 1 , 1 , 100 ) # set the network parameters w1 , b1 = 1 , 0. w2 , b2 = 1 , 0 # affine operation (input layer) l1 = w1 * x_train + b1 # RELU activation h = g ( l1 ) # output linear layer y_train = w2 * h + b2 plt . plot ( x_train , y_train , '-b' ) plt . title ( 'ReLU Activation' ) plt . show () Sigmoid activation In [8]: # input vector x_train = np . linspace ( - 1 , 1 , 100 ) # set the network parameters w1 , b1 = 10 , 0. w2 , b2 = 1 , 0 # affine operation (input layer) l1 = w1 * x_train + b1 # Sigmoid activation h = sig ( l1 ) # output linear layer y_train = w2 * h + b2 plt . plot ( x_train , y_train , '-b' ) plt . title ( 'Sigmoid Activation' ) plt . show () Plot a few cases to become familiar with the activation In [9]: #weights and biases that we want to explore. weight1, bias1, weight2, bias2 weights1 = 1 , 0 , 1 , 0 weights2 = 1 , 0.5 , 1 , 0 weights3 = 1 , 0.5 , 1 , - 0.5 weights4 = 1 , 0.5 , 4 , -. 5 weights_list = [ weights1 , weights2 , weights3 , weights4 ] def simple_FFN ( w1 , b1 , w2 , b2 , activation ): \"\"\" Takes weights, biases, and an activation function and returns a simple prediction. Arguments: w1, w2: weights 1 and 2 b1, b2: biases 1 and 2 \"\"\" # linear input layer l1 = w1 * x_train + b1 #activation function + output linear layer y_pred = w2 * activation ( l1 ) + b2 return y_pred #make our plot plt . figure ( figsize = [ 12 , 8 ]) for i , w_list in enumerate ( weights_list ): #make our weight dictionary then feed the dictionary as arguments to the FFN to get a prediction. w_dict = dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], w_list )) # print(w_dict) y_train_pred = simple_FFN ( ** w_dict , activation = g ) #make the plot plt . subplot ( 2 , 2 , i + 1 ) plt . plot ( x_train , y_train_pred , 'b' ) plt . ylim ([ - 1 , 1 ]) plt . xlim ([ - 1 , 1 ]) plt . title ( 'w1, b1, w2, b2 = {} ' . format ( w_list )) plt . ylabel ( \"y(x)\" ) plt . xlabel ( \"x\" ) plt . grid ( 'on' ) plt . tight_layout () Explore the sigmoid activation In [10]: #weights and biases that we want to explore. weight1, bias1, weight2, bias2 weights_1 = 10 , 0 , 1 , 0 weights_2 = 10 , 5 , 1 , 0 weights_3 = 10 , 5 , 1 , -. 5 weights_4 = 10 , 5 , 2 , -. 5 weights_list = [ weights_1 , weights_2 , weights_3 , weights_4 ] #make our plot plt . figure ( figsize = [ 12 , 8 ]) for i , w_list in enumerate ( weights_list ): #make our weight dictionary then feed the dictionary as arguments to the FFN to get a prediction. #note how we have changed the activation function to sigmoid. w_dict = dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], w_list )) y_train_pred = simple_FFN ( ** w_dict , activation = sig ) #make the plot plt . subplot ( 2 , 2 , i + 1 ) plt . plot ( x_train , y_train_pred , 'b' ) plt . ylim ([ - 1 , 1.6 ]) plt . xlim ([ - 1 , 1 ]) plt . title ( 'w1, b1, w2, b2 = {} ' . format ( w_list )) plt . ylabel ( \"y(x)\" ) plt . xlabel ( \"x\" ) plt . grid ( 'on' ) plt . tight_layout () Break Out Room 1 Design a simple FFNN and fit a simple dataset Load the toyDataSet_1.csv from the data directory. Write an FFNN with one hidden layer of one neuron and fit the data. Between ReLU and Sigmoid , choose which activation function works better Make a plot with the ground truth data and the prediction Use the sklearn mean_squared_error() to evaluate the prediction Neglect splitting into training and testing sets for this simple task. Just fit and evaluate the prediction on the entire set In [11]: def plot_toyModels ( x_data , y_data , y_pred = None ): # plot the prediction and the ground truth if type ( y_data ) != type ( None ): plt . plot ( x_data , y_data , 'or' , label = 'data' ) if type ( y_pred ) != type ( None ): plt . plot ( x_data , y_pred , '-b' , linewidth = 4 , label = 'FFNN' , alpha =. 7 ) plt . xlabel ( 'x' ) plt . ylabel ( 'y(x)' ) plt . legend () In [12]: toySet_1 = pd . read_csv ( '../data/toyDataSet_1.csv' ) x_train = toySet_1 [ 'x' ] . values . reshape ( - 1 , 1 ) y_train = toySet_1 [ 'y' ] . values . reshape ( - 1 , 1 ) plot_toyModels ( x_train , y_train ) In [68]: ## your code here # set the network parameters w1 = b1 = w2 = b2 = # affine operation l1 = TODO # activation (Choose between ReLu or Sigmoid) h = TODO # output linear layer y_model_train = TODO # Make a plot (use the ploting function defined earlier) plot_toyModels ( x_train , y_train , y_model_train ) # Use MSE to evaluate the prediction mse_toy = TODO print ( 'The MSE for the training set is ' , np . round ( mse_toy , 5 )) File \" \" , line 4 w1 = &#94; SyntaxError : invalid syntax In [15]: # %load '../solutions/sol_1.py' A function for a more complex Forward Pass Let's write a function for the forward propagation through an FFNN with one input, one linear output neuron, and one hidden layers with arbitrary number of neurons. General Scheme: One input vector: $x$ $$$$ Affine (linear) transformation: $l_1$ where $w_{1},~b_{1}$ are the parameter vectors (or $w_{1i},~b_{1i}$): $$l_1 = \\sum_{i=1}&#94;\\text{neurons} w_{1i}x+b_{1i} = w&#94;T_1 x + b_1 = w_1 \\cdot x + b_1 = W_1\\cdot X$$ $$$$ Activation function (nonlinear transformation): $g(\\cdot)$ $$h = g(l_1)$$ $$$$ Linear Output layer with a vector for weights $w_o$ and a scalar bias $b_o$: $$y = w_o&#94;T h+b_o = w_o \\cdot h + b_o = W_o\\cdot H$$ In [16]: def myFFNN ( X , W1 , Wo , activation = 'relu' ): \"\"\" This function is a simple feed forward nueral network that takes in two weight vectors and a design matrix and returns a prediction (yhat). Network specifications: input dimensions = 1 output dimensions = 1 hidden layers = 1 **hidden neurons are determined by the size of W1 or W0** Parameters: Design Matrix: X: the design matrix on which to make the predictions. weights vectors: W1 : parameters of first layer Wo : parameters of output layer activation: The default activation is the relu. It can be changed to sigmoid \"\"\" # Input Layer: # add a constant column for the biases to the input vector X ones = np . ones (( len ( X ), 1 )) l1 = X l1 = np . append ( l1 , ones , axis = 1 ) # hidden layer: Affine and activation a1 = np . dot ( W1 , l1 . T ) if activation == 'relu' : h1 = g ( a1 ) elif activation == 'sigmoid' : h1 = sig ( a1 ) # Output layer (linear layer) (2 steps) # (a) Add a const column the h1 for the affine transformation ones = np . ones (( len ( X ), 1 )) H = np . append ( h1 . T , ones , axis = 1 ) . T # (b) Affine a = np . dot ( Wo , H ) y_hat = a . T return y_hat Use the previous parameters in our forward propagation function to fit the toyDataSet_1.csv. Plot the results and print the associated loss (the MSE) In [17]: w11 = 2 b11 = 0.0 w21 = 1 b21 = 0.5 # make the parameters matrices # First layer W1 = np . array ([[ w11 , b11 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , b21 ]]) # run the model y_model_1 = myFFNN ( x_train , W1 , Wo ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_train , y_model_1 ) # quantify your prediction Loss_1 = mean_squared_error ( y_train , y_model_1 ) print ( 'MSE Loss = ' , np . round ( Loss_1 , 4 )) MSE Loss = 0.0023 FFNN is a Universal Approximator Explore what functions can be generated by a single-hidden layer network with many neurons. There is a rigorous proof that a FFNN can approximate any continuous function if the network has sufficient hidden neurons. For more information check the paper NeuralNets_UniversalApproximators in the notes directory In [18]: # Two Neurons w11 = -. 8 b11 = -. 1 w12 = . 4 b12 = -. 1 w21 = 1.3 w22 = -. 8 b2 = 0.5 # First Layer W1 = np . array ([[ w11 , b11 ], [ w12 , b12 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , w22 , b2 ]]) # run the model y_model_p = myFFNN ( x_train , W1 , Wo , activation = 'relu' ) plot_toyModels ( x_train , y_data = None , y_pred = y_model_p ) In [19]: # Three Neurons w11 = -. 1 b11 = . 3 w12 = . 9 b12 = -. 1 w13 = . 7 b13 = -. 2 w21 = - 1. w22 = -. 7 w33 = . 8 b2 = 0.25 # First Layer W1 = np . array ([[ w11 , b11 ], [ w12 , b12 ], [ w13 , b13 ]]) # Output Layer (only one bias term) Wo = np . array ([[ w21 , w22 , w33 , b2 ]]) # run the model y_model_p = myFFNN ( x_train , W1 , Wo ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_data = None , y_pred = y_model_p ) plt . show () In [20]: # Random numbers between a,b # (b-a) * np.random.random_sample((4, 4)) + a a = - 20 b = 20 # N neurons N = 50 # Create random parameter matrices W1 = ( b - a ) * np . random . random_sample (( N , 2 )) + a Wo = ( b - a ) * np . random . random_sample (( 1 , N + 1 )) + a # make a bigger interval x_train_p2 = np . linspace ( - 2 , 2 , 1000 ) x_train_p2 = x_train_p2 . reshape ( - 1 , 1 ) ## run the models and plot the predictions plt . figure ( figsize = [ 12 , 4 ]) # # RELU ACTIVATION y_model_p2 = myFFNN ( x_train_p2 , W1 , Wo , activation = 'relu' ) plt . subplot ( 1 , 2 , 1 ) plot_toyModels ( x_train_p2 , y_data = None , y_pred = y_model_p2 ) plt . title ( 'Relu activation' ) # ## SIGMOID ACTIVATION y_model_p2 = myFFNN ( x_train_p2 , W1 , Wo , activation = 'sigmoid' ) plt . subplot ( 1 , 2 , 2 ) plot_toyModels ( x_train_p2 , y_data = None , y_pred = y_model_p2 ) plt . title ( 'Sigmoid activation' ) plt . show () 3. TensorFlow and Keras Keras, Sequential: [Source] ( https://keras.io/models/sequential/ ) There are many powerful packages to work with neural networks like TensorFlow and PyTorch . These packages provide both the forward and backward propagations, where the latter is used to train (optimize) a network. Training means to find the optimal parameters for a specific task. Here, we use TensorFlow (TF) and Keras to employ FFNN. Use Keras to fit the simple toyDataSet_1 dataset. Tune the weights manually. Learn the Sequential method Use backpropagation supported by TF to find the optimal parameters for the same dataset. Learn the fit method Import packages from keras In [21]: from tensorflow.keras import layers from tensorflow.keras import models from tensorflow.keras import optimizers Read again the toyDataSet_1 and define the weights used in solution 1 In [22]: toySet_1 = pd . read_csv ( '../data/toyDataSet_1.csv' ) x_train = toySet_1 [ 'x' ] . values . reshape ( - 1 , 1 ) y_train = toySet_1 [ 'y' ] . values . reshape ( - 1 , 1 ) w1 = 2 b1 = 0.0 w2 = 1 b2 = 0.5 Use Keras to build the previous simple architecture and fit the toyDataSet. Set manually the previously used weights In [47]: model = models . Sequential ( name = 'Single_neurons_model_fixedWeights' ) # hidden layer with 1 neuron (or node) model . add ( layers . Dense ( 1 , activation = 'relu' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' , input_shape = ( 1 ,))) # output layer, one neuron model . add ( layers . Dense ( 1 , activation = 'linear' )) model . summary () Model: \"Single_neurons_model_fixedWeights\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_18 (Dense) (None, 1) 2 _________________________________________________________________ dense_19 (Dense) (None, 1) 2 ================================================================= Total params: 4 Trainable params: 4 Non-trainable params: 0 _________________________________________________________________ In [48]: # A FUNCTION THAT READS AND PRINTS OUT THE MODEL WEIGHTS/BIASES def print_weights ( model ): weights = model . get_weights () print ( dict ( zip ([ \"w1\" , \"b1\" , \"w2\" , \"b2\" ], [ weight . flatten ()[ 0 ] for weight in weights ]))) print ( 'Initial values of the parameters' ) print_weights ( model ) # MANUALLY SETTING THE WEIGHTS/BIASES # Read and then define the weights weights = model . get_weights () # hidden layer weights [ 0 ][ 0 ] = np . array ([ w1 ]) #weights weights [ 1 ] = np . array ([ b1 ]) # biases # output layer weights [ 2 ] = np . array ([[ w2 ]]) # weights weights [ 3 ] = np . array ([ b2 ]) # bias # Set the weights model . set_weights ( weights ) print ( ' \\n After setting the parameters' ) print_weights ( model ) Initial values of the parameters {'w1': 0.011672685, 'b1': 0.027603041, 'w2': 1.4585346, 'b2': 0.0} After setting the parameters {'w1': 2.0, 'b1': 0.0, 'w2': 1.0, 'b2': 0.5} In [49]: y_model_tf1 = model . predict ( x_train ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_train , y_pred = y_model_tf1 ) # quantify your prediction Loss_tf1 = mean_squared_error ( y_train , y_model_tf1 ) print ( 'MSE Loss = ' , np . round ( Loss_tf1 , 4 )) MSE Loss = 0.0023 Train the network: Find the optimal weights Back propagation The backward pass is the training. It is based on the chain rule of calculus, and it updates the parameters. The optimization is done by minimizing the loss function. Batching, stochastic gradient descent, and epochs Shuffle and split the entire dataset in mini-batches to help escape from local minima In [63]: model_t = models . Sequential ( name = 'Single_neurons_model_training' ) # hidden layer with 1 neurons (or nodes) model_t . add ( layers . Dense ( 1 , activation = 'relu' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' , input_shape = ( 1 ,))) # output layer, one neuron model_t . add ( layers . Dense ( 1 , activation = 'linear' )) # model_t.summary() In [64]: # sgd = optimizers.SGD(lr=0.005) sgd = optimizers . Adam ( lr = 0.005 ) model_t . compile ( loss = 'MSE' , optimizer = sgd ) history = model_t . fit ( x_train , y_train , epochs = 2000 , batch_size = 64 , verbose = 0 ) Plot training & validation loss values In [65]: plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plt . plot ( history . history [ 'loss' ], 'b' ) plt . title ( 'Model loss' ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) plt . subplot ( 1 , 2 , 2 ) plt . loglog ( history . history [ 'loss' ], 'b' ) plt . title ( 'Model loss (loglog)' ) plt . ylabel ( 'Loss' ) plt . xlabel ( 'Epoch' ) plt . show () Visualize the prediction In [66]: y_model_t = model_t . predict ( x_train ) # plot the prediction and the ground truth plot_toyModels ( x_train , y_train , y_pred = y_model_t ) # quantify your prediction Loss_tf1_t = mean_squared_error ( y_train , y_model_t ) print ( 'MSE Loss = ' , np . round ( Loss_tf1_t , 4 )) print ( 'MSE with manually tuned weihgts' , np . round ( Loss_tf1 , 4 )) MSE Loss = 0.0022 MSE with manually tuned weihgts 0.0023 Check the parameters In [72]: weights_t = model_t . get_weights () print ( \"Trained by TF weights:\" ) print_weights ( model_t ) print ( \" \\n Manually fixed weights:\" ) #, weights) print_weights ( model ) Trained by TF weights: {'w1': 1.5288035, 'b1': 0.0145217795, 'w2': 1.2909468, 'b2': 0.4919371} Manually fixed weights: {'w1': 2.0, 'b1': 0.0, 'w2': 1.0, 'b2': 0.5} Add more neurons and inspect the performance during the training Explore different activation function, optimizer, number of hidden neurons and layers In [78]: model = models . Sequential ( name = 'Many_neurons_model_relu' ) ## First hidden layer model . add ( layers . Dense ( 5 , activation = 'relu' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' , input_shape = ( 1 ,))) ## Extra hidden layer # model.add(layers.Dense(20, activation='relu', input_shape=(1,))) ## output layer, one neuron model . add ( layers . Dense ( 1 , activation = 'linear' )) optimizer = optimizers . SGD ( lr = 0.005 ) # optimizer = optimizers.Adam(lr=0.005) model . compile ( loss = 'MSE' , optimizer = sgd ) plt . figure ( figsize = [ 16 , 8 ]) epochs = 100 for i in range ( 6 ): # Train and Fit and MSE model . fit ( x_train , y_train , epochs = epochs , batch_size = 64 , verbose = 0 ) y_model_t = model . predict ( x_train ) loss = mean_squared_error ( y_train , y_model_t ) # Plot plt . subplot ( 2 , 3 , i + 1 ) plot_toyModels ( x_train , y_train , y_model_t ) plt . title ( 'Epoch ' + str ( epochs * ( i + 1 )) + '. MSE = ' + str ( np . round ( loss , 4 ))) In [79]: model . summary () Model: \"Many_neurons_model_relu\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense_38 (Dense) (None, 5) 10 _________________________________________________________________ dense_39 (Dense) (None, 1) 6 ================================================================= Total params: 16 Trainable params: 16 Non-trainable params: 0 _________________________________________________________________ Helper functions: For plotting train set, test set, neural network predictions For plotting the training and validation loss functions In [80]: def array_exists ( arr ): return hasattr ( arr , 'shape' ) #check if the numpy array exists def reshape_if_exists ( arr ): if array_exists ( arr ): return arr [ 'x' ] . values . reshape ( - 1 , 1 ), arr [ 'y' ] . values . reshape ( - 1 , 1 ) else : return None , None def reshape_and_extract_sets ( train_set , test_set ): \"\"\" Extracts x_train, y_train, x_test and y_test and reshapes them for using with keras. \"\"\" x_train , y_train = reshape_if_exists ( train_set ) x_test , y_test = reshape_if_exists ( test_set ) return x_train , y_train , x_test , y_test def plot_sets ( train_set = None , test_set = None , NN_model = None ): \"\"\" plots the train set, test set, and Neural network model. This function is robust to lack of inputs. You can feed it any combination of train_set, test_set and \"\"\" x_train , y_train , x_test , y_test = reshape_and_extract_sets ( train_set , test_set ) if array_exists ( train_set ): plt . plot ( x_train , y_train , 'or' , label = 'train data' ) if array_exists ( test_set ): plt . plot ( x_test , y_test , '&#94;g' , label = 'test data' ) # if the neural network model was provided, plot the predictions. if type ( NN_model ) != type ( None ): NN_preds = NN_model . predict ( x_train ) sorted_idx = np . argsort ( x_train . reshape ( - 1 ,)) plt . plot ( x_train [ sorted_idx ], NN_preds [ sorted_idx ], '-b' , linewidth = 4 , label = 'FFNN' , alpha = 0.7 ) plt . xlabel ( \"x\" ) plt . ylabel ( \"y(x)\" ) plt . legend () plt . show () def plot_loss ( model_history ): plt . loglog ( model_history . history [ 'loss' ], linewidth = 4 , label = 'Training' ) plt . loglog ( model_history . history [ 'val_loss' ], linewidth = 4 , label = 'Validation' , alpha = 0.7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . legend () plt . show () Split the data and train a network with the training set and evaluate the model on the test set In [81]: # The usual split toy_train , toy_test = train_test_split ( toySet_1 , train_size = 0.7 , random_state = 109 ) # use helper functions to extract and plot the datasets x_train , y_train , x_test , y_test = reshape_and_extract_sets ( toy_train , toy_test ) plot_sets ( toy_train , test_set = toy_test ) In [85]: model_2 = models . Sequential ( name = 'Many_neurons_model_relu_2' ) # hidden layer with 2 neurons (or nodes) model_2 . add ( layers . Dense ( 10 , activation = 'relu' , kernel_initializer = 'random_normal' , bias_initializer = 'random_uniform' , input_shape = ( 1 ,))) ## Add an extra layer # model_2.add(layers.Dense(10, activation='relu', input_shape=(1,))) # output layer, one neuron model_2 . add ( layers . Dense ( 1 , activation = 'linear' )) # optimizer = optimizers.SGD(lr=0.01) optimizer = optimizers . Adam ( 0.01 ) model_2 . compile ( loss = 'MSE' , optimizer = optimizer ) In [86]: history_2 = model_2 . fit ( x_train , y_train , epochs = 1500 , batch_size = 64 , verbose = 0 , validation_data = ( x_test , y_test )) Plot the training and validation loss functions In [87]: plot_loss ( history_2 ) Plot the predictions along with the ground truth data In [88]: plot_sets ( train_set = toy_train , test_set = toy_test , NN_model = model_2 ) Break Out Room 2 Let's fit something very nonlinear Load the toyDataSet_2.csv from the data directory. Split the data in training and testing sets. Use Keras to desing an FFNN with Sequential() : two hidden layers of 20 neurons with tanh() activation function tanh Use Adam optimizer with learning rate 0.005 Adam(0.005) Define MSE as the loss function and compile() Train the model with the training set and validate with the testing set: fit() Plot the training and validation loss functions plot_loss() Plot the prediction along with the ground truth data In [89]: toySet_2 = pd . read_csv ( '../data/toyDataSet_2.csv' ) toy_train2 , toy_test2 = train_test_split ( toySet_2 , train_size = 0.7 , random_state = 109 ) x_train2 , y_train2 , x_test2 , y_test2 = reshape_and_extract_sets ( toy_train2 , toy_test2 ) plot_sets ( toy_train2 , toy_test2 ) In [192]: ############################# # Design the neural network ############################# model = # hidden layer with 20 neurons (or nodes) model . add ( ?? ) # Add another hidden layer of 20 neurons model . add ( ?? ) # output layer, one neuron model . add ( ?? ) ############################################## ## SET OPTIMIZER AND LOSS. COMPILE AND FIT ############################################## optimizer = model . compile ( ?? ) history_toy2 = model . fit ( ?? ) # PLOT THE LOSS FUNCTIONS plot_loss ( ?? ) # PLOT DATA AND PREDICTIONS plot_sets ( ?? ) File \" \" , line 4 model = &#94; SyntaxError : invalid syntax In [94]: # %load '../solutions/sol_2.py' Classification Task using NeuralNets In [95]: # we'll use keras a lot more in the last few weeks of the course from keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) Using TensorFlow backend. (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) In [97]: print ( 'This picture belongs to the class for number' , y_train [ 10 ]) fig , ax = plt . subplots () ax . grid ( 'off' ) ax . imshow ( x_train [ 10 ], cmap = 'gray' ); This picture belongs to the class for number 3 In [98]: x_train = x_train . reshape ( x_train . shape [ 0 ], 784 ) x_test = x_test . reshape ( x_test . shape [ 0 ], 784 ) # check if the shapes are ok print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) # checking the min and max of x_train and x_test print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) # NORMALIZE x_train = ( x_train - x_train . min ()) / ( x_train . max () - x_train . min ()) x_test = ( x_test - x_train . min ()) / ( x_train . max () - x_train . min ()) print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) (60000, 784) (60000,) (10000, 784) (10000,) 0 255 0 255 0.0 1.0 0.0 255.0 In [153]: model_mnist = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( 784 ,)), tf . keras . layers . Dense ( 128 , activation = 'relu' ), tf . keras . layers . Dense ( 10 , activation = 'softmax' ) ]) # One could also do: # model_mnist = tf.keras.models.Sequential() # model_mnist = tf.keras.layers.Input(shape = (784,)), # model_mnist.add(layers.Dense(784, activation='relu')) # model_mnist.add(layers.Dense(10, activation='softmax')) model_mnist . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = tf . keras . optimizers . Adam ( 0.001 ), metrics = [ 'accuracy' ] ) In [154]: trained_mnist = model_mnist . fit ( x = x_train , y = y_train , epochs = 6 , batch_size = 128 , validation_data = ( x_test , y_test ), ) Train on 60000 samples, validate on 10000 samples Epoch 1/6 60000/60000 [==============================] - 2s 37us/sample - loss: 0.3574 - accuracy: 0.9021 - val_loss: 23.9483 - val_accuracy: 0.9441 Epoch 2/6 60000/60000 [==============================] - 2s 29us/sample - loss: 0.1614 - accuracy: 0.9544 - val_loss: 18.0437 - val_accuracy: 0.9594 Epoch 3/6 60000/60000 [==============================] - 2s 28us/sample - loss: 0.1162 - accuracy: 0.9667 - val_loss: 15.2839 - val_accuracy: 0.9669 Epoch 4/6 60000/60000 [==============================] - 2s 29us/sample - loss: 0.0892 - accuracy: 0.9744 - val_loss: 14.4179 - val_accuracy: 0.9705 Epoch 5/6 60000/60000 [==============================] - 2s 29us/sample - loss: 0.0718 - accuracy: 0.9792 - val_loss: 12.7250 - val_accuracy: 0.9751 Epoch 6/6 60000/60000 [==============================] - 2s 29us/sample - loss: 0.0587 - accuracy: 0.9832 - val_loss: 13.1930 - val_accuracy: 0.9725 Helper function for plotting model accuracy and loss for training and validation In [155]: def plot_accuracy_loss ( model_history ): plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plt . semilogx ( model_history . history [ 'accuracy' ], label = 'train_acc' , linewidth = 4 ) plt . semilogx ( model_history . history [ 'val_accuracy' ], label = 'val_acc' , linewidth = 4 , alpha =. 7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Accuracy' ) plt . legend () plt . subplot ( 1 , 2 , 2 ) plt . loglog ( model_history . history [ 'loss' ], label = 'train_loss' , linewidth = 4 ) plt . loglog ( model_history . history [ 'val_loss' ], label = 'val_loss' , linewidth = 4 , alpha =. 7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Loss' ) plt . legend () plt . show () In [156]: plot_accuracy_loss ( trained_mnist ) Two helper functions. Let inspect the performance visually In [157]: # Make a single prediction and validate it def example_NN_prediction ( dataset = x_test , model_ = model_mnist ): \"\"\" This tests our MNist FFNN by examining a single prediction on the test set and checking if it matches the real label. Arguments: n: if you select n then you will choose the nth test set \"\"\" mnist_preds = model_mnist . predict ( x_test ) all_predictions = np . argmax ( mnist_preds , axis = 1 ) n = np . random . choice ( 784 ) digit = x_test [ n ,:] actual_label = y_test [ n ] plt . imshow ( digit . reshape ( - 1 , 28 )) prediction_array = model_ . predict ( digit . reshape ( 1 , - 1 )) prediction = np . argmax ( prediction_array ) if prediction == y_test [ n ]: print ( \"The Mnist model correctly predicted:\" , prediction ) else : print ( \"The true label was\" , actual_label ) print ( \"The Mnist model incorrectly predicted:\" , prediction ) #################################################### # Make a many predictions and validate them ################################################### def example_NN_predictions ( model_ , dataset_ = x_test , response_ = y_test , get_incorrect = False ): \"\"\" This tests our MNist FFNN by examining 3 images and checking if our nueral network can correctly classify them. Arguments: model_ : the mnist model you want to check predictions for. get_incorrect (boolean): if True, the model will find 3 examples where the model made a mistake. Otherwise it just select randomly. \"\"\" dataset = dataset_ . copy () response = response_ . copy () # If get_incorrect is True, then get an example of incorrect predictions. # Otherwise get random predictions. if not get_incorrect : n = np . random . choice ( dataset . shape [ 0 ], size = 3 ) digits = dataset [ n ,:] actual_label = response [ n ] else : # Determine where the model is making mistakes: mnist_preds = model_mnist . predict ( dataset ) all_predictions = np . argmax ( mnist_preds , axis = 1 ) incorrect_index = all_predictions != response incorrect = x_test [ incorrect_index , :] # Randomly select a mistake to show: n = np . random . choice ( incorrect . shape [ 0 ], size = 3 ) digits = incorrect [ n ,:] # determine the correct label labels = response [ incorrect_index ] actual_label = labels [ n ] #get the predictions and make the plot: fig , ax = plt . subplots ( 1 , 3 , figsize = ( 12 , 4 )) ax = ax . flatten () for i in range ( 3 ): #show the digit: digit = digits [ i ,:] ax [ i ] . imshow ( digit . reshape ( 28 , - 1 )) #reshape the image to 28 by 28 for viewing # reshape the input correctly and get the prediction: prediction_array = model_ . predict ( digit . reshape ( 1 , - 1 )) prediction = np . argmax ( prediction_array ) #Properly label the prediction (correct vs incorrect): if prediction == actual_label [ i ]: ax [ i ] . set_title ( \"Correct Prediction: \" + str ( prediction )) else : ax [ i ] . set_title ( 'Incorrect Prediction: {} (True label: {} )' . format ( prediction , actual_label [ i ])) plt . tight_layout () In [158]: example_NN_prediction () The Mnist model correctly predicted: 0 In [159]: example_NN_predictions ( model_ = model_mnist , get_incorrect = False ) Let's see where the network makes the wrong predictions In [160]: example_NN_predictions ( model_ = model_mnist , get_incorrect = True ) Break Out Room 3 Try this on your own, with the Iris dataset that we saw in Section 5 Load and split the Iris dataset. Use Keras to build and train a network for fitting the data. Use two hidden layers of 32 neurons each with relu activation functions. Figure out how many output neurons you need. Use the training data set for training and testing set for evaluation. Train for 100 epochs and try Adam optimizer with learning rate 0.005 Use the sparse_categorical_crossentropy loss function. Plot the accuracy and loss by using the plot_accuracy_loss() In [166]: from sklearn import datasets In [162]: iris_data = datasets . load_iris () In [163]: X = pd . DataFrame ( data = iris_data . data , columns = iris_data . feature_names ) y = pd . DataFrame ( data = iris_data . target , columns = [ 'species' ]) In [164]: X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.7 , random_state = 41 ) In [168]: # %load '../solutions/sol_3.py' Rolling Average: A useful representation for the accuracy and loss In [173]: def get_rolling_avg ( arr , rolling = 10 ): return pd . Series ( arr ) . rolling ( rolling ) . mean () def plot_accuracy_loss_rolling ( model_history ): rollNum = 10 plt . figure ( figsize = [ 12 , 4 ]) plt . subplot ( 1 , 2 , 1 ) plt . semilogx ( get_rolling_avg ( model_history . history [ 'accuracy' ], rollNum ), label = 'train_acc' , linewidth = 4 ) plt . semilogx ( get_rolling_avg ( model_history . history [ 'val_accuracy' ], rollNum ), label = 'val_acc' , linewidth = 4 , alpha =. 7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Rolling Accuracy' ) plt . legend () plt . subplot ( 1 , 2 , 2 ) plt . loglog ( get_rolling_avg ( model_history . history [ 'loss' ], rollNum ), label = 'train_loss' , linewidth = 4 ) plt . loglog ( get_rolling_avg ( model_history . history [ 'val_loss' ], rollNum ), label = 'val_loss' , linewidth = 4 , alpha =. 7 ) plt . xlabel ( 'Epoch' ) plt . ylabel ( 'Rolling Loss' ) plt . legend () plt . show () In [174]: plot_accuracy_loss_rolling ( iris_trained ) Neural Networks are great, so far ... But, what about the overfitting problem ?? In [176]: # Increase the size of the testing set to encourage overfitting X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.6 , random_state = 41 ) model_iris = tf . keras . models . Sequential ([ tf . keras . layers . Input ( shape = ( 4 ,)), tf . keras . layers . Dense ( 32 , activation = 'relu' ), tf . keras . layers . Dense ( 32 , activation = 'relu' ), tf . keras . layers . Dense ( 3 , activation = 'softmax' ) ]) model_iris . compile ( loss = 'sparse_categorical_crossentropy' , optimizer = tf . keras . optimizers . Adam ( 0.005 ), metrics = [ 'accuracy' ], ) ################## # TRAIN THE MODEL ################## iris_trained_ofit = model_iris . fit ( x = X_train . to_numpy (), y = y_train . to_numpy (), verbose = 0 , epochs = 500 , validation_data = ( X_test . to_numpy (), y_test . to_numpy ()), ) In [177]: plot_accuracy_loss ( iris_trained_ofit ) In [178]: plot_accuracy_loss_rolling ( iris_trained_ofit ) Regularization is needed! Wait until the next section End of Section","tags":"sections","url":"sections/sec_9/"},{"title":"S-Section 09: Feed forward neural networks","text":"Jupyter Notebooks S-Section 9: Feed forward neural networks","tags":"sections","url":"sections/section9/"},{"title":"Lecture 29:Neural Networks 2 - Anatomy of NN & Design Choices","text":"In [0]: #Import libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras import models % matplotlib inline In [0]: # We set a random seed to ensure our results are reproducible seed = 399 np . random . seed ( seed ) tf . random . set_seed ( seed ) Multilayer Perceptron We're using a slightly altered version of heart data from the previous exercise to illustrate a concept. Notice how now a single sigmoid will not be able to do an acceptible job of classification. In [0]: #Read the data and assign predictor and reponse variables altered_heart_data = pd . read_csv ( 'data/altered_heart_data.csv' ) x = altered_heart_data . MaxHR . values y = altered_heart_data . AHD . values In [0]: #Plot the data plt . scatter ( x , y , alpha = 0.2 ) plt . xlabel ( 'MaxHR' ) plt . ylabel ( 'Heart Disease (AHD)' ); Now we will use Keras to construct our MLP. You only need to add the output layer. Each step is described in the comments. In [0]: ## First we instantiate a Keras model MLP = models . Sequential ( name = 'MLP' ) ## [Adding Layers] # Next we add a hidden layer with 2 nodes and a sigmoid activation function MLP . add ( layers . Dense ( 2 , activation = 'sigmoid' , input_shape = ( 1 ,), name = 'hidden' )) # Now add the output layer # Use the code above as an example. You only need to change the arguments # Choose number of nodes and activation and name it 'output' # (the input shape will be infered from the hidden layer's output shape) # your code here ________ # [Compilation] # here we set the loss to be minimized and a metric to monitor when fitting the MLP MLP . compile ( loss = 'binary_crossentropy' , metrics = [ 'accuracy' ]) This simple model benefits from setting some reasonable initial weights. During fitting these weights will be optimized. In [0]: # Get original random weights weights = MLP . get_weights () # Hidden layer # Weights weights [ 0 ][ 0 ] = np . array ([ 3.1 , - 3.2 ]) # Biases weights [ 1 ] = np . array ([ - 350. , 402. ]) # Output layer # Weights weights [ 2 ] = np . array ([[ 1.29 ],[ 1.11 ]]) # Biases weights [ 3 ] = np . array ([ - 1.11 ]) # Update weights MLP . set_weights ( weights ) You should always inspect your Keras model with the summary() method. Note the number of parameters in each layer: Hidden has 4 - it contains 2 nodes each with a weight and bias. Output has 3 - it has a weight for each node in the previous layer plus a bias term. In [0]: #View model summary MLP . summary () Now we fit (or 'train') the MLP on our data, updating the weights to minimize the loss we specified in the call to compile() . (There will be more details on how this update happens in future lectures). One full training cycle on our data is called an 'epoch.' Usually multiple epochs are required before a model converges. Specify a number of epochs to train for. In [0]: #Fit the model on the data by specifing number of epochs MLP . fit ( ___ ); We can plot the training history and observe that as the weights were updated our loss declined and the accuracy improved. In [0]: #Plotting the model history history = MLP . history . history fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) ax [ 0 ] . plot ( history [ 'loss' ], c = 'r' , label = 'loss' ) ax [ 0 ] . set_ylabel ( 'crossentropy loss' ) ax [ 1 ] . plot ( history [ 'accuracy' ], c = 'b' ) ax [ 1 ] . set_ylabel ( 'accuracy' ) for axis in ax : axis . set_xlabel ( 'epoch' ) Let's look at the individual outputs of the 2 nodes in the hidden layer. In [0]: # Create xs for input to predict on x_linspace = np . linspace ( np . min ( x ), np . max ( x ), 500 ) # Get output from the hidden layer nodes hidden = models . Model ( inputs = MLP . input , outputs = MLP . get_layer ( 'hidden' ) . output ) hidden_pred = hidden . predict ( x_linspace ) h1_pred = hidden_pred [:, 0 ] h2_pred = hidden_pred [:, 1 ] In [0]: #Plot output from h1 & h2 fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 10 )) ax . scatter ( x , y , alpha = 0.4 , label = 'Altered Heart Data' ) ax . plot ( x_linspace , h1_pred , lw = 4 , alpha = 0.6 , label = r '$h_1 = \\sigma(W_1x+\\beta_1)$' ) ax . plot ( x_linspace , h2_pred , lw = 4 , alpha = 0.6 , label = r '$h_2 = \\sigma(W_2x+\\beta_2)$' ) # Set title ax . set_title ( 'Hidden Layer Nodes' , fontsize = 24 ) # Create labels (very important!) ax . set_xlabel ( '$x$' , fontsize = 24 ) # Notice we make the labels big enough to read ax . set_ylabel ( '$y$' , fontsize = 24 ) ax . tick_params ( labelsize = 24 ) # Make the tick labels big enough to read ax . legend ( fontsize = 24 , loc = 'best' ); # Create a legend and make it big enough to read We can see that each node in the hidden layer indeed outputs a different sigmoid. Now let's look at how they are combined by the output layer. In [0]: # Get output layer predictions y_pred = MLP . predict ( x_linspace ) In [0]: # Plot predictions fig , ax = plt . subplots ( 1 , 1 , figsize = ( 14 , 10 )) ax . scatter ( x , y , alpha = 0.4 , label = r 'Altered Heart Data' ) ax . plot ( x_linspace , y_pred , lw = 4 , label = r '$y_ {pred} = \\sigma(W_3h_ {1} + W_4h_ {2} +\\beta_ {3} )$' ) # Create labels (very important!) ax . set_xlabel ( '$x$' , fontsize = 24 ) # Notice we make the labels big enough to read ax . set_ylabel ( '$y$' , fontsize = 24 ) ax . tick_params ( labelsize = 24 ) # Make the tick labels big enough to read ax . legend ( fontsize = 24 , loc = 'best' ); # Create a legend and make it big enough to read Finally, let's compare the MLP's accuracy to a baseline that always predicts the majority class. Try and see if you can get over 80% accuracy (86%+ is possible). You may need to change the number of epochs above and rerun the notebook. In [0]: def accuracy ( y_true , y_pred ): assert y_true . shape [ 0 ] == y_pred . shape [ 0 ] return sum ( y_true == ( y_pred >= 0.5 ) . astype ( int )) / len ( y_true ) In [0]: ### edTest(test_performance) ### final_pred = MLP . predict ( x ) . flatten () baseline_acc = accuracy ( y , np . zeros ( len ( y ))) # predictions are all zeros MLP_acc = accuracy ( y , final_pred ) print ( f 'Baseline Accuracy: { baseline_acc : .2% } ' ) print ( f 'MLP Accuracy: { MLP_acc : .2% } ' )","tags":"labs","url":"labs/lecture-29/notebook/"},{"title":"Lecture 29:Neural Networks 2 - Anatomy of NN & Design Choices","text":"Slides PDF | Lecture 29: Anatomy of NN PPTX | Lecture 29: Anatomy of NN PDF | Lecture 29: Design Layers PPTX | Lecture 29: Design Layers Exercises Lecture 29: A.1 - Constructing an MLP [Notebook]","tags":"lectures","url":"lectures/lecture29/"},{"title":"Lecture 28: Neural Networks 1 - Perceptron & MLP","text":"In [0]: #Import the libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline In [0]: #Read the dataset and take a quick look heart_data = pd . read_csv ( 'data/Heart.csv' , index_col = 0 ) heart_data . head () In [0]: #Assign the predictor and reponse variables x = heart_data . ___ . ___ #Remember to replace the string column values to 0 and 1 y = heart_data . ___ . ___ . ___ In [0]: #Plot the predictor and reponse vairables as a scatter plot with axes label plt . scatter ( ___ ) plt . xlabel ( ___ ) plt . ylabel ( ___ ) plt . legend ( loc = 'best' ) Construct the components of our percpetron. In [0]: ### edTest(test_affine) ### def affine ( x , w , b ): \"\"\"Return affine transformation of x INPUTS ====== x: A numpy array of points in x w: A float representing the weight of the perceptron b: A float representing the bias of the perceptron RETURN ====== z: A numpy array of points after the affine transformation \"\"\" # your code here z = ___ return z In [0]: ### edTest(test_sigmoid) ### def sigmoid ( z ): # hint: numpy has an exponentiation function, np.exp() # your code here h = ___ return h In [0]: ### edTest(test_neuron_predict) ### def neuron_predict ( x , w , b ): #Call the previous functions # your code here h = ___ return h Manually set the weight and bias parameters. Recall from lecture that the weight changes the slope of the sigmoid and the bias shifts the function to the left or right. In [0]: # Hint: try values between -1 and 1 w = ___ # Hint: try values between 50 and 100 b = ___ Use the perceptron to make predictions and plot our results. In [0]: # The forward mode or predict of a single neuron # Create evenly spaced values of x to predict on x_linspace = np . linspace ( x . min (), x . max (), 500 ) h = neuron_predict ( x_linspace , w , b ) In [0]: # Plot Predictions fig , ax = plt . subplots ( 1 , 1 , figsize = ( 11 , 7 )) ax . scatter ( x , y , label = r 'Heart Data' , alpha = 0.2 ) ax . plot ( x_linspace , h , lw = 2 , c = 'orange' , label = r 'Single Neuron' ) # first value in x_linspace with a probability < 0.5 db = x_linspace [ np . argmax ( h < 0.5 )] ax . axvline ( x = db , alpha = 0.3 , linestyle = '-.' , c = 'r' , label = 'Decision Boundary' ) # Proper plot labels are very important! # Make the tick labels big enough to read ax . tick_params ( labelsize = 16 ) plt . xlabel ( 'MaxHR' , fontsize = 16 ) plt . ylabel ( 'Heart Disease (AHD)' , fontsize = 16 ) # Create a legend and make it big enough to read ax . legend ( fontsize = 16 , loc = 'best' ) plt . show () One way to assess our perceptron model's performance is to look at the binary cross entropy loss. In [0]: def loss ( y_true , y_pred , eps = 1e-15 ): assert y_true . shape [ 0 ] == y_pred . shape [ 0 ] # Clipping y_pred = np . clip ( y_pred , eps , 1 - eps ) return - sum ( y_true * np . log ( y_pred ) + ( 1 - y_true ) * ( np . log ( 1 - y_pred ))) In [0]: ## Print the loss h = neuron_predict ( x , w , b ) print ( loss ( y , h )) To ensure our perceptron model is not trivial we need to compare its accuracy to a baseline which always predicts the majority class (i.e., no heart disease). Play with your weights above and rerun the notebook until you can outperform the baseline. In [0]: def accuracy ( y_true , y_pred ): assert y_true . shape [ 0 ] == y_pred . shape [ 0 ] return sum ( y_true == ( y_pred >= 0.5 ) . astype ( int )) / len ( y_true ) In [0]: ### edTest(test_performance) ### # For the baseline predictions are all ones baseline_acc = accuracy ( y , np . ones ( len ( y ))) perceptron_acc = accuracy ( y , h ) print ( f 'Baseline Accuracy: { baseline_acc : .2% } ' ) print ( f 'Perceptron Accuracy: { perceptron_acc : .2% } ' )","tags":"labs","url":"labs/lecture-28/notebook/"},{"title":"Lecture 28: Neural Networks 1 - Perceptron & MLP","text":"In [1]: #Import library import numpy as np import matplotlib.pyplot as plt import tensorflow as tf from sklearn.datasets import load_iris from tensorflow.keras.utils import to_categorical % matplotlib inline In [2]: #Load the iris data iris_data = load_iris () #Get the predictor and reponse variables X = iris_data . data y = iris_data . target #See the shape of the data print ( f 'X shape: { X . shape } ' ) print ( f 'y shape: { y . shape } ' ) X shape: (150, 4) y shape: (150,) In [3]: #One-hot encode target labels Y = to_categorical ( y ) print ( f 'Y shape: { Y . shape } ' ) Y shape: (150, 3) Load and inspect the pre-trained weights and biases. Compare their shapes to the NN diagram. In [4]: #Load and inspect the pre-trained weights and biases weights = np . load ( 'data/weights.npy' , allow_pickle = True ) # weights for hidden (1st) layer w1 = weights [ 0 ] # biases for hidden (1st) layer b1 = weights [ 1 ] # weights for output (2nd) layer w2 = weights [ 2 ] #biases for output (2nd) layer b2 = weights [ 3 ] In [5]: #Compare their shapes to that in the NN diagram. for arr , name in zip ([ w1 , b1 , w2 , b2 ], [ 'w1' , 'b1' , 'w2' , 'b2' ]): print ( f ' { name } - shape: { arr . shape } ' ) print ( arr ) print () w1 - shape: (4, 3) [[-0.42714605 -0.72814226 0.37730372] [ 0.39002347 -0.73936987 0.7850246 ] [ 0.12336338 -0.7267647 -0.48210236] [ 0.20957732 -0.7505736 -1.3789996 ]] b1 - shape: (3,) [0. 0. 0.31270522] w2 - shape: (3, 3) [[ 0.7043929 0.13273811 -0.845736 ] [-0.8318007 -0.6977086 0.75894 ] [ 1.1978723 0.14868832 -0.473792 ]] b2 - shape: (3,) [-1.2774311 0.45491916 0.73040146] For the first affine transformation we need to multiple the augmented input by the first weight matrix (i.e., layer). $$ \\begin{bmatrix} 1 & X_{11} & X_{12} & X_{13} & X_{14}\\\\ 1 & X_{21} & X_{22} & X_{23} & X_{24}\\\\ \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & X_{n1} & X_{n2} & X_{n3} & X_{n4}\\\\ \\end{bmatrix} \\begin{bmatrix} b_{1}&#94;1 & b_{2}&#94;1 & b_{3}&#94;1\\\\ W_{11}&#94;1 & W_{12}&#94;1 & W_{13}&#94;1\\\\ W_{21}&#94;1 & W_{22}&#94;1 & W_{23}&#94;1\\\\ W_{31}&#94;1 & W_{32}&#94;1 & W_{33}&#94;1\\\\ W_{41}&#94;1 & W_{42}&#94;1 & W_{43}&#94;1\\\\ \\end{bmatrix} = \\begin{bmatrix} z_{11}&#94;1 & z_{12}&#94;1 & z_{13}&#94;1\\\\ z_{21}&#94;1 & z_{22}&#94;1 & z_{23}&#94;1\\\\ \\vdots & \\vdots & \\vdots \\\\ z_{n1}&#94;1 & z_{n2}&#94;1 & z_{n3}&#94;1\\\\ \\end{bmatrix} = \\textbf{Z}&#94;{(1)} $$ About the notation: superscript refers to the layer and subscript refers to the index in the particular matrix. So $W_{23}&#94;1$ is the weight in the 1st layer connecting the 2nd input to 3rd hidden node. Compare this matrix representation to the slide image. Also note the bias terms and ones that have been added to 'augment' certain matrices. You could consider $b_1&#94;1$ to be $W_{01}&#94;1$. 1. Augment X with a column of ones to create `X_aug` 2. Create the first layer weight matrix `W1` by vertically stacking the bias vector `b1`on top of `w1` (consult `add_ones_col` for ideas. Don't forget your `Tab` and `Shift+Tab` tricks!) 3. Do the matrix multiplication to find `Z1` In [0]: def add_ones_col ( X ): '''Augment matrix with a column of ones''' X_aug = np . hstack (( np . ones (( X . shape [ 0 ], 1 )), X )) return X_aug In [0]: #Use add_ones_col() X_aug = add_ones_col ( ___ ) #Use np.vstack to add biases to weight matrix W1 = np . vstack (( ___ , ___ )) #Use np.dot() to multiple X_aug and W1 Z1 = np . dot ( ___ , ___ ) Next, we use our non-linearity $$ \\textit{a}_{\\text{relu}}(\\textbf{Z}&#94;{(1)})= \\begin{bmatrix} h_{11} & h_{12} & h_{13}\\\\ h_{21} & h_{22} & h_{23}\\\\ \\vdots & \\vdots & \\vdots \\\\ h_{n1} & h_{n2} & h_{n3}\\\\ \\end{bmatrix}= \\textbf{H} $$ 1. Define the ReLU activation 2. use `plot_activation_func` to confirm implementation 3. Use relu on `Z1` to create `H` In [0]: def relu ( z : np . array ) -> np . array : # hint: # relu(z) = 0 when z < 0 # otherwise relu(z) = z # your code here h = np . maximum ( ___ , ___ ) # np.maximum() will help return h In [0]: #Helper code to plot the activation function def plot_activation_func ( f , name ): lin_x = np . linspace ( - 10 , 10 , 200 ) h = f ( lin_x ) plt . plot ( lin_x , h ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( f ' { name } Activation Function' ) plot_activation_func ( relu , name = 'RELU' ) In [0]: # use your relu activation function on Z1 H = relu ( ___ ) The next step is very similar to the first and so we've filled it in for you. $$ \\begin{bmatrix} 1 & h_{11} & h_{12} & h_{13}\\\\ 1 & h_{21} & h_{22} & h_{23}\\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & h_{n1} & h_{n2} & h_{n3}\\\\ \\end{bmatrix} \\begin{bmatrix} b_{1}&#94;{(2)} & b_{2}&#94;2 & b_{3}&#94;2\\\\ W_{11}&#94;2 & W_{12}&#94;2 & W_{13}&#94;2\\\\ W_{21}&#94;2 & W_{22}&#94;2 & W_{23}&#94;2\\\\ W_{31}&#94;2 & W_{32}&#94;2 & W_{33}&#94;2\\\\ \\end{bmatrix}= \\begin{bmatrix} z_{11}&#94;2 & z_{12}&#94;2 & z_{13}&#94;2\\\\ z_{21}&#94;2 & z_{22}&#94;2 & z_{23}&#94;2\\\\ \\vdots & \\vdots & \\vdots \\\\ z_{n1}&#94;2 & z_{n2}&#94;2 & z_{n3}&#94;2\\\\ \\end{bmatrix} = \\textbf{Z}&#94;{(2)} $$ 1. Augment `H` with ones to create `H_aug` 2. Combine `w2` and `b2` to create the output weight matric `W2` 3. Perform the matrix multiplication to produce `Z2` In [0]: #Use add_ones_col() H_aug = ___ #Use np.vstack to add biases to weight matrix W2 = ___ #Use np.dot() Z2 = np . dot ( H_aug , W2 ) Finally we use the softmax activation on Z2 . Now for each observation we have an output vector of length 3 which can be interpreted as a probability (they sum to 1). $$ \\textit{a}_{\\text{softmax}}(\\textbf{Z}&#94;2)= \\begin{bmatrix} \\hat{y}_{11} & \\hat{y}_{12} & \\hat{y}_{13}\\\\ \\hat{y}_{21} & \\hat{y}_{22} & \\hat{y}_{23}\\\\ \\vdots & \\vdots & \\vdots \\\\ \\hat{y}_{n1} & \\hat{y}_{n2} & \\hat{y}_{n3}\\\\ \\end{bmatrix} = \\hat{\\textbf{Y}} $$ 1. Define softmax 2. Use `softmax` on `Z2` to create `Y_hat` In [0]: def softmax ( z : np . array ) -> np . array : ''' Input: z - 2D numpy array of logits rows are observations, classes are columns Returns: y_hat - 2D numpy array of probabilities rows are observations, classes are columns ''' # hint: we are summing across the columns y_hat = np . exp ( ___ ) / np . sum ( np . exp ( ___ ), axis = ___ , keepdims = True ) return y_hat In [0]: #Calling the softmax function Y_hat = softmax ( ___ ) Now let's see how accuract the model's predictions are! Use `np.argmax` to collapse the columns of `Y_hat` to create `y_hat`, a vector of class labels like the original `y` before one-hot encoding. In [0]: ### edTest(test_acc) ### # Compute the accuracy y_hat = np . argmax ( ___ , axis = ___ ) acc = sum ( y == y_hat ) / len ( y ) print ( f 'accuracy: { acc : .2% } ' )","tags":"labs","url":"labs/lecture-28/notebook-2/"},{"title":"Lecture 28: Neural Networks 1 - Perceptron & MLP","text":"Slides PDF | Lecture 1: Perceptron & MLP A PPTX | Lecture 1: Perceptron & MLP A PDF | Lecture 1: Perceptron & MLP B PPTX | Lecture 1: Perceptron & MLP B Exercises Lecture 28: A.1 - Build a Single Neuron by Hand [Notebook] Lecture 28: B.1 - MLP by Hand [Notebook]","tags":"lectures","url":"lectures/lecture28/"},{"title":"Lecture 27: Case Study 2","text":"CS109A Introduction to Data Science Case Study: Hunting for Flavors PARTS 1- 4: Problem Statement, Obtaining Data, Cleaning Data, Exploring Data Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner In [1]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2020-CS109A/master/themes/static/css/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: # import the necessary libraries import re import requests import random import pandas as pd import matplotlib import matplotlib.pyplot as plt from matplotlib.ticker import PercentFormatter import numpy as np from time import sleep from bs4 import BeautifulSoup # global properties data_dir = \"data/\" # where to save data num_search_pages = 50 # how many search pages to cull through # NOTE: # if you haven't yet downloaded the data, this should be set to True download_data = False Disclaimer Alcohol is drug. There are state and federal laws that govern the sale, distribution, and consumption of such. In the United States, those who consume alcohol must be at least 21 years of age. In no way am I, or anyone else at IACS or Harvard at large, promoting or encouraging the usage of alcohol. My intention is not to celebrate it. Anyone who chooses to consume alcohol should be of legal age and should do so responsibly. Abusing alcohol has serious, grave effects. The point of this exercise is purely pedagogical, and it illustrates the wide range of tasks to which one can apply data science and machine learning. That is, I am focusing on a particular interest and demonstrating how it can be used to answer questions that one may be interested in for one's own personal life. You could easily imagine this being used in professional settings, too. Learning Objectives Help see the big picture process of conducting a project, and to illustrate some of the nuanced details and common pitfalls. 1. Problem Overview Whiskey is a type of alcohol, and there are many different types of whiskey, including bourbon, which will be the focus of this project. I am interested in determining: Are there certain attributes of bourbons that are predictive of good (i.e., highly rated by users) bourbons? Find hidden gems (i.e., should be good but current reviews are absent or unsupportive of such) Find over-hyped whiskeys (i.e., the reviews seem high but the attributes aren't indicative) Are there significant results if we target experts' ratings instead of average customer ratings? Are there certain attributes of bourbons that are predictive of expensive bourbons? Find under-priced whiskeys Find over-priced whiskeys Which bourbons are more similar to each other? Which attributes are important for determining similarness? (e.g., does price play a role?) 2. Obtaining Data We need a website that has a bunch of whiskey data. Distiller.com seems to be the most authoritative and comprehensive site. Using distiller.com as our source, I don't see a way to display a list of all of their bourbons. But, if you search for the keyword bourbon , over 2,000 search results appear, each with a link to the particular whiskey. After manual inspection, these in fact are bourbons, but a few are not and merely have some association with bourbon (e.g., non-bourbon whiskeys that were casked in old bourbon barrels). Let's crawl the search results pages to create a set of all candidate bourbons (i.e., whiskey_urls )! Then, using this set, let's download each page. Note, we use a set() instead of a list, in case there are duplicates. Fetching list of webpages via Requests In [3]: whiskey_urls = set () if download_data : # we define this for convenience, as every state's url begins with this prefix base_url = 'https://distiller.com/search?term=bourbon' # visits each search result page for page_num in range ( 1 , num_search_pages ): cur_page = requests . get ( 'https://distiller.com/search?page=' + str ( page_num ) + '&term=bourbon' ) # uses BeautifulSoup to extract all links to whiskeys bs_page = BeautifulSoup ( cur_page . content , \"html.parser\" ) for link in bs_page . findAll ( 'a' , attrs = { 'href' : re . compile ( \"&#94;/spirits/\" )}): whiskey_urls . add ( link . get ( 'href' )) sleep ( 1 ) # saves each URL to disk, so that we don't have to crawl the search results again f = open ( \"whiskey_urls.txt\" , \"w\" ) for url in whiskey_urls : f . write ( url + \" \\n \" ) f . close () # fetches each page and saves it to the hard drive for url in whiskey_urls : cur_page = requests . get ( 'https://distiller.com' + url ) . content # writes file f = open ( data_dir + url [ 9 :], 'wb' ) f . write ( cur_page ) f . close () # sleeps between 1-3 seconds, in case the site tries to detect crawling sleep ( random . randint ( 1 , 3 )) else : # if the files have already been saved to disk # then you can just load them here, instead of crawling again with open ( 'whiskey_urls.txt' ) as f : whiskey_urls = set ( line . strip () for line in f ) We now have a list of all whiskey urls, in whiskey_urls , along with the actual page downloaded to our hard drive. We downloaded them to the hard drive for convenience, so that in the future, we don't have to spend the 2 hours crawling all pages again. Let's now load each of these pages! In [4]: whiskeys = {} # loads whiskey webpage for i , url in enumerate ( whiskey_urls ): filename = data_dir + url [ 9 :] file_contents = open ( filename , 'r' ) . read () # instantiates a new BeautifulSoup object soup = BeautifulSoup ( file_contents , \"html.parser\" ) # extracts details about the whiskey name = soup . find ( 'h1' , attrs = { 'class' : re . compile ( \"secondary-headline name\" )}) . text . strip () location = soup . find ( 'h2' , attrs = { 'class' : \"ultra-mini-headline location middleweight\" }) . text . strip () soup . find ( 'div' , attrs = { 'class' : \"spirit-badge\" }) badge = \"\" if soup . find ( 'div' , attrs = { 'class' : \"spirit-badge\" }) != None : badge = soup . find ( 'div' , attrs = { 'class' : \"spirit-badge\" }) . text . strip () num_ratings = 0 rating = \"N/A\" if soup . find ( 'span' , attrs = { 'itemprop' : \"ratingCount\" }) != None : num_ratings = int ( soup . find ( 'span' , attrs = { 'itemprop' : \"ratingCount\" }) . text . strip ()) rating = float ( soup . find ( 'span' , attrs = { 'itemprop' : \"ratingValue\" }) . text . strip ()) age = soup . find ( 'li' , attrs = { 'class' : \"detail age\" }) . find ( 'div' , attrs = 'value' ) . text . strip () price = int ( re . findall ( \"cost-(\\d)\" , str ( soup . find ( 'div' , attrs = { 'class' : re . compile ( \"spirit-cost\" )})))[ 0 ]) abv = \"\" if soup . find ( 'li' , attrs = { 'class' : \"detail abv\" }) . find ( 'div' , attrs = 'value' ) . text != \"\" : abv = float ( soup . find ( 'li' , attrs = { 'class' : \"detail abv\" }) . find ( 'div' , attrs = 'value' ) . text ) whiskey_type = soup . find ( 'li' , attrs = { 'class' : \"detail whiskey-style\" }) . div . text cask_type = \"\" if soup . find ( 'li' , attrs = { 'class' : \"detail cask-type\" }) != None : cask_type = soup . find ( 'li' , attrs = { 'class' : \"detail cask-type\" }) . find ( 'div' , attrs = 'value' ) . text . strip () review = \"\" expert = \"\" score = \"\" flavor_summary = \"\" flavor_profile = [] # check if an expert reviewed it if soup . find ( 'p' , attrs = { 'itemprop' : \"reviewBody\" }) != None : review = soup . find ( 'p' , attrs = { 'itemprop' : \"reviewBody\" }) . text . replace ( \" \\\" \" , \"\" ) . strip () expert = soup . find ( 'div' , attrs = { 'class' : 'meet-experts' }) . a . text . strip () score = int ( soup . find ( 'div' , attrs = { 'class' : \"distiller-score\" }) . span . text . strip ()) flavor_summary = soup . find ( 'h3' , attrs = { 'class' : \"secondary-headline flavors middleweight\" }) . text . strip () # extracts flavor profile flavor_profile = eval ( soup . find ( 'canvas' ) . attrs [ 'data-flavors' ]) cur_whiskey = [ name , whiskey_type , cask_type , location , age , abv , price , badge , num_ratings , \\ rating , flavor_summary , expert , score ] if flavor_profile : cur_whiskey . extend ( list ( flavor_profile . values ())) else : cur_whiskey . extend ( np . zeros ( 14 )) cur_whiskey . append ( review ) whiskeys [ i ] = cur_whiskey df = pd . DataFrame . from_dict ( whiskeys , orient = 'index' , \\ columns = [ 'Name' , 'Type' , 'Cask' , 'Location' , 'Age' , 'ABV %' , 'Price' , 'Badge' , \\ '# Ratings' , \"Customers' Rating\" , 'Flavor Summary' , 'Expert' , 'Expert Score' , \\ 'Smoky' , 'Peaty' , 'Spicy' , 'Herbal' , 'Oily' , 'Full-bodied' , 'Rich' , \\ 'Sweet' , 'Briny' , 'Salty' , 'Vanilla' , 'Tart' , 'Fruity' , 'Floral' , 'Review' ]) 3. Data Sanity Check / Cleaning What do our features look like? Are any features wonky, inconsistent, useless, or missing values? Let's use only the whiskeys that have been reviewed by experts In [5]: pd . set_option ( 'display.max_columns' , None ) df2 = df . loc [( df [ 'Expert' ] != \"\" )] print ( len ( df2 )) 710 In [6]: df2 [ 'Type' ] . value_counts () Out[6]: Bourbon 586 Single Malt 27 Blended American Whiskey 14 Aged Rum 12 Peated Single Malt 11 Other Whiskey 11 Flavored Whiskey 5 Gold Rum 4 Rhum Agricole Vieux 4 Tequila Reposado 4 Blended 3 American Single Malt 3 Spiced Rum 3 Tequila Añejo 3 Flavored Rum 2 Barrel-Aged Gin 2 Rye 2 Canadian 2 Dark Rum 2 Cachaça 2 Other Brandy 1 Rhum Agricole Éléve Sous Bois 1 Rhum Agricole Blanc 1 Dairy/Egg Liqueurs 1 Silver Rum 1 White 1 Old Tom Gin 1 Other Liqueurs 1 Name: Type, dtype: int64 Let's only use the bourbons. We have 586 bourbons, which is my primary focus. This isn't tons, but the non-bourbons will likely add noise, as they are different alcohols. In [7]: pd . set_option ( 'display.max_rows' , None ) df2 = df2 . loc [( df [ 'Type' ] == \"Bourbon\" )] Let's inspect the data types In [8]: df2 . dtypes Out[8]: Name object Type object Cask object Location object Age object ABV % object Price int64 Badge object # Ratings int64 Customers' Rating object Flavor Summary object Expert object Expert Score object Smoky float64 Peaty float64 Spicy float64 Herbal float64 Oily float64 Full-bodied float64 Rich float64 Sweet float64 Briny float64 Salty float64 Vanilla float64 Tart float64 Fruity float64 Floral float64 Review object dtype: object \"Customers' Rating\" feature should be a Float. Let's fix it. In [9]: df2 . loc [ df2 [ 'Customers \\' Rating' ] == \"N/A\" ] Out[9]: Name Type Cask Location Age ABV % Price Badge # Ratings Customers' Rating Flavor Summary Expert Expert Score Smoky Peaty Spicy Herbal Oily Full-bodied Rich Sweet Briny Salty Vanilla Tart Fruity Floral Review 1765 Tacoma New West Bourbon Bourbon new, charred American oak Heritage Distilling Co. // Washington, USA NAS 46 2 0 N/A Vanilla & Sweet Brock Schulte 78 0.0 0.0 30.0 40.0 0.0 40.0 30.0 80.0 20.0 20.0 100.0 30.0 60.0 50.0 Nose is full of sweet corn, fresh caramel, and... In [10]: # there still exists 1 whiskey that has no Customer Rating, so let's remove it df2 = df2 . loc [ df2 [ 'Customers \\' Rating' ] != \"N/A\" ] df2 = df2 . astype ({ 'Customers \\' Rating' : 'float64' }) \"Age\" feature should represent years. Let's fix it. In [11]: # we can keep the 'Age' feature for now but be mindful # that it's missing for nearly half of the whiskeys len ( df2 . loc [( df2 [ 'Age' ] == 'NAS' ) | ( df2 [ 'Age' ] == 'nas' ) | ( df2 [ 'Age' ] == '' )]) Out[11]: 378 In [12]: # let's replace all missing values with a reasonable value. # for now, let's use 0 as a placeholder so that we can later swap it out. df2 [ 'Age' ] = df2 [ 'Age' ] . replace ([ 'NAS' , 'nas' , 'N/A' , '' ], '0' ) In [13]: # remove the 'Years' part of the text df2 [ 'Age' ] . replace ( to_replace = ' [yY]ear[sS]*' , value = '' , regex = True ) Out[13]: 0 0 4 0 12 7 y, 2 m,16 d 21 0 22 0 26 17 27 0 28 0 38 6 40 17 49 0 52 0 53 0 59 0 60 10 65 0 67 0 75 0 81 0 85 0 97 15 98 12 99 0 113 0 115 0 118 12 119 6 YR 3 MO 10 DY 126 0 127 0 128 0 129 0 130 0 136 0 140 22 143 0 147 0 148 0 149 3 151 0 153 0 154 6 YR 4 MO 12 DY 159 0 162 6 YR 4 MO 21 DY 164 0 165 0 166 11 171 0 176 0 180 0 181 0 185 0 188 13 191 0 193 0 195 0 197 0 198 0 202 9 207 0 217 0 224 0 230 0 231 17 236 6 256 0 257 14 258 6 263 0 264 0 265 0 269 12 276 0 279 0 282 0 283 0 284 0 285 0 287 0 291 0 292 0 293 0 296 8 303 15 308 7 yrs, 9 mos 309 0 313 0 315 9 to 11 318 7 321 0 322 15 325 0 326 0 328 9 Months 329 0 336 0 337 0 343 0 344 8 345 11 347 0 355 9 8 Months 357 0 365 0 369 13 371 0 373 9 374 0 380 0 391 0 398 12 404 0 410 0 412 0 415 0 416 0 417 9 419 0 421 0 422 0 424 10 429 0 434 0 435 0 437 23 439 0 440 4 442 0 446 0 448 10 449 0 453 0 460 0 465 0 468 6, 5 months 471 0 473 0 474 0 478 12 479 0 481 0 486 0 488 12 YR 5 MO 489 15 490 0 493 0 496 0 498 6 YR 3 MO 1 DY 508 0 514 10 516 6 YR 5 MO 1 DY 519 0 520 0 527 0 531 9 545 0 546 0 547 0 548 0 549 0 554 6 556 0 559 5 562 0 570 0 581 0 590 0 592 14 593 12 597 7 Y, 2 M, 28 D 605 12 609 10 611 0 614 12 618 0 620 0 622 12 YR 5 MO 623 1 626 6 YR 3 MO 6 DY 629 0 631 0 636 0 639 9 641 9 644 12 647 0 649 0 653 4 658 0 664 10 666 9 670 7 671 0 675 0 681 8 683 10 688 0 702 27 703 7.2 704 32 Months 705 0 710 0 714 6 719 18 - 20 months 722 12 727 0 736 0 751 0 752 0 754 23 755 15 770 0 772 7 775 0 776 0 777 0 782 0 790 0 791 11 793 0 800 0 801 0 824 0 826 0 828 0 831 0 832 21 838 0 842 0 843 0 850 0 851 0 861 8 863 7 865 0 868 0 871 12 882 0 885 0 888 0 895 0 899 5 904 0 916 11 917 10 918 0 928 0 931 0 932 0 937 0 939 0 940 0 942 0 945 8 953 0 960 0 963 12 970 0 971 0 975 12 976 0 979 12 980 0 982 0 990 0 993 14 996 0 1000 0 1002 0 1003 6 YR 10 MO 1 DY 1004 0 1005 0 1017 0 1026 0 1027 0 1031 0 1032 0 1051 0 1055 0 1056 0 1059 0 1063 8 1064 6 YR 4 MO 6 DY 1070 0 1073 6 YR 6 MO 19 DY 1080 0 1082 0 1088 6 1089 6 YR, 2 MO, 1 D 1095 0 1096 0 1098 10 1099 0 1100 0 1102 11 1108 0 1109 9 1114 0 1117 0 1118 0 1119 8 1122 7 1127 20 1128 0 1129 0 1130 7 1133 0 1134 0 1135 0 1149 0 1160 0 1161 0 1162 12 1164 0 1166 0 1170 0 1173 9 1181 0 1182 9 1185 0 1192 22 1193 25 1195 0 1196 6 1205 6 YR 2 MO 10 DY 1212 12 1218 4 1219 0 1227 0 1229 0 1237 0 1239 0 1240 0 1244 4 months 1250 0 1252 6-8 1253 0 1258 0 1260 0 1262 5 1270 0 1271 0 1275 9 1279 0 1283 0 1297 17 1298 0 1310 0 1317 20 1322 0 1324 14 1326 0 1330 0 1341 0 1348 24 1349 0 1355 0 1357 4 1358 0 1365 0 1371 0 1372 0 1376 10 1378 0 1379 0 1381 0 1382 0 1383 18 1384 0 1386 17 1390 17 1391 0 1394 0 1395 0 1396 6, 5 months 1404 0 1407 10 1414 0 1421 0 1423 0 1429 10 1434 0 1436 9 1437 0 1442 9 1443 20 1444 0 1447 3 1454 0 1455 0 1456 0 1463 15 1467 0 1481 0 1484 11 1485 8 1500 10 1502 0 1503 0 1504 0 1507 0 1511 20 1513 0 1514 0 1518 0 1523 0 1524 0 1525 3 1528 17 1529 0 1531 4 1537 0 1543 0 1545 17 1549 12 1550 6 YR 3 MO 14 DY 1553 0 1560 0 1570 0 1573 12 1575 0 1582 3 1583 0 1585 9 1589 22 1590 15 1592 0 1595 0 1596 0 1602 0 1604 5 1605 12 1607 12 1620 6 Y, 7 M, 23 D 1621 26 1627 0 1628 0 1634 0 1635 13 1637 0 1640 12 1641 0 1646 0 1648 0 1649 0 1650 0 1657 0 1664 10 1665 0 1668 10 1671 0 1674 6, 11 mo 1675 17 1676 0 1679 0 1681 0 1682 0 1686 0 1689 0 1700 10 1708 12 1710 0 1721 11 1725 0 1727 0 1728 8 YR 3 MO 1729 0 1732 0 1736 10 1738 0 1743 3 1744 0 1748 15 1749 8 1752 12 1757 23 1758 0 1759 0 1767 9 1768 0 1776 0 1778 0 1779 0 1785 0 1789 0 1794 0 1798 0 1813 10 1823 5 1826 0 1830 0 1832 0 1833 28 1834 12 1837 0 1839 0 1847 0 1849 0 1851 12 1855 0 1858 0 1861 0 1862 0 1864 0 1868 0 1873 0 1882 15 1884 5 YR 6 MO 1899 0 1903 0 1905 0 1909 0 1917 10 1920 0 1925 0 1926 7 1929 12 1932 12 1933 21 1936 10 1939 0 1945 0 1947 0 1952 0 1957 0 1959 0 1964 6 YR 8 MO 14 DY 1965 6 1969 0 1972 17 1978 0 1980 0 1981 0 1983 0 1991 16 1992 0 1999 13 2000 0 2004 11 2011 0 2012 0 2020 23 2026 0 2027 0 2037 12 2044 0 2045 0 2055 0 2062 0 2063 0 2066 0 2069 17 2071 0 2073 0 2078 12 2079 12 2083 0 2087 9 2088 11 2092 0 2096 0 2098 11 2099 0 2106 3 2107 10 2108 14 2110 3 2120 0 2125 0 2128 19 months 2129 9 2131 10 2134 0 2137 0 2138 0 2143 0 2156 6 YR 4 MO 2159 0 2160 0 2164 0 2167 0 2173 0 2179 0 2184 0 2185 12 2195 5 Name: Age, dtype: object In [14]: # manually cleaning up values that otherwise would be a bit impossible to automatically clean-up df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '6.*' , value = '6' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '(\\d+) [Yy].*' , value = ' \\\\ 1' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '4 [Mm]onths' , value = '4' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '9 [Mm]onths' , value = '9' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '18 - 20 [Mm]onths' , value = '1.5' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '32 [Mm]onths' , value = '2.67' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '9 [Mm]onths' , value = '9' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '9 to 11' , value = '0.75' , regex = True ) In [15]: # let's look at all of the items that had an Age statement listed # (now that all values have been cleaned-up) df2 . loc [ df2 [ 'Age' ] > '0' ][ 'Age' ] Out[15]: 12 7 26 17 38 6 40 17 60 10 97 15 98 12 118 12 119 6 140 22 149 3 154 6 162 6 166 11 188 13 202 9 231 17 236 6 257 14 258 6 269 12 296 8 303 15 308 7 315 0.75 318 7 322 15 328 9 344 8 345 11 355 9 369 13 373 9 398 12 417 9 424 10 437 23 440 4 448 10 468 6 478 12 488 12 489 15 498 6 514 10 516 6 531 9 554 6 559 5 592 14 593 12 597 7 605 12 609 10 614 12 622 12 623 1 626 6 639 9 641 9 644 12 653 4 664 10 666 9 670 7 681 8 683 10 702 27 703 7.2 704 2.67 714 6 719 1.5 722 12 754 23 755 15 772 7 791 11 832 21 861 8 863 7 871 12 899 5 916 11 917 10 945 8 963 12 975 12 979 12 993 14 1003 6 1063 8 1064 6 1073 6 1088 6 1089 6 1098 10 1102 11 1109 9 1119 8 1122 7 1127 20 1130 7 1162 12 1173 9 1182 9 1192 22 1193 25 1196 6 1205 6 1212 12 1218 4 1244 4 1252 6 1262 5 1275 9 1297 17 1317 20 1324 14 1348 24 1357 4 1376 10 1383 18 1386 17 1390 17 1396 6 1407 10 1429 10 1436 9 1442 9 1443 20 1447 3 1463 15 1484 11 1485 8 1500 10 1511 20 1525 3 1528 17 1531 4 1545 17 1549 12 1550 6 1573 12 1582 3 1585 9 1589 22 1590 15 1604 5 1605 12 1607 12 1620 6 1621 26 1635 13 1640 12 1664 10 1668 10 1674 6 1675 17 1700 10 1708 12 1721 11 1728 8 1736 10 1743 3 1748 15 1749 8 1752 12 1757 23 1767 9 1813 10 1823 5 1833 28 1834 12 1851 12 1882 15 1884 5 1917 10 1926 7 1929 12 1932 12 1933 21 1936 10 1964 6 1965 6 1972 17 1991 16 1999 13 2004 11 2020 23 2037 12 2069 17 2078 12 2079 12 2087 9 2088 11 2098 11 2106 3 2107 10 2108 14 2110 3 2128 19 2129 9 2131 10 2156 6 2185 12 2195 5 Name: Age, dtype: object In [16]: df2 = df2 . astype ({ 'Age' : 'float64' }) In [17]: # how many had values? len ( df2 . loc [ df2 [ 'Age' ] > 0 ]) Out[17]: 206 In [18]: df2 [ 'Age' ] . describe () Out[18]: count 585.000000 mean 3.776274 std 6.010627 min 0.000000 25% 0.000000 50% 0.000000 75% 7.000000 max 28.000000 Name: Age, dtype: float64 In [19]: df2 . loc [ df2 [ 'Age' ] > 0 ] . hist ( column = 'Age' , bins = 'auto' ) Out[19]: array([[ ]], dtype=object) I think it's fair to impute all missing values (i.e., 0) with 7. This is based on research, too (Googling and personal knowledge) In [20]: df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( 0 , 7 ) In [21]: df2 [ 'Age' ] . describe () Out[21]: count 585.000000 mean 8.311316 std 3.607727 min 0.750000 25% 7.000000 50% 7.000000 75% 7.000000 max 28.000000 Name: Age, dtype: float64 In [22]: df2 . hist ( column = 'Age' , bins = 'auto' ) Out[22]: array([[ ]], dtype=object) What's the distribution of the \"Flavor Summary\" feature? Is it consistent enough to use? In [23]: df2 [ 'Flavor Summary' ] . value_counts () Out[23]: Rich & Full Bodied 54 Sweet & Rich 40 Sweet 36 Vanilla & Sweet 34 Spicy 33 Vanilla & Rich 24 Full Bodied & Rich 20 Sweet & Vanilla 20 Spicy & Rich 18 Vanilla 18 Vanilla & Full Bodied 17 Fruity & Sweet 17 Full Bodied & Spicy 17 Spicy & Vanilla 16 Rich & Vanilla 13 Sweet & Spicy 13 Rich & Spicy 13 Full Bodied 11 Vanilla & Spicy 11 Full Bodied & Vanilla 10 Spicy & Sweet 10 Spicy & Full Bodied 10 Fruity 9 Rich 9 Sweet & Full Bodied 9 Rich & Sweet 9 Sweet & Fruity 8 Spicy & Fruity 7 Fruity & Rich 7 Spicy & Smoky 5 Fruity & Spicy 5 Fruity & Vanilla 4 Sweet & Oily 3 Floral & Fruity 3 Floral 3 Spicy & Herbal 3 Sweet & Herbal 3 Floral & Vanilla 2 Oily & Rich 2 Tart 2 Full Bodied & Fruity 2 Fruity & Floral 2 Rich & Oily 2 Sweet & Smoky 2 Full Bodied & Sweet 2 Fruity & Herbal 2 Spicy & Oily 1 Smoky & Spicy 1 Sweet & Briny 1 Oily & Sweet 1 Oily 1 Rich & Fruity 1 Vanilla & Floral 1 Sweet & Salty 1 Oily & Full Bodied 1 Spicy & Tart 1 Smoky & Sweet 1 Rich & Smoky 1 Herbal & Tart 1 Floral & Herbal 1 Full Bodied & Oily 1 Smoky & Vanilla 1 Vanilla & Fruity 1 Herbal 1 Smoky & Rich 1 Herbal & Floral 1 Tart & Vanilla 1 Floral & Sweet 1 Spicy & Floral 1 Herbal & Fruity 1 Fruity & Full Bodied 1 Name: Flavor Summary, dtype: int64 Ok, there's a long tail of values, and it seems the Flavors are just the two most prominent flavors listed for each whiskey, although some only list one flavor. Perhaps this offers no additional information/signal than using the raw values of the flavors. Although, it might be worth experimenting with this by turning this feature into 2 new features: primary flavor, secondary flavor. These would need to be one-hot encoded though, and since there are 14 distinct flavors, that would create 28 new features (or 26). Again, these 26 features might be redundant and not help our models. What is the \"Badge\" feature like? In [24]: df2 [ 'Badge' ] . value_counts () Out[24]: 428 RARE 119 Requested By\\nElw00t 2 Requested By\\njd139 1 Requested By\\ntjbriley 1 Requested By\\nBourbon_Obsessed_Lexington 1 Requested By\\nCymru-and-the-Ferg 1 Requested By\\ndanmeister33 1 Requested By\\nCblake34 1 Requested By\\ndjriebesell 1 Requested By\\nandrewls24 1 Requested By\\nspectorjuan 1 Requested By\\ncubfancccc 1 Requested By\\nsamueljcarlson 1 Requested By\\nJFForbes 1 Requested By\\nJamesSpears 1 Requested By\\nZonaPT 1 Requested By\\nrsbolen 1 Requested By\\nstevenblackburn7 1 Requested By\\nmcoop8 1 Requested By\\nSharksfan321 1 Requested By\\nEast17 1 Requested By\\ntkezo645 1 Requested By\\nalshepherd1 1 Requested By\\nSoba45 1 Requested By\\nBourbonPizon 1 Requested By\\nhomerhomerson 1 Requested By\\nGlengoolieBlue 1 Requested By\\nAJLovesWhiskey 1 Requested By\\ncpreynolds87 1 Requested By\\nEzikiel 1 Requested By\\njimcorwin3 1 Requested By\\nTmoore8601 1 Requested By\\nbodkins 1 Requested By\\nstonetone96 1 Requested By\\nGilly 1 Requested By\\nAWhite 1 Requested By\\nJacob-Haralson 1 Requested By\\n1901 1 Name: Badge, dtype: int64 We see that all Badge values are either 'RARE' or just requests from users for an expert to review it. So, let's change the badge column to being a 'Rare' column. In [25]: df2 [ 'Rare' ] = [ True if x == 'RARE' else False for x in df2 [ 'Badge' ]] #df['Badge'] #.map({\"RARE\": True}) del df2 [ 'Badge' ] df2 [ 'Rare' ] . value_counts () Out[25]: False 466 True 119 Name: Rare, dtype: int64 What is the \"Expert\" feature like? In [26]: df2 [ 'Expert' ] . value_counts () Out[26]: Jacob Grier 92 Jake Emen 85 Amanda Schuster 76 Stephanie Moreno 66 Rob Morton 62 Keith Allison 26 Colin Howard 23 Sam Davies 21 Nicole Gilbert 17 Distiller Staff 15 Brock Schulte 14 Paul Belbusti 13 Ryan Conklin 12 Jack Robertiello 10 Tim Knittel 10 Katrina Niemisto 8 Dennis Gobis 4 Ron Bechtol 4 Blair Phillips 4 Jason Albaum 3 Matthew Sheinberg 3 Thijs Klaverstijn 3 Derek Gamlin 2 Phil Olson 2 Liza Weisstuch 2 Lucas Gamlin 2 Michael J. Neff 2 Anna Archibald 1 Perri Salka 1 Eric Abert 1 Brad Japhe 1 Name: Expert, dtype: int64 Let's cast our features to the correct data types and view summary statistics In [27]: df2 = df2 . astype ({ 'Expert Score' : 'int32' , 'Customers \\' Rating' : 'float64' , 'ABV %' : 'float64' }) In [28]: df2 . describe () Out[28]: Age ABV % Price # Ratings Customers' Rating Expert Score Smoky Peaty Spicy Herbal Oily Full-bodied Rich Sweet Briny Salty Vanilla Tart Fruity Floral count 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 mean 8.311316 49.899838 2.803419 416.352137 3.756325 86.447863 21.485470 0.230769 53.726496 25.714530 30.695726 58.447863 57.798291 58.381197 4.018803 5.476923 50.340171 21.548718 37.066667 17.835897 std 3.607727 7.170325 1.055314 1097.906235 0.521737 5.737503 18.322834 2.187232 19.560069 19.547419 22.712536 18.317344 18.361300 16.824334 8.589855 9.603433 20.526266 17.517548 21.334758 20.336899 min 0.750000 40.000000 1.000000 1.000000 1.000000 65.000000 0.000000 0.000000 0.000000 0.000000 0.000000 5.000000 0.000000 10.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 7.000000 45.000000 2.000000 22.000000 3.460000 83.000000 10.000000 0.000000 40.000000 10.000000 10.000000 45.000000 45.000000 50.000000 0.000000 0.000000 35.000000 10.000000 20.000000 0.000000 50% 7.000000 47.500000 3.000000 83.000000 3.780000 87.000000 20.000000 0.000000 55.000000 25.000000 30.000000 60.000000 60.000000 60.000000 0.000000 0.000000 50.000000 20.000000 35.000000 10.000000 75% 7.000000 53.500000 4.000000 238.000000 4.170000 91.000000 30.000000 0.000000 70.000000 40.000000 45.000000 70.000000 70.000000 70.000000 5.000000 10.000000 70.000000 30.000000 50.000000 30.000000 max 28.000000 72.050000 5.000000 9072.000000 4.880000 98.000000 90.000000 40.000000 100.000000 90.000000 100.000000 100.000000 100.000000 99.000000 80.000000 80.000000 100.000000 75.000000 100.000000 95.000000 In [48]: df2 . dtypes Out[48]: Name object Type object Cask object Location object Age float64 ABV % float64 Price int64 # Ratings int64 Customers' Rating float64 Flavor Summary object Expert object Expert Score int32 Smoky float64 Peaty float64 Spicy float64 Herbal float64 Oily float64 Full-bodied float64 Rich float64 Sweet float64 Briny float64 Salty float64 Vanilla float64 Tart float64 Fruity float64 Floral float64 Review object Rare bool dtype: object 4. EDA Now that our data is cleaned, let's explore it and try to understand any patterns. This understanding will impact our modelling choices. Based on the .describe() statistics above, let's first look at the most extreme values of features that seem a bit lopsided in their distribution of values. Which are the most \"Smoky\" ? Intent is to see if there are any errors or something worth noting. In [29]: df2 . sort_values ( by = [ 'Smoky' ], ascending = False )[ 0 : 15 ][[ 'Name' , 'Smoky' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[29]: Name Smoky ABV % Price Customers' Rating Expert Score 0 Cleveland Bourbon 90.0 50.00 2 2.48 70 2055 Warbringer Mesquite Smoked Southwest Bourbon 80.0 49.00 3 4.16 85 1073 Booker's Bourbon Batch 2019-04 \"Beaten Biscuits\" 80.0 63.05 3 4.21 87 1595 Jim Beam Black Label Extra-Aged 80.0 43.00 1 3.28 84 1757 Pappy Van Winkle 23 Year 80.0 47.80 5 4.54 89 1181 Jim Beam Double Oak 75.0 43.00 2 3.21 82 1785 Rebel Yell Kentucky Straight Bourbon 100 Proof 75.0 50.00 1 3.46 86 315 Booker's 25th Anniversary Bourbon 75.0 65.40 4 4.45 96 1640 Elijah Craig Barrel Proof Bourbon 70.0 68.50 4 4.33 93 1447 Garrison Brothers Texas Straight Bourbon 70.0 47.00 4 3.51 84 1823 Barrell Bourbon Batch 001 70.0 60.80 3 4.35 88 556 Elk Rider Bourbon 70.0 46.00 2 3.02 79 2164 Old Forester Single Barrel Bourbon Barrel Stre... 70.0 65.00 3 3.80 84 365 Lexington Bourbon 70.0 43.00 2 2.77 70 1936 Parker's Heritage Heavy Char Bourbon 10 Year 65.0 60.00 4 4.88 92 Which are the most \"Peaty\" ? Intent is to see if there are any errors or something worth noting. In [30]: df2 . sort_values ( by = [ 'Peaty' ], ascending = False )[ 0 : 10 ][[ 'Name' , 'Peaty' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[30]: Name Peaty ABV % Price Customers' Rating Expert Score 1529 New Riff Backsetter Peated Backset Bourbon 40.0 50.0 2 3.06 84 1117 Backbone Prime Blended Bourbon 20.0 52.0 2 3.61 82 2055 Warbringer Mesquite Smoked Southwest Bourbon 15.0 49.0 3 4.16 85 562 J. Riddle Peated Bourbon 15.0 45.5 2 3.35 87 171 Evan Williams Single Barrel 10.0 43.3 2 3.87 96 1905 Old Bardstown Black Label Kentucky Straight Bo... 10.0 45.0 1 2.90 86 2179 Knob Creek Small Batch Bourbon 10.0 50.0 2 3.47 84 963 Lux Row Distillers Double Barrel Bourbon 12 Year 5.0 59.2 4 4.03 91 1000 Still & Oak Straight Bourbon 5.0 43.0 2 3.40 83 1240 Angel's Envy Bourbon Finished in Port Wine Bar... 5.0 62.0 5 4.26 92 Which are the least \"Spicy\" ? Intent is to see if there are any errors or something worth noting. In [31]: df2 . sort_values ( by = [ 'Spicy' ], ascending = True )[ 0 : 14 ][[ 'Name' , 'Spicy' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[31]: Name Spicy ABV % Price Customers' Rating Expert Score 1031 Dancing Pines Bourbon 0.0 44.00 2 3.09 82 224 Rebel Yell Kentucky Straight Bourbon 0.0 40.00 1 2.88 84 1729 Burnside Oregon Oaked Bourbon 10.0 48.00 2 3.43 84 618 Three Chord Blended Bourbon 10.0 40.50 2 3.10 72 681 Old Charter 8 Year 10.0 40.00 1 2.88 80 1381 Rough Rider Straight Bourbon 15.0 45.00 2 3.34 85 939 TX Straight Bourbon Whiskey 15.0 47.00 4 3.47 84 1778 Orange County Distillery Bourbon 15.0 45.00 3 3.38 79 671 County Seat Spirits Hidden Copper Bourbon 15.0 45.00 2 3.50 80 1991 Black Maple Hill 16 Year Bourbon 15.0 47.50 4 4.62 97 130 Early Times 354 Bourbon 15.0 40.00 1 3.03 87 256 Larceny Small Batch Kentucky Straight Bourbon 15.0 46.00 1 3.47 88 236 Wild Turkey Rare Breed 15.0 54.10 2 3.84 92 1129 George T. Stagg Bourbon (Fall 2014) 15.0 69.05 3 4.53 92 Which are the most \"Herbal\" ? Intent is to see if there are any errors or something worth noting. In [32]: df2 . sort_values ( by = [ 'Herbal' ], ascending = False )[ 0 : 20 ][[ 'Name' , 'Herbal' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[32]: Name Herbal ABV % Price Customers' Rating Expert Score 592 Remus Volstead Reserve 14 Year Bottled in Bond... 90.0 50.0 5 4.25 89 1349 Temperance Trader Chinato Barrel-Finished Bourbon 90.0 45.0 2 3.61 87 1503 Redemption High Rye Bourbon 82.0 46.0 2 3.35 82 1851 Elijah Craig Barrel Proof Bourbon Batch C919 80.0 68.4 3 4.33 91 865 Treaty Oak Red Handed Bourbon (Kentucky & Virg... 80.0 47.5 2 3.68 83 373 Belle Meade Cask Strength Single Barrel Bourbo... 80.0 61.2 3 4.37 95 623 Cody Road Bourbon 78.0 45.0 2 2.72 71 895 Evan Williams White Label Bottled in Bond Bourbon 76.0 50.0 1 3.35 80 1830 Black Button Four Grain Bourbon 75.0 42.0 2 3.31 87 1442 Yellowstone Kentucky Straight Bourbon 9 Year (... 75.0 50.5 4 4.08 88 1371 Rock Hill Farms Bourbon 74.0 50.0 3 4.19 91 1637 Old Fitzgerald Bottled In Bond Bourbon 73.0 50.0 1 3.39 84 166 Jim Beam Signature Craft High Rye Bourbon 11 Year 70.0 45.0 4 3.50 84 1429 Henry McKenna 10 Year Bottled in Bond Bourbon 70.0 50.0 2 3.93 91 1095 Four Roses Small Batch Select Bourbon 70.0 52.0 4 4.00 89 343 St. Augustine Double Cask Bourbon 70.0 43.9 2 3.48 83 1748 I.W. Harper 15 Year Bourbon 70.0 43.0 3 4.06 94 38 Heaven Hill Bottled In Bond 6 Year 70.0 50.0 1 3.43 79 326 Four Roses Limited Edition Small Batch Bourbon... 70.0 56.3 4 4.48 93 258 Booker's Bourbon Batch 2015-04 \"Oven Buster Ba... 70.0 63.5 3 3.66 85 Which are the most \"Oily\" ? Intent is to see if there are any errors or something worth noting. In [33]: df2 . sort_values ( by = [ 'Oily' ], ascending = False )[ 0 : 15 ][[ 'Name' , 'Oily' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[33]: Name Oily ABV % Price Customers' Rating Expert Score 1757 Pappy Van Winkle 23 Year 100.0 47.80 5 4.54 89 1640 Elijah Craig Barrel Proof Bourbon 100.0 68.50 4 4.33 93 1442 Yellowstone Kentucky Straight Bourbon 9 Year (... 95.0 50.50 4 4.08 88 1851 Elijah Craig Barrel Proof Bourbon Batch C919 90.0 68.40 3 4.33 91 514 Parker's Heritage Cognac Barrel Finish 10 Year... 90.0 50.00 3 4.53 96 303 George T. Stagg Bourbon (Fall 2013) 90.0 64.10 5 4.62 97 865 Treaty Oak Red Handed Bourbon (Kentucky & Virg... 90.0 47.50 2 3.68 83 1182 Barrell Bourbon Batch 008 85.0 66.40 4 4.20 83 916 Four Roses Limited Edition Single Barrel Bourb... 80.0 54.20 4 4.25 98 1785 Rebel Yell Kentucky Straight Bourbon 100 Proof 80.0 50.00 1 3.46 86 2079 1792 Aged Twelve Years 80.0 48.30 2 3.88 88 329 Colonel E.H. Taylor, Jr. Small Batch Bottled i... 80.0 50.00 2 4.14 90 1073 Booker's Bourbon Batch 2019-04 \"Beaten Biscuits\" 80.0 63.05 3 4.21 87 1262 J. Henry & Sons 5 Year Wisconsin Straight Bour... 80.0 60.00 3 4.18 83 1524 Treaty Oak Ghost Hill Texas Bourbon 80.0 47.50 2 3.50 80 Which are the least \"Full-bodied\" ? Intent is to see if there are any errors or something worth noting. In [34]: df2 . sort_values ( by = [ 'Full-bodied' ], ascending = True )[ 0 : 20 ][[ 'Name' , 'Full-bodied' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[34]: Name Full-bodied ABV % Price Customers' Rating Expert Score 618 Three Chord Blended Bourbon 5.0 40.50 2 3.10 72 0 Cleveland Bourbon 10.0 50.00 2 2.48 70 1031 Dancing Pines Bourbon 10.0 44.00 2 3.09 82 1641 291 Colorado Bourbon 15.0 50.00 3 3.60 78 2027 Feisty Spirits Blue Corn Bourbon 15.0 44.00 2 2.50 81 496 Bird Dog Small Batch Bourbon 15.0 43.00 2 3.14 82 2063 Ancient Age 20.0 40.00 1 2.55 78 1218 Burnside Bourbon 20.0 48.00 2 3.14 79 658 Maryland Club Straight Bourbon 20.0 47.50 2 2.00 78 1391 Central Standard Bourbon 20.0 45.00 2 2.42 80 1423 Hudson Baby Bourbon 20.0 46.00 4 3.28 83 1513 Missouri Spirits Bourbon Whiskey 20.0 40.00 2 3.32 72 1543 J.W. Overbey Bourbon 20.0 45.00 4 3.00 69 2088 Jim Beam Signature Craft Whole Rolled Oat Bour... 20.0 45.00 4 3.59 81 415 New Holland Beer Barrel Bourbon 20.0 40.00 3 3.07 65 486 Double Diamond Limited Edition Bourbon 267 20.0 40.00 2 1.00 80 1026 Barrell Bourbon New Year 2019 20.0 56.05 4 3.97 77 1729 Burnside Oregon Oaked Bourbon 20.0 48.00 2 3.43 84 180 Feisty Spirits Better Days Bourbon 20.0 44.00 2 3.33 80 22 Delaware Phoenix Bourbon 20.0 50.00 3 3.00 75 Which are the most \"Briny\" ? Intent is to see if there are any errors or something worth noting. In [35]: df2 . sort_values ( by = [ 'Briny' ], ascending = False )[ 0 : 20 ][[ 'Name' , 'Briny' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[35]: Name Briny ABV % Price Customers' Rating Expert Score 1524 Treaty Oak Ghost Hill Texas Bourbon 80.0 47.50 2 3.50 80 1834 Elijah Craig 12 Year 60.0 47.00 2 3.80 93 1481 Jefferson's Ocean Aged at Sea Voyage 15 Specia... 50.0 45.00 4 3.87 82 527 Old Soul Blended Straight Bourbon 45.0 45.00 2 3.49 74 1882 Pappy Van Winkle Family Reserve 15 Year 40.0 53.50 4 4.53 89 224 Rebel Yell Kentucky Straight Bourbon 40.0 40.00 1 2.88 84 623 Cody Road Bourbon 40.0 45.00 2 2.72 71 1149 Murray Hill Club Blended Bourbon 40.0 51.00 4 3.49 82 1757 Pappy Van Winkle 23 Year 30.0 47.80 5 4.54 89 1447 Garrison Brothers Texas Straight Bourbon 30.0 47.00 4 3.51 84 664 Michter's 10 Year Single Barrel Bourbon 30.0 47.20 4 4.20 90 1117 Backbone Prime Blended Bourbon 30.0 52.00 2 3.61 82 861 1792 Ridgemont Reserve Bourbon 8 Year 30.0 46.85 2 3.61 90 171 Evan Williams Single Barrel 30.0 43.30 2 3.87 96 1905 Old Bardstown Black Label Kentucky Straight Bo... 30.0 45.00 1 2.90 86 1317 Pappy Van Winkle 20 Year 30.0 45.20 4 4.67 92 1205 Booker's Bourbon Batch 2018-02 Backyard BBQ 25.0 64.40 3 4.20 83 2055 Warbringer Mesquite Smoked Southwest Bourbon 25.0 49.00 3 4.16 85 990 Noah's Mill Bourbon 20.0 57.15 3 4.02 93 198 Old Bardstown Estate Bottled Kentucky Straight... 20.0 50.50 2 3.30 84 Which are the most \"Salty\" ? Intent is to see if there are any errors or something worth noting. In [36]: df2 . sort_values ( by = [ 'Salty' ], ascending = False )[ 0 : 20 ][[ 'Name' , 'Salty' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[36]: Name Salty ABV % Price Customers' Rating Expert Score 1524 Treaty Oak Ghost Hill Texas Bourbon 80.0 47.50 2 3.50 80 1864 Blaum Bros. Galena Reserve (Series 0) 55.0 57.80 3 4.50 86 207 Remus Repeal Reserve Series III Straight Bourbon 55.0 50.00 4 3.88 86 1481 Jefferson's Ocean Aged at Sea Voyage 15 Specia... 50.0 45.00 4 3.87 82 1834 Elijah Craig 12 Year 50.0 47.00 2 3.80 93 162 Booker's Bourbon Batch 2020-01 \"Granny's Batch\" 40.0 63.20 4 4.07 92 1837 Coppersea Excelsior Bourbon 40.0 48.00 4 2.88 81 1785 Rebel Yell Kentucky Straight Bourbon 100 Proof 40.0 50.00 1 3.46 86 1757 Pappy Van Winkle 23 Year 40.0 47.80 5 4.54 89 1882 Pappy Van Winkle Family Reserve 15 Year 40.0 53.50 4 4.53 89 2179 Knob Creek Small Batch Bourbon 40.0 50.00 2 3.47 84 1732 Colter's Run Bourbon 35.0 44.00 2 3.57 79 373 Belle Meade Cask Strength Single Barrel Bourbo... 30.0 61.20 3 4.37 95 293 Ezra Brooks Kentucky Straight Bourbon 90 Proof 30.0 45.00 1 3.41 88 171 Evan Williams Single Barrel 30.0 43.30 2 3.87 96 1317 Pappy Van Winkle 20 Year 30.0 45.20 4 4.67 92 1728 Barrell Bourbon Batch 005 30.0 62.35 4 4.07 81 224 Rebel Yell Kentucky Straight Bourbon 30.0 40.00 1 2.88 84 1130 Virgin Bourbon 7 Year 101 30.0 50.50 1 3.35 83 126 Henry DuYore's Straight Bourbon Whiskey 30.0 45.60 2 3.32 81 Which are the most \"Tart\" ? Intent is to see if there are any errors or something worth noting. In [37]: df2 . sort_values ( by = [ 'Tart' ], ascending = False )[ 0 : 20 ][[ 'Name' , 'Tart' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[37]: Name Tart ABV % Price Customers' Rating Expert Score 592 Remus Volstead Reserve 14 Year Bottled in Bond... 75.0 50.00 5 4.25 89 1925 Redemption Temptation Bourbon 74.0 41.00 2 2.91 77 895 Evan Williams White Label Bottled in Bond Bourbon 72.0 50.00 1 3.35 80 1524 Treaty Oak Ghost Hill Texas Bourbon 70.0 47.50 2 3.50 80 1768 Peach Street Colorado Straight Bourbon 70.0 46.00 3 3.59 83 782 Johnny Drum Green Label Bourbon 70.0 40.00 2 3.19 82 1933 Elijah Craig Single Barrel 21 Year 69.0 45.00 5 4.14 87 1371 Rock Hill Farms Bourbon 68.0 50.00 3 4.19 91 865 Treaty Oak Red Handed Bourbon (Kentucky & Virg... 65.0 47.50 2 3.68 83 176 Oola Waitsburg Bourbon 65.0 47.00 2 3.37 80 1503 Redemption High Rye Bourbon 65.0 46.00 2 3.35 82 1310 The Walking Dead Kentucky Straight Bourbon 65.0 47.00 2 3.09 83 448 Buffalo Trace Experimental Collection French O... 60.0 45.00 4 3.35 88 283 Calumet Farm Bourbon 60.0 43.00 2 3.10 72 1637 Old Fitzgerald Bottled In Bond Bourbon 60.0 50.00 1 3.39 84 1575 George T. Stagg Bourbon (Fall 2019) 60.0 58.45 4 4.59 98 710 Old Heaven Hill Gold Label Bottled In Bond Bou... 60.0 50.00 1 3.12 77 918 Daviess County Kentucky Straight Bourbon 60.0 48.00 2 3.32 89 851 Coopers' Craft Barrel Reserve Straight Bourbon 60.0 50.00 2 3.65 85 446 Duke Kentucky Straight Bourbon 60.0 44.00 2 3.18 83 Which are the most \"Fruity\" ? Intent is to see if there are any errors or something worth noting. In [38]: df2 . sort_values ( by = [ 'Fruity' ], ascending = False )[ 0 : 20 ][[ 'Name' , 'Fruity' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[38]: Name Fruity ABV % Price Customers' Rating Expert Score 325 Heaven Hill Select Stock Bourbon 100.0 65.10 4 4.09 84 2195 Barrell Bourbon Batch 007 90.0 61.20 3 3.50 83 448 Buffalo Trace Experimental Collection French O... 90.0 45.00 4 3.35 88 453 W.H. Harrison Governor's Reserve Bourbon 90.0 56.50 3 3.36 84 1056 Parker's Heritage Master Distiller's Blend of ... 90.0 63.50 3 4.18 93 1980 1792 High Rye Bourbon 90.0 47.15 2 3.72 85 1727 Four Roses Limited Edition Small Batch Bourbon... 90.0 53.65 4 4.28 92 1525 Blaum Bros Knotter Bourbon 3 Year (Batch #6) 85.0 45.00 2 3.49 76 1959 Woodford Reserve Master's Collection Brandy Ca... 85.0 45.20 4 4.17 91 895 Evan Williams White Label Bottled in Bond Bourbon 81.0 50.00 1 3.35 80 629 Hancock's President's Reserve Single Barrel Bo... 80.0 44.45 3 3.81 88 751 Daviess County Kentucky Straight Bourbon Frenc... 80.0 48.00 2 3.73 91 916 Four Roses Limited Edition Single Barrel Bourb... 80.0 54.20 4 4.25 98 1002 10th Mountain Bourbon 80.0 46.00 3 3.30 75 1349 Temperance Trader Chinato Barrel-Finished Bourbon 80.0 45.00 2 3.61 87 1051 Woodford Reserve Master's Collection Four Wood... 80.0 47.20 4 4.30 96 1839 Woodford Reserve Master's Collection Sonoma-Cu... 80.0 45.20 4 3.69 84 1832 Dark Corner Distillery Lewis Redmond Bourbon 80.0 43.00 3 2.90 77 1729 Burnside Oregon Oaked Bourbon 80.0 48.00 2 3.43 84 1833 Hirsch Selection 28 Year Bourbon 80.0 43.40 5 3.89 97 Which are the oldest? Intent is to see if there are any errors or something worth noting. In [39]: df2 . sort_values ( by = [ 'Age' ], ascending = False )[ 0 : 20 ][[ 'Name' , 'Age' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[39]: Name Age ABV % Price Customers' Rating Expert Score 1833 Hirsch Selection 28 Year Bourbon 28.0 43.40 5 3.89 97 702 Heaven Hill 27 Year Barrel Proof Kentucky Stra... 27.0 47.35 5 4.26 93 1621 Old Blowhard 26 Year Bourbon 26.0 45.35 5 3.68 81 1193 Michter's 25 Year Single Barrel Bourbon 25.0 54.30 5 4.49 87 1348 Rhetoric 24 Year Bourbon 24.0 45.40 4 4.21 89 1757 Pappy Van Winkle 23 Year 23.0 47.80 5 4.54 89 437 Rhetoric 23 Year Bourbon 23.0 45.30 4 4.21 85 2020 Evan Williams 23 Year Bourbon 23.0 53.50 5 4.37 86 754 Elijah Craig 23 Year Bourbon 23.0 45.00 5 4.19 90 140 Blade And Bow Bourbon 22 Year (2015 Release) 22.0 46.00 5 4.29 87 1192 Rhetoric 22 Year Bourbon 22.0 45.00 4 4.29 87 1589 Lost Prophet 22 Year Bourbon 22.0 45.05 4 4.34 90 1933 Elijah Craig Single Barrel 21 Year 21.0 45.00 5 4.14 87 832 Rhetoric 21 Year Bourbon 21.0 45.10 4 4.05 90 1511 Michter's 20 Year Single Barrel Bourbon 20.0 57.10 5 4.49 94 1317 Pappy Van Winkle 20 Year 20.0 45.20 4 4.67 92 1127 Barterhouse 20 Year Bourbon 20.0 45.10 4 4.06 87 1443 Rhetoric 20 Year Bourbon 20.0 45.00 4 3.92 89 2128 Batch 206 Old Log Cabin Bourbon 19.0 43.00 2 3.15 83 1383 Elijah Craig 18 Year 18.0 45.00 5 4.31 92 Which are the most popular ? Intent is to see if there are any errors or something worth noting. In [40]: df2 . sort_values ( by = [ '# Ratings' ], ascending = False )[ 0 : 20 ][[ 'Name' , '# Ratings' , 'Rare' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[40]: Name # Ratings Rare ABV % Price Customers' Rating Expert Score 276 Blanton's Original Single Barrel 9072 False 46.5 2 4.30 89 826 Buffalo Trace Bourbon 8913 False 45.0 2 3.66 83 917 Eagle Rare 10 Year Bourbon 8656 False 45.0 2 4.02 91 113 Maker's Mark Bourbon 7209 False 45.0 2 3.46 87 85 Woodford Reserve Bourbon 7087 False 45.2 2 3.65 85 1744 Bulleit Bourbon 6712 False 45.0 2 3.48 86 1909 Four Roses Single Barrel Bourbon 5890 False 50.0 2 4.00 90 465 Basil Hayden's Bourbon 5328 False 40.0 2 3.62 80 1133 Weller Special Reserve 4931 False 45.0 2 3.89 91 285 Angel's Envy Bourbon Finished in Port Wine Bar... 4741 False 43.3 2 3.86 84 1592 Woodford Reserve Double Oaked 4590 False 45.2 2 4.11 92 329 Colonel E.H. Taylor, Jr. Small Batch Bottled i... 4444 False 50.0 2 4.14 90 2179 Knob Creek Small Batch Bourbon 4354 False 50.0 2 3.47 84 380 Elijah Craig Small Batch Bourbon 4345 False 47.0 2 3.68 85 479 Four Roses Small Batch Bourbon 4159 False 45.0 2 3.83 92 171 Evan Williams Single Barrel 3983 False 43.3 2 3.87 96 1429 Henry McKenna 10 Year Bottled in Bond Bourbon 3868 False 50.0 2 3.93 91 256 Larceny Small Batch Kentucky Straight Bourbon 3620 False 46.0 1 3.47 88 81 Maker's Mark 46 3551 False 47.0 2 3.74 90 263 Weller Antique 107 3495 False 53.5 2 4.12 92 Ah, interestingly, the single-most popular, Blanton's is actually very hard to find these days. The field wrongly states it's not rare, but it is essentially impossible to find in most US States, and bottles are commonly marked up from $40 MSRP to $200. I'm very surprised that this has the most reviews, but I suspect it's because it has the highest allure amongst the rare ones that are somewhat possible to find. Years ago, it was very easy to find, so maybe some reviews were from this time. Additionally, Weller Special Reserve is also impossible to get within most places in the US, for most times of the year, but it has TONS of allure and attention. People obsess over it. Weller Antique 107 is even rarer. The rest are very common within stores and bars, so the data makes sense for these. Which are the best according to customers ? Intent is to see if there are any errors or something worth noting. In [41]: df2 . sort_values ( by = [ \"Customers \\' Rating\" ], ascending = False )[ 0 : 20 ][[ 'Name' , '# Ratings' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[41]: Name # Ratings ABV % Price Customers' Rating Expert Score 1936 Parker's Heritage Heavy Char Bourbon 10 Year 2 60.00 4 4.88 92 1283 Abraham Bowman Sweet XVI Bourbon 15 58.00 3 4.84 93 871 William Larue Weller Bourbon (Fall 2020) 1 67.25 4 4.75 93 2078 William Larue Weller Bourbon (Fall 2016) 134 67.70 4 4.71 96 1570 William Larue Weller Bourbon (Fall 2017) 182 64.10 4 4.71 90 593 William Larue Weller Bourbon (Fall 2015) 232 67.30 3 4.70 98 1463 King of Kentucky 15 Year Kentucky Straight Bou... 23 65.50 5 4.69 90 296 Old Forester President's Choice Bourbon 3 59.30 5 4.67 90 21 Four Roses Limited Edition 50th Anniversary Sm... 158 54.30 4 4.67 94 1317 Pappy Van Winkle 20 Year 893 45.20 4 4.67 92 1957 George T. Stagg Bourbon (Fall 2017) 387 64.60 4 4.66 94 727 William Larue Weller Bourbon (Fall 2018) 183 62.85 4 4.65 96 2069 Russell's Reserve 1998 19 51.10 5 4.64 92 303 George T. Stagg Bourbon (Fall 2013) 422 64.10 5 4.62 97 97 George T. Stagg Bourbon (Fall 2015) 104 69.10 3 4.62 91 1991 Black Maple Hill 16 Year Bourbon 69 47.50 4 4.62 97 2108 King of Kentucky 14 Year Kentucky Straight Bou... 23 67.50 5 4.61 94 1596 George T. Stagg Bourbon (Fall 2018) 382 62.45 4 4.59 93 1575 George T. Stagg Bourbon (Fall 2019) 395 58.45 4 4.59 98 1932 William Larue Weller Bourbon (Fall 2013) 139 68.10 4 4.57 96 This seems correct to me, not because I've tasted any of these, but because these are famous and are highly coveted. I've never heard of the best rated, Parker's , though. I'd be suspicious that it's an outlier and wrong, especially considering it only has 2 reviews from users; however, the expert also gave it a high score, so it seems like a valid entry. In [42]: df2 . sort_values ( by = [ \"Customers \\' Rating\" ], ascending = True )[ 0 : 20 ][[ 'Name' , '# Ratings' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[42]: Name # Ratings ABV % Price Customers' Rating Expert Score 486 Double Diamond Limited Edition Bourbon 267 1 40.0 2 1.00 80 2167 Syntax Spirits Bourbon 2 47.5 2 2.00 80 658 Maryland Club Straight Bourbon 2 47.5 2 2.00 78 1920 Winchester \"Extra Smooth\" Bourbon 75 45.0 1 2.21 65 1108 Detroit City Two-Faced Bourbon 8 47.0 2 2.25 77 1244 Black Button Little Barrel Bourbon 8 42.0 3 2.38 75 230 Adirondack 601 Bourbon 21 43.2 3 2.40 78 1250 Old Crow Kentucky Straight Bourbon 197 40.0 1 2.40 71 1391 Central Standard Bourbon 9 45.0 2 2.42 80 801 Evan Williams Green Label 177 40.0 1 2.45 81 0 Cleveland Bourbon 116 50.0 2 2.48 70 2027 Feisty Spirits Blue Corn Bourbon 2 44.0 2 2.50 81 2063 Ancient Age 212 40.0 1 2.55 78 611 Graveyard Sam's Baby Bourbon 5 45.0 2 2.55 72 1279 John B. Stetson Kentucky Straight Bourbon Whiskey 52 42.0 2 2.56 68 328 New Liberty Bloody Butcher Bourbon 19 47.5 2 2.57 87 1689 Kentucky Tavern Bourbon 47 40.0 1 2.59 80 1826 Cabin Still Bourbon 20 40.0 1 2.61 79 824 Yellow Rose Double Barrel Bourbon 24 43.0 2 2.66 83 195 Old Hickory Great American Straight Bourbon 13 43.0 2 2.67 80 I've never heard of any of these, so this list seems reasonable. Plus, the experts gave them all horrible reviews, so I don't suspect anything suspicious is going on (e.g., customers ironically rating a controversial, highly-appraised whiskey as being horribly low, as if to troll the ratings). Which are the best according to experts ? Intent is to see if there are any errors or something worth noting. In [43]: df2 . sort_values ( by = [ 'Expert Score' ], ascending = False )[ 0 : 20 ][[ 'Name' , '# Ratings' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[43]: Name # Ratings ABV % Price Customers' Rating Expert Score 12 Booker's Bourbon Batch 2015-01 \"Big Man, Small... 698 64.35 3 4.38 98 1635 Four Roses Limited Edition Single Barrel Bourb... 158 59.80 5 4.23 98 1575 George T. Stagg Bourbon (Fall 2019) 395 58.45 4 4.59 98 593 William Larue Weller Bourbon (Fall 2015) 232 67.30 3 4.70 98 916 Four Roses Limited Edition Single Barrel Bourb... 503 54.20 4 4.25 98 1917 Parker's Heritage Promise of Hope 224 48.00 4 4.45 98 303 George T. Stagg Bourbon (Fall 2013) 422 64.10 5 4.62 97 1671 Wild Turkey Diamond Anniversary Bourbon 278 45.50 4 4.33 97 1991 Black Maple Hill 16 Year Bourbon 69 47.50 4 4.62 97 1833 Hirsch Selection 28 Year Bourbon 11 43.40 5 3.89 97 1932 William Larue Weller Bourbon (Fall 2013) 139 68.10 4 4.57 96 1537 Four Roses Limited Edition Small Batch Bourbon... 207 54.30 5 4.42 96 489 George T. Stagg Bourbon (Fall 2020) 9 65.20 4 4.47 96 1051 Woodford Reserve Master's Collection Four Wood... 113 47.20 4 4.30 96 1004 Four Roses Limited Edition Small Batch Bourbon... 176 55.60 4 4.35 96 231 Eagle Rare 17 Year Bourbon (Fall 2014) 177 45.00 3 4.31 96 1573 Old Forester Birthday Bourbon 2018 225 50.50 4 4.42 96 514 Parker's Heritage Cognac Barrel Finish 10 Year... 11 50.00 3 4.53 96 171 Evan Williams Single Barrel 3983 43.30 2 3.87 96 2078 William Larue Weller Bourbon (Fall 2016) 134 67.70 4 4.71 96 This seems right. Never heard of Hirsch or Parker's but the others are speciality versions of famous/popular whiskeys, so this makes sense. Which are the most expensive ? Intent is to see if there are any errors or something worth noting. In [44]: df2 . sort_values ( by = [ 'Price' ], ascending = False )[ 0 : 15 ][[ 'Name' , '# Ratings' , 'ABV %' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ]] Out[44]: Name # Ratings ABV % Price Customers' Rating Expert Score 592 Remus Volstead Reserve 14 Year Bottled in Bond... 31 50.00 5 4.25 89 1649 Angel's Envy Bourbon Finished in Mizunara Oak 3 48.90 5 3.58 91 702 Heaven Hill 27 Year Barrel Proof Kentucky Stra... 28 47.35 5 4.26 93 140 Blade And Bow Bourbon 22 Year (2015 Release) 56 46.00 5 4.29 87 303 George T. Stagg Bourbon (Fall 2013) 422 64.10 5 4.62 97 1779 Russell's Reserve 2002 30 57.30 5 4.43 95 296 Old Forester President's Choice Bourbon 3 59.30 5 4.67 90 2108 King of Kentucky 14 Year Kentucky Straight Bou... 23 67.50 5 4.61 94 2020 Evan Williams 23 Year Bourbon 35 53.50 5 4.37 86 1511 Michter's 20 Year Single Barrel Bourbon 110 57.10 5 4.49 94 982 Angel's Envy Bourbon Finished in Port Wine Bar... 105 61.20 5 4.09 93 1383 Elijah Craig 18 Year 689 45.00 5 4.31 92 1833 Hirsch Selection 28 Year Bourbon 11 43.40 5 3.89 97 590 Jim Beam Distiller's Masterpiece 96 50.00 5 4.24 86 264 Angel's Envy Bourbon Finished in Port Wine Bar... 4 60.20 5 4.00 94 We don't have high granularity (prices are just 1-5), which is perhaps a blessing in disguise -- most bourbons are \\$30 - \\\\$50, but some rare ones, especially due to price gouging, can be \\$100 - \\\\$3,000. That's a wild range and is largely due to rarity, allure, and sensationalism within human behavior, as opposed to actual qualities of the bourbon. So, maybe it's good that we don't have to deal with outlier whiskeys have extraordinary prices. Where do they come from? Intent is to see if there are any errors or something worth noting. In [45]: df2 [ 'Location' ] . value_counts () Out[45]: Booker's // Kentucky, USA 22 Four Roses // Kentucky, USA 18 Buffalo Trace // Kentucky, USA 16 Old Forester // Kentucky, USA 16 Jim Beam // Kentucky, USA 15 Heaven Hill // Kentucky, USA 14 Elijah Craig // Kentucky, USA 13 Woodford Reserve // Kentucky, USA 12 Wild Turkey // Kentucky, USA 10 Barton 1792 // Kentucky, USA 7 Knob Creek // Kentucky, USA 7 Michter's // Kentucky, USA 7 Redemption // Indiana, USA 6 Barrell Craft Spirits // (bottled in) Kentucky, USA 6 Colonel E.H. Taylor, Jr. // Kentucky, USA 6 A. Smith Bowman // Virginia, USA 6 Angel's Envy // Kentucky, USA 6 Yellowstone // Kentucky, USA 5 Evan Williams // Kentucky, USA 5 Wyoming Whiskey // Wyoming, USA 5 Barrell Craft Spirits // USA 5 Parker's Heritage Collection // Kentucky, USA 5 George Remus // Indiana, USA 5 William Larue Weller // Kentucky, USA 5 Maker's Mark // Kentucky, USA 5 Eagle Rare // Kentucky, USA 5 Belle Meade // Indiana (bottled in Tennessee), USA 5 George T. Stagg // Kentucky, USA 5 Weller // Kentucky, USA 4 Blood Oath // Kentucky, USA 4 Rebel Yell // Kentucky, USA 4 Laws Whiskey House // Colorado, USA 4 Garrison Brothers // Texas, USA 4 Rhetoric // Kentucky, USA 4 Larceny // Kentucky, USA 4 Jefferson's // Kentucky, USA 3 Barrell Craft Spirits // Tennessee, USA 3 Kentucky Bourbon Distillers, Ltd. // Kentucky, USA 3 Bulleit // Kentucky, USA 3 St. Augustine // Florida, USA 3 Orphan Barrel Whisky Co. // Kentucky, USA 3 Buffalo Trace Antique Collection // Kentucky, USA 3 Daviess County // Kentucky, USA 3 Bardstown Bourbon Company // Kentucky, USA 3 Blaum Bros. // Indiana (further aged & bottled in Illinois), USA 3 Pappy Van Winkle // Kentucky, USA 3 Old Grand-Dad // Kentucky, USA 3 New Riff // Kentucky, USA 3 Woodinville // Washington, USA 3 Sam Houston // Kentucky, USA 2 Abraham Bowman // Virginia, USA 2 Baker's // Kentucky, USA 2 Feisty Spirits // Colorado, USA 2 Coopers' Craft // Kentucky, USA 2 Virgil Kaine // (bottled in) South Carolina, USA 2 HIRSCH // Kentucky, USA 2 Black Maple Hill // Kentucky, USA 2 Sonoma County Distilling // California, USA 2 Old Rip Van Winkle // Kentucky, USA 2 Belle Meade // (bottled in) Tennessee, USA 2 Rock Town // Arkansas, USA 2 Old Fitzgerald // Kentucky, USA 2 Black Button Distilling // New York, USA 2 King of Kentucky // Kentucky, USA 2 291 // Colorado, USA 2 Temperance Trader // Indiana (bottled in Oregon), USA 2 Backbone Bourbon // Indiana (bottled in Kentucky), USA 2 Ezra Brooks // Kentucky, USA 2 Burnside // Oregon, USA 2 Smooth Ambler // USA 2 Penelope // Indiana, USA 2 Blanton's // Kentucky, USA 2 Jefferson's // USA 2 HIRSCH // Indiana, USA 2 Hudson Whiskey // New York, USA 2 Rabbit Hole // Kentucky, USA 2 Russell's Reserve // Kentucky, USA 2 Chattanooga Whiskey Co. // Tennessee, USA 2 FEW // Illinois, USA 2 Berkshire Mountain // Massachusettes, USA 2 Bardstown Bourbon Company // Tennessee, USA 2 Ranger Creek // Texas, USA 2 Basil Hayden's // Kentucky, USA 2 Kentucky, USA 2 Balcones // Texas, USA 2 Batch 206 // Washington, USA 2 I.W. Harper // Kentucky, USA 2 Ancient Age // Kentucky, USA 1 Downslope Distilling // Colorado, USA 1 Tatoosh // USA 1 Elk Rider // Washington , USA 1 Early Times // Kentucky, USA 1 Old Ezra // Kentucky, USA 1 OYO // Ohio, USA 1 PM Spirits // Indiana, USA 1 Wiggly Bridge // Maine, USA 1 Pinhook // USA 1 Dark Horse Distillery // Kansas, USA 1 Eight & Sand // Indiana, USA 1 J. Henry & Sons // Wisconsin, USA 1 10th Mountain // Vail, Colorado, USA 1 Coppersea // New York, USA 1 Grand Traverse Distillery // Michigan, USA 1 Redemption // Indiana , USA 1 New Holland // Indiana (bottled in Michigan), USA 1 Belle Meade // Tennessee, USA 1 Milam & Greene // USA 1 New Riff // Indiana (bottled in Kentucky), USA 1 Hillrock Estate // USA 1 Western Spirits Beverage Company // USA 1 Western Spirits Company // Kentucky, USA 1 Breckenridge // USA 1 Van Brunt Stillhouse // New York, USA 1 Ghost Hill // Texas, USA 1 District Made // Washington D.C., USA 1 Winchester // USA 1 The Family Jones // Colorado, USA 1 Prohibition Distillery // New York, USA 1 Elmer T. Lee // Kentucky, USA 1 Stagg Jr. // Kentucky , USA 1 Medley Bros. // Kentucky, USA 1 Dark Corner Distillery // South Carolina, USA 1 Valley Shine Distillery // USA 1 Duke // Kentucky, USA 1 W.H. Harrison // Indiana, USA 1 Metze's // Indiana, USA 1 Noah's Mill // Kentucky, USA 1 SILO // Vermont, USA 1 Illinois, USA 1 Chicken Cock // Indiana (aged & bottled in Kentucky), USA 1 Deadwood // Indiana, USA 1 Central Standard // Wisconsin, USA 1 33 // USA 1 Black Dirt // New York, USA 1 Koval // Illinois, USA 1 Pennsylvania, USA 1 Cyrus Noble // Kentucky, USA 1 Rhetoric // Kentucky (bottled in Tennessee), USA 1 Gristmill Distillers // New York, USA 1 George Remus // Indiana , USA 1 Cascade Alchemy // South Carolina (bottled in Oregon), USA 1 PennyPacker // Kentucky, USA 1 Abraham Bowman // Virginia , USA 1 Booker's // Kentucky , USA 1 Wathen's // Kentucky, USA 1 Corner Creek // Kentucky, USA 1 Blaum Bros. // Illinois, USA 1 Dancing Pines // Colorado, USA 1 Old Soul // USA 1 Penelope // USA 1 Sazerac // USA 1 Two James Spirits // (bottled in) Michigan, USA 1 Orange County Distillery // New York, USA 1 Spirit Works // California, USA 1 Old Crow // USA 1 Kooper Family // Indiana (aged in Texas), USA 1 David Nicholson // Kentucky (bottled in Missouri), USA 1 Pinhook // Kentucky, USA 1 Peach Street // Colorado, USA 1 Short Mountain // Tennessee, USA 1 Heaven's Door // Tennessee, USA 1 MB Roland Distillery // Kentucky, USA 1 Port Chilkoot // Alaska, USA 1 Kentucky Owl // Kentucky, USA 1 Steward's Whiskies // Kentucky, Tennessee and Indiana, USA 1 Stagg Jr. // Kentucky, USA 1 Michter's // USA 1 Yellow Rose // Texas, USA 1 Cedar Ridge // Iowa, USA 1 The Walking Dead // Kentucky, USA 1 Long Island Spirits // New York, USA 1 Johnny Drum // Kentucky, USA 1 Jos. A. Magnus & Co. // Indiana (Finished and Bottled in Washington DC), USA 1 Sonoma County Distilling Co. // California, USA 1 Graveyard Sam's // Pennsylvania, USA 1 Union Horse // Kansas, USA 1 HIRSCH // Indiana (bottled in Ohio), USA 1 Finger Lakes Distilling // New York, USA 1 Six & Twenty // South Carolina, USA 1 Old Bardstown Distilling Company // Kentucky, USA 1 Colorado Gold // Colorado, USA 1 Prichard's // Tennessee, USA 1 Cooperstown Distillery // New York, USA 1 Tom's Town // Missouri, USA 1 Barrell Craft Spirits // Tennessee , USA 1 Old Elk // (bottled in) Colorado, USA 1 A. Smith Bowman // USA 1 Headframe Spirits // Montana, USA 1 Treaty Oak Distilling // USA 1 Rough Rider // Long Island , USA 1 TahWahKaro // Texas, USA 1 Redemption Whiskey // Indiana, USA 1 Fighting Cock // Kentucky, USA 1 Widow Jane // USA 1 Henry McKenna // Kentucky, USA 1 Dry Fly // Washington, USA 1 Rod & Rifle // Tennessee, USA 1 Heaven's Door // USA 1 Jos. A. Magnus & Co. // (blended & bottled in Washington D.C.), USA 1 Fistful of Bourbon // USA 1 New Liberty // Pennsylvania, USA 1 Chicken Cock // Kentucky , USA 1 Bond & Lillard // Kentucky, USA 1 Oola // Washington, USA 1 Indiana (bottled in Pennsylvania), USA 1 Koenig Distillery and Winery // (bottled in) Idaho, USA 1 Journeyman Distillery // Michigan, USA 1 Copper Fiddle // Illinois, USA 1 Three Chord // USA 1 Bulleit // Kentucky , USA 1 Old Bardstown // Kentucky, USA 1 Deadwood // Indiana , USA 1 Filibuster // USA 1 J.W. Overbey & Co. // New York, USA 1 Treaty Oak // USA 1 Indiana (bottled in California), USA 1 The Clover // Indiana , USA 1 Detroit City // USA 1 Old Charter // Kentucky, USA 1 Smooth Ambler // West Virginia, USA 1 Wigle // Pennsylvania, USA 1 Still & Oak // Wisconsin, USA 1 Booker's // Kentucky , USA 1 Yellow Rose // USA 1 Breaking & Entering // Kentucky , USA 1 Legent // Kentucky, USA 1 Temperance Trader // (bottled in) Oregon, USA 1 Deerhammer // Colorado, USA 1 Stetson // Kentucky, USA 1 Ransom Spirits // Oregon, USA 1 SOA Spirits // Indiana, USA 1 James E. Pepper // Indiana (bottled in Kentucky), USA 1 Knob Creek // Kentucky , USA 1 Taconic Distillery // New York, USA 1 Catskill Distilling Co. // New York, USA 1 Delaware Phoenix // New York, USA 1 Bird Dog // Kentucky, USA 1 Big House // Indiana (bottled in Kentucky), USA 1 Hood River Distilling // Kentucky (bottled in Oregon), USA 1 Kinsey // (bottled in) Pennsylvania, USA 1 Amador Whiskey Co. // Kentucky (Finished and Bottled in California), USA 1 Five & 20 Spirits // New York, USA 1 Sweetens Cove // Tennessee , USA 1 Two James // Michigan, USA 1 Cabin Still // Kentucky, USA 1 Old Ripy // Kentucky, USA 1 McAfee's Benchmark // Kentucky, USA 1 Blade And Bow // Kentucky, USA 1 Frey Ranch // Nevada, USA 1 J. Henry & Sons // Wisconsin , USA 1 Charred Oak Spirits // Wisconsin, USA 1 Eagle Rare // Kentucky , USA 1 2bar Spirits // Washington, USA 1 35 Maple Street // USA 1 Orphan Barrel Whisky Co. // Kentucky (bottled in Tennessee), USA 1 Colter's Run // Idaho, USA 1 TX Whiskey // Texas, USA 1 Adirondack Distilling Company // New York, USA 1 Old Tub // Kentucky, USA 1 Barrell Craft Spirits // Kentucky, USA 1 Missouri Spirits // Missouri, USA 1 Very Old Barton // Kentucky, USA 1 Early Times // Kentucky , USA 1 Watershed // Ohio, USA 1 Kiepersol Estates // Texas, USA 1 Defiance // USA 1 Still Austin // Texas, USA 1 Cody Road // Iowa, USA 1 O4D // Indiana (aged in Georgia), USA 1 Old Hickory Great American // Indiana (bottled in Ohio), USA 1 Parker's Heritage Collection // Kentucky , USA 1 Warbringer // (bottled in) California , USA 1 Tom's Town // Tennessee (bottled in Missouri), USA 1 Kinsey // (bottled in) Pennsylvania , USA 1 Diageo // Kentucky, USA 1 The Splinter Group // Tennessee and Kentucky (Finished and Bottled in California), USA 1 Lux Row Distillers // Kentucky, USA 1 Clyde May's // Alabama, USA 1 Cleveland Whiskey // Ohio, USA 1 Taconic Distillery // USA 1 Kings County // New York, USA 1 Syntax Spirits // Colorado, USA 1 Oregon Spirit Distillers // Oregon, USA 1 Jim Beam // Kentucky , USA 1 Name: Location, dtype: int64 Some distilleries produce different brands of whiskey. Most come from Kentucky. You can see that some distilleries produce tons of different types, but this can be a bit misleading because some of those different types are just slight variations (e.g., Eagle Rare 10, Eagle Rare 17), whereas others are completely different brands (e.g., Buffalo Trace, Blanton's). For now, it's probably best to just ignore the location feature, but we'll keep it in mind for modelling, if we get desperate. One idea would be to create 2 fields from this: 1 for the geographic state (e.g., Kentucky), and another for the distillery (e.g., Booker's). Let's look at the distribution of flavor values? Intent is to see if there are any errors or something worth noting. In [46]: fig , axs = plt . subplots ( nrows = 5 , ncols = 3 , figsize = ( 20 , 20 ), facecolor = 'w' , edgecolor = 'k' ) fig . subplots_adjust ( hspace = . 5 , wspace =. 2 ) axs = axs . ravel () fontsize = 10 flavors = [ 'Smoky' , 'Peaty' , 'Spicy' , 'Herbal' , 'Oily' , 'Full-bodied' , 'Rich' , \\ 'Sweet' , 'Briny' , 'Salty' , 'Vanilla' , 'Tart' , 'Fruity' , 'Floral' ] # plot histograms for i , flavor in enumerate ( flavors ): axs [ i ] . hist ( df2 [ flavor ], alpha = 0.7 , color = 'lightblue' , bins = 'auto' , density = False , histtype = 'bar' , edgecolor = 'k' ) axs [ i ] . set_title ( \"Distribution of \" + flavor + \" Flavor\" , fontsize = fontsize ) axs [ i ] . set_xlabel ( flavor + \" Flavor\" , fontsize = fontsize ) axs [ i ] . set_ylabel ( 'Count' , fontsize = fontsize ) # removes the empty one, since we only have 14 flavors, not 15 axs [ 14 ] . set_axis_off () These all seem pretty reasonable, and I'm glad that the values have a good spread. A few flavors are a bit skewed, and these are the ones that we inspected above. Let's look for any patterns/correlations that may exists between our features. Since some of the above flavors are skewed (e.g., Salty is usually 0), we would not be able to discern any meaningful trend, so we can throw this out from our visualization. Otherwise, our graph woud just be a bunch of points overlapping one another at the 0 value. In [47]: grid_features = [ 'Smoky' , 'Spicy' , 'Herbal' , 'Oily' , 'Full-bodied' , 'Rich' , \\ 'Sweet' , 'Vanilla' , 'Fruity' , 'Floral' , \\ 'Age' , 'Price' , 'Customers \\' Rating' , 'Expert Score' ] scatter = pd . plotting . scatter_matrix ( df2 [ grid_features ], alpha = 0.4 , figsize = ( 20 , 20 )); for ax in scatter . ravel (): ax . set_xlabel ( ax . get_xlabel (), rotation = 90 ) ax . set_ylabel ( ax . get_ylabel (), rotation = 90 ) We see that: Customer's Rating is highly correlated with the expert's rating The higher the Price , the more likely it is to have a high score from both customers and experts The higher the Richness , the more Full-bodied and Sweet it tends to be (strong correlations) The higher the Oiliness , the more likely it is to be Full-bodied No individual flavor seems correlated with the scores from customers or experts. The closest trend is from Full-bodied and Rich , as they seem slightly directly correlated with the scores. This is an indication that predicting the score is not trivially easy; the Full-bodied and Richness can play some role, but if flavors give any indication, it'll be due to a combination of flavors instead of any one particular flavor.","tags":"labs","url":"labs/lecture-27/notebook/"},{"title":"Lecture 27: Case Study 2","text":"CS109A Introduction to Data Science Case Study: Hunting for Flavors PART 5: Are there certain attributes of bourbons that are predictive of good bourbons? Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner In [1]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2020-CS109A/master/themes/static/css/cs109.css\" ) . text HTML ( styles ) Out[1]: In [2]: # import the necessary libraries import re import requests import random import pandas as pd import math import matplotlib import matplotlib.pyplot as plt from matplotlib.ticker import PercentFormatter from sklearn.neighbors import KNeighborsRegressor from matplotlib import gridspec from sklearn.model_selection import KFold import numpy as np import seaborn as sns from time import sleep from bs4 import BeautifulSoup from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn import preprocessing # global properties data_dir = \"data/\" # where to save data num_search_pages = 50 # how many search pages to cull through # NOTE: # if you haven't yet downloaded the data, this should be set to True download_data = False Disclaimer Alcohol is drug. There are state and federal laws that govern the sale, distribution, and consumption of such. In the United States, those who consume alcohol must be at least 21 years of age. In no way am I, or anyone else at IACS or Harvard at large, promoting or encouraging the usage of alcohol. My intention is not to celebrate it. Anyone who chooses to consume alcohol should be of legal age and should do so responsibly. Abusing alcohol has serious, grave effects. The point of this exercise is purely pedagogical, and it illustrates the wide range of tasks to which one can apply data science and machine learning. That is, I am focusing on a particular interest and demonstrating how it can be used to answer questions that one may be interested in for one's own personal life. You could easily imagine this being used in professional settings, too. Learning Objectives Help see the big picture process of conducting a project, and to illustrate some of the nuanced details and common pitfalls. Predicting good bourbons The point of this notebook is to address the first question that we posed in the beginning: Are there certain attributes of bourbons that are predictive of good (i.e., highly rated by users) bourbons? Find hidden gems (i.e., should be good but current reviews are absent or unsupportive of such) Find over-hyped whiskeys (i.e., the reviews seem high but the attributes aren't indicative) Are there significant results if we target experts' ratings instead of average customer ratings? To this effect, let's first load and clean the data, identically to what we did in the previous notebook. NOTE: The only reason this notebook exists as a separate notebook, instead of one long notebook, is for readability and organization. Load and clean the data Fetching a list of webpages via Requests In [3]: whiskey_urls = set () if download_data : # we define this for convenience, as every state's url begins with this prefix base_url = 'https://distiller.com/search?term=bourbon' # visits each search result page for page_num in range ( 1 , num_search_pages ): cur_page = requests . get ( 'https://distiller.com/search?page=' + str ( page_num ) + '&term=bourbon' ) # uses BeautifulSoup to extract all links to whiskeys bs_page = BeautifulSoup ( cur_page . content , \"html.parser\" ) for link in bs_page . findAll ( 'a' , attrs = { 'href' : re . compile ( \"&#94;/spirits/\" )}): whiskey_urls . add ( link . get ( 'href' )) sleep ( 1 ) # saves each URL to disk, so that we don't have to crawl the search results again f = open ( \"whiskey_urls.txt\" , \"w\" ) for url in whiskey_urls : f . write ( url + \" \\n \" ) f . close () # fetches each page and saves it to the hard drive for url in whiskey_urls : cur_page = requests . get ( 'https://distiller.com' + url ) . content # writes file f = open ( data_dir + url [ 9 :], 'wb' ) f . write ( cur_page ) f . close () # sleeps between 1-3 seconds, in case the site tries to detect crawling sleep ( random . randint ( 1 , 3 )) else : # if the files have already been saved to disk # then you can just load them here, instead of crawling again with open ( 'whiskey_urls.txt' ) as f : whiskey_urls = set ( line . strip () for line in f ) In [4]: whiskeys = {} # loads whiskey webpage for i , url in enumerate ( whiskey_urls ): filename = data_dir + url [ 9 :] file_contents = open ( filename , 'r' ) . read () # instantiates a new BeautifulSoup object soup = BeautifulSoup ( file_contents , \"html.parser\" ) # extracts details about the whiskey name = soup . find ( 'h1' , attrs = { 'class' : re . compile ( \"secondary-headline name\" )}) . text . strip () location = soup . find ( 'h2' , attrs = { 'class' : \"ultra-mini-headline location middleweight\" }) . text . strip () soup . find ( 'div' , attrs = { 'class' : \"spirit-badge\" }) badge = \"\" if soup . find ( 'div' , attrs = { 'class' : \"spirit-badge\" }) != None : badge = soup . find ( 'div' , attrs = { 'class' : \"spirit-badge\" }) . text . strip () num_ratings = 0 rating = \"N/A\" if soup . find ( 'span' , attrs = { 'itemprop' : \"ratingCount\" }) != None : num_ratings = int ( soup . find ( 'span' , attrs = { 'itemprop' : \"ratingCount\" }) . text . strip ()) rating = float ( soup . find ( 'span' , attrs = { 'itemprop' : \"ratingValue\" }) . text . strip ()) age = soup . find ( 'li' , attrs = { 'class' : \"detail age\" }) . find ( 'div' , attrs = 'value' ) . text . strip () price = int ( re . findall ( \"cost-(\\d)\" , str ( soup . find ( 'div' , attrs = { 'class' : re . compile ( \"spirit-cost\" )})))[ 0 ]) abv = \"\" if soup . find ( 'li' , attrs = { 'class' : \"detail abv\" }) . find ( 'div' , attrs = 'value' ) . text != \"\" : abv = float ( soup . find ( 'li' , attrs = { 'class' : \"detail abv\" }) . find ( 'div' , attrs = 'value' ) . text ) whiskey_type = soup . find ( 'li' , attrs = { 'class' : \"detail whiskey-style\" }) . div . text cask_type = \"\" if soup . find ( 'li' , attrs = { 'class' : \"detail cask-type\" }) != None : cask_type = soup . find ( 'li' , attrs = { 'class' : \"detail cask-type\" }) . find ( 'div' , attrs = 'value' ) . text . strip () review = \"\" expert = \"\" score = \"\" flavor_summary = \"\" flavor_profile = [] # check if an expert reviewed it if soup . find ( 'p' , attrs = { 'itemprop' : \"reviewBody\" }) != None : review = soup . find ( 'p' , attrs = { 'itemprop' : \"reviewBody\" }) . text . replace ( \" \\\" \" , \"\" ) . strip () expert = soup . find ( 'div' , attrs = { 'class' : 'meet-experts' }) . a . text . strip () score = int ( soup . find ( 'div' , attrs = { 'class' : \"distiller-score\" }) . span . text . strip ()) flavor_summary = soup . find ( 'h3' , attrs = { 'class' : \"secondary-headline flavors middleweight\" }) . text . strip () # extracts flavor profile flavor_profile = eval ( soup . find ( 'canvas' ) . attrs [ 'data-flavors' ]) cur_whiskey = [ name , whiskey_type , cask_type , location , age , abv , price , badge , num_ratings , \\ rating , flavor_summary , expert , score ] if flavor_profile : cur_whiskey . extend ( list ( flavor_profile . values ())) else : cur_whiskey . extend ( np . zeros ( 14 )) cur_whiskey . append ( review ) whiskeys [ i ] = cur_whiskey df = pd . DataFrame . from_dict ( whiskeys , orient = 'index' , \\ columns = [ 'Name' , 'Type' , 'Cask' , 'Location' , 'Age' , 'ABV %' , 'Price' , 'Badge' , \\ '# Ratings' , \"Customers' Rating\" , 'Flavor Summary' , 'Expert' , 'Expert Score' , \\ 'Smoky' , 'Peaty' , 'Spicy' , 'Herbal' , 'Oily' , 'Full-bodied' , 'Rich' , \\ 'Sweet' , 'Briny' , 'Salty' , 'Vanilla' , 'Tart' , 'Fruity' , 'Floral' , 'Review' ]) Clean the data In [5]: pd . set_option ( 'display.max_columns' , None ) pd . set_option ( 'display.max_rows' , None ) # filter by only those that are bourbons and reviewed by an expert df2 = df . loc [( df [ 'Expert' ] != \"\" )] df2 = df2 . loc [( df [ 'Type' ] == \"Bourbon\" )] # remove the single bourbon that was not rated by customers df2 = df2 . loc [ df2 [ 'Customers \\' Rating' ] != \"N/A\" ] df2 = df2 . astype ({ 'Customers \\' Rating' : 'float64' }) # let's fix the 'Age' feature df2 [ 'Age' ] = df2 [ 'Age' ] . replace ([ 'NAS' , 'nas' , 'N/A' , '' ], '0' ) df2 [ 'Age' ] . replace ( to_replace = ' [yY]ear[sS]*' , value = '' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '6.*' , value = '6' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '(\\d+) [Yy].*' , value = ' \\\\ 1' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '4 [Mm]onths' , value = '4' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '9 [Mm]onths' , value = '9' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '18 - 20 [Mm]onths' , value = '1.5' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '32 [Mm]onths' , value = '2.67' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '9 [Mm]onths' , value = '9' , regex = True ) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( to_replace = '9 to 11' , value = '0.75' , regex = True ) df2 = df2 . astype ({ 'Age' : 'float64' }) df2 [ 'Age' ] = df2 [ 'Age' ] . replace ( 0 , 7 ) # fix the 'Badge' feature by making a new 'Rare' feature df2 [ 'Rare' ] = [ True if x == 'RARE' else False for x in df2 [ 'Badge' ]] #df['Badge'] #.map({\"RARE\": True}) del df2 [ 'Badge' ] # convert to appropriate data types df2 = df2 . astype ({ 'Expert Score' : 'int32' , 'Customers \\' Rating' : 'float64' , 'ABV %' : 'float64' }) In [6]: df2 . describe () Out[6]: Age ABV % Price # Ratings Customers' Rating Expert Score Smoky Peaty Spicy Herbal Oily Full-bodied Rich Sweet Briny Salty Vanilla Tart Fruity Floral count 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 585.000000 mean 8.311316 49.899838 2.803419 416.352137 3.756325 86.447863 21.485470 0.230769 53.726496 25.714530 30.695726 58.447863 57.798291 58.381197 4.018803 5.476923 50.340171 21.548718 37.066667 17.835897 std 3.607727 7.170325 1.055314 1097.906235 0.521737 5.737503 18.322834 2.187232 19.560069 19.547419 22.712536 18.317344 18.361300 16.824334 8.589855 9.603433 20.526266 17.517548 21.334758 20.336899 min 0.750000 40.000000 1.000000 1.000000 1.000000 65.000000 0.000000 0.000000 0.000000 0.000000 0.000000 5.000000 0.000000 10.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 25% 7.000000 45.000000 2.000000 22.000000 3.460000 83.000000 10.000000 0.000000 40.000000 10.000000 10.000000 45.000000 45.000000 50.000000 0.000000 0.000000 35.000000 10.000000 20.000000 0.000000 50% 7.000000 47.500000 3.000000 83.000000 3.780000 87.000000 20.000000 0.000000 55.000000 25.000000 30.000000 60.000000 60.000000 60.000000 0.000000 0.000000 50.000000 20.000000 35.000000 10.000000 75% 7.000000 53.500000 4.000000 238.000000 4.170000 91.000000 30.000000 0.000000 70.000000 40.000000 45.000000 70.000000 70.000000 70.000000 5.000000 10.000000 70.000000 30.000000 50.000000 30.000000 max 28.000000 72.050000 5.000000 9072.000000 4.880000 98.000000 90.000000 40.000000 100.000000 90.000000 100.000000 100.000000 100.000000 99.000000 80.000000 80.000000 100.000000 75.000000 100.000000 95.000000 Solving the Problem: Predicting good bourbons Goal: Predict Customers' Rating using flavors and other features Data: All of our 585 bourbons, but split into train/dev/test Features: From our EDA, it appears safe to use most of our features. There's a chance that some of the features are collinear, and if the results demonstrate such, then we can always remove them. For now, we will focus on using the following features: all 14 flavors Age ABV % Price Rare Expert score NOTE: we will also keep track of the Names, just for manually inspecting our errors, but this will not be used as a feature to any model we use. Accuracy Metric: Mean Squared Error 1. Prepare the data for all experiments We have 585 bourbons. I think it's reasonable to: train on 400 develop on 100 test on 85 In [7]: features = [ 'Name' , 'Age' , 'ABV %' , 'Price' , 'Rare' , 'Expert Score' , \\ 'Smoky' , 'Peaty' , 'Spicy' , 'Herbal' , 'Oily' , 'Full-bodied' , 'Rich' , \\ 'Sweet' , 'Briny' , 'Salty' , 'Vanilla' , 'Tart' , 'Fruity' , 'Floral' ] In [32]: # splitting the data into \"train\" and test sets x_ , xtest , y_ , ytest = train_test_split ( df2 [ features ], df2 [ \"Customers \\' Rating\" ], test_size = 85 , random_state = 538 ) # this further divides our 'train' into proper 'train' and 'dev' splits xtrain , xdev , ytrain , ydev = train_test_split ( x_ , y_ , test_size = 100 , random_state = 22 ) print ( len ( xtrain ), len ( xdev ), len ( xtest )) 400 100 85 NOTE: We will never look at or access the xtest or ytest data until our final prediction 2. Linear Regression (Baseline model) Simple Multi-linear Regression Model choices: Scale data: no Polynomial features: no Before we do anything fancy, let's start simple by throwing all of our features at it and dissecting the results from one linear model. In [9]: # fit a linear regression lreg = LinearRegression () lreg . fit ( xtrain . drop ([ 'Name' ], axis = 1 ), ytrain ) Out[9]: LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False) Let's evaluate on the train and dev sets. In [10]: # evaluate on the training set y_train_pred = lreg . predict ( xtrain . drop ([ 'Name' ], axis = 1 )) train_MSE = mean_squared_error ( ytrain , y_train_pred ) round ( train_MSE , 5 ) print ( \"train_MSE:\" , train_MSE ) # evaluate on the dev set y_dev_pred = lreg . predict ( xdev . drop ([ 'Name' ], axis = 1 )) dev_MSE = mean_squared_error ( ydev , y_dev_pred ) round ( dev_MSE , 5 ) print ( \"dev_MSE:\" , dev_MSE ) train_MSE: 0.09467981675773907 dev_MSE: 0.08281208809296803 Is this value of 0.083 for dev_MSE good? I have no idea, but seems really low (we will soon discuss the fact that it's lower than the train_MSE )! Since we have 19 features, we cannot easily plot the regression line to inspect how well it fits the target labels. Instead, let's look at the residuals to see: if they are normally distributed if there are any trends with respect to the predict y-value In [11]: # sort the values of x before line plot residuals = ydev - y_dev_pred # plot the histograms of residuals fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 4 )) gs = gridspec . GridSpec ( 1 , 2 , width_ratios = [ 3 , 5 ]) ax [ 0 ] = plt . subplot ( gs [ 0 ]) ax [ 0 ] . set_xlabel ( 'Residuals' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) ax [ 0 ] . hist ( residuals , alpha = 0.5 , bins = 8 ) # plot the predictions vs residuals ax [ 1 ] = plt . subplot ( gs [ 1 ]) ax [ 1 ] . plot ( y_dev_pred , y_dev_pred - ydev , 'o' , color = '#2ea2db' , alpha = 0.7 , label = 'Residual' ) ax [ 1 ] . plot ([ y_dev_pred . min (), y_dev_pred . max ()],[ 0 , 0 ], 'k--' , label = 'Line $y=0$' ) ax [ 1 ] . set_xlabel ( 'Predicted income values' , fontsize = 10 ) ax [ 1 ] . set_ylabel ( 'Residuals' , fontsize = 10 ) ax [ 1 ] . legend ( loc = 'upper right' , fontsize = 10 ) fig . suptitle ( 'Analysis of Residuals (Dev set)' , fontsize = 14 ); Oh, that's good. That's really good! If it didn't look like it were normally distributed (left graph), then it would indicate that our features aren't linearly independent, and that we'd need to carefully remove some. Notwithstanding, it still might be the case that some features aren't too helpful. On the right, we see that there seems very little pattern as to how far off our predictions are. That is, when we make extreme predictions (very low or very high scores), our predictions tend to be inaccurate. When we make average score predictions, our residuals are kind of uniformly spread. Let's look at the coefficients , to gain insights: In [12]: coefs = pd . DataFrame ( lreg . coef_ , columns = [ 'Coefficients' ], index = features [ 1 :]) coefs Out[12]: Coefficients Age 0.011533 ABV % 0.020882 Price 0.126616 Rare -0.060881 Expert Score 0.038997 Smoky 0.002060 Peaty -0.003457 Spicy -0.001789 Herbal -0.001086 Oily -0.000878 Full-bodied 0.002990 Rich -0.000871 Sweet -0.000399 Briny 0.001004 Salty 0.000766 Vanilla -0.000305 Tart 0.003016 Fruity -0.000137 Floral 0.000110 It can be difficult to interpret the meaning of these coefficients because: some features have different scales (e.g., the 14 flavors each is from 1-100 but price is from 1-5) one feature is categorical (Rare). It makes sense that the flavors all have low values, and that some are slightly negative. For example, most bourbons have low flavor values for Herbal . So, if a bourbon has an Herbal value of 50, this feature alone will contribute a 0.0543 decrease in the overall prediction of the customer rating. Keep in mind, this rating should be between 0 and 5. Speaking of which, linear regression is entirely capable of making predictions beyond 5, which is an unfortunate artifact. If we print y_dev_pred.max() and y_dev_pred.min() , we see that all predictions were between 2.72 and 4.88, which is perfectly reasonable! Regarding the other coefficients, they all seem justifiable to me and nothing has caused concern for me yet. For example, the Expert Score is out of 100 and has one of the largest coefficients. As an example, if an Expert Score is 50, it alone will contribute 1.94. But, if it has a score of 100, it'll contribute twice as much (3.8897). Clearly, such a high-rated bourbon ought to receive higher than a 3.7098 from customers, and this is what we rely on the other features to contribute. So, a higher ABV % , Price , and Age all contribute toward a higher score. The Rare feature is possibly just noise. It has a low value (-0.06). So, if a bourbon is denoted as being rare, it only contribute 0.02. Possible explanations are the training/dev data and that the tag itself seems a bit unreliable. Within the EDA, I detailed that several very hard-to-find whiskeys didn't have the rarity tag. Moreover, if a whiskey truly is rare, it can be rare because it's highly desired and in limited supply, or because it's from a very small distillery and produces very basic, low-quality whiskey. QUESTION: how do we know if we're suffering from not having enough data? (Baseline model) Simple Multi-linear Regression + Boostrapping Using just 1 run, as we did above, is short-sighted, and we can't place too much stock into the performance or features' coefficients. Let's perform bootstrapping with 100 samples (using 100% of the training data each time), while keeping track of (a) the coefficients so that we can see a distribution of them; and (2) the MSE performance on the dev set. In [13]: num_bootstraps = 100 bootstrap_coefs = [] train_MSEs = [] dev_MSEs = [] # bootstrapping will shuffle the rows, so we need to ensure # that the y's maintain the same order as the x's merged = pd . concat ([ xtrain , ytrain ], axis = 1 , sort = False ) for i in range ( num_bootstraps ): boot_df = merged . sample ( frac = 1 , replace = True ) xtrain_boot = boot_df . drop ([ \"Name\" , \"Customers \\' Rating\" ], axis = 1 ) ytrain_boot = boot_df [ \"Customers \\' Rating\" ] boot_lr = LinearRegression () boot_lr . fit ( xtrain_boot , ytrain_boot ) # evaluate on the train set y_train_pred_boot = boot_lr . predict ( xtrain_boot ) train_MSE_boot = mean_squared_error ( ytrain_boot , y_train_pred_boot ) round ( train_MSE_boot , 5 ) train_MSEs . append ( train_MSE_boot ) # evaluate on the dev set y_dev_pred_boot = boot_lr . predict ( xdev . drop ([ 'Name' ], axis = 1 )) dev_MSE_boot = mean_squared_error ( ydev , y_dev_pred_boot ) round ( dev_MSE_boot , 5 ) dev_MSEs . append ( dev_MSE_boot ) bootstrap_coefs . append ( boot_lr . coef_ ) bootstrap_coefs = np . array ( bootstrap_coefs ) In [14]: ## PLOTS THE HISTOGRAMS OF BETA COEFFICIENTS fig , ax = plt . subplots ( nrows = 7 , ncols = 3 , figsize = ( 20 , 20 ), facecolor = 'w' , edgecolor = 'k' ) fig . subplots_adjust ( hspace = . 5 , wspace =. 2 ) ax = ax . ravel () fontsize = 10 for i in range ( 1 , len ( features )): betavals = bootstrap_coefs [:, i - 1 ] betavals . sort () x1 = np . percentile ( betavals , 2.5 ) x2 = np . percentile ( betavals , 97.5 ) x = np . linspace ( x1 , x2 , 500 ) counts , bins = np . histogram ( betavals ) y = counts . max () ax [ i - 1 ] . hist ( bootstrap_coefs [:, i - 1 ], bins = 10 , color = \"#FF7E79\" , alpha = 0.3 , edgecolor = 'black' , linewidth = 1 ) ax [ i - 1 ] . fill_between ( x , y , color = '#007D66' , alpha = 0.2 ) # prettify ax [ i - 1 ] . set_ylabel ( f 'Count' , fontsize = 10 ) ax [ i - 1 ] . set_xlabel ( r '$\\beta$ for ' + features [ i ], fontsize = 10 ) ax [ 19 ] . set_axis_off () ax [ 20 ] . set_axis_off () plt . xticks ( fontsize = 20 ) fig . suptitle ( f '95 % confidence interval for feature coefficients' , fontsize = 16 ) fig . subplots_adjust ( top = 0.95 ) sns . despine () Hey, this looks pretty good! Each beta coefficient looks pretty normally distributed, and the range of values isn't too large. Part of this narrow range can be explained by the small data size. To see the variation of the actual predictions, let's plot the MSE that came from each dev split. In [15]: plt . figure ( figsize = ( 6 , 4 )) plt . hist ( train_MSEs , bins = 10 , alpha = 0.5 , label = \"Train split\" ) plt . hist ( dev_MSEs , bins = 10 , alpha = 0.5 , label = \"Dev split\" ) plt . xlabel ( \"MSE Values\" , size = 10 ) plt . ylabel ( \"Count\" , size = 10 ) plt . title ( \"Performance differences\" ) plt . legend ( loc = 'upper right' ) Out[15]: Ah, shucks, that's completely unexpected. The training accuracy should be better (lower) than the dev accuracy. QUESTION: what could be causes? ANSWER: In general, this could mean at least one of several things likely occurred: our training and dev splits aren't too similar to one another our training and/or dev splits are too small there isn't enough of a signal to properly learn (e.g., bad features, or too many features) we didn't train our model long enough (not applicable for simple linear regression) Our data is really small (400 training samples). So, let's try cross-validation in addition to bootstrapping! Simple Multi-linear Regression + Bootstrapping + Cross-validation Using just 1 split, as we did above, is short-sighted, and we can't place too much stock into the coefficients and results. So, let's use 10-fold cross-validation for each of 100 bootstrapped samples! In [16]: num_bootstraps = 100 bootstrap_coefs = [] # i want things to be random, but consistent for everyone who runs this. # we need to generate 100 bootstraps, each of which should # be a random sample of the data. if a single bootstrap # sample has a `random_state` defined, then the sample will be # the same every time. well, i want 100 of these unique ones, # so i generate a list of 100 random seeds. this list of seeds # needs to be fixed, which is what i do in the following 2 lines: random . seed ( a = 21 ) bootstrap_seeds = random . sample ( range ( 100000 ), num_bootstraps ) # now we include all of the original 'train' # which encompasses the original 'dev' but excludes the 'test' merged = pd . concat ([ x_ , y_ ], axis = 1 , sort = False ) train_MSEs = [] dev_MSEs = [] for i in range ( num_bootstraps ): boot_df = merged . sample ( frac = 1 , replace = True , random_state = bootstrap_seeds [ i ]) kf = KFold ( n_splits = 10 , random_state = 270 ) #kf.get_n_splits(boot_df.drop([\"Customers\\' Rating\"], axis=1)) for train_index , dev_index in kf . split ( boot_df ): xtrain_cv = boot_df . iloc [ train_index ] . drop ([ \"Name\" , \"Customers \\' Rating\" ], axis = 1 ) ytrain_cv = boot_df . iloc [ train_index ][ \"Customers \\' Rating\" ] xdev_cv = boot_df . iloc [ dev_index ] . drop ([ \"Name\" , \"Customers \\' Rating\" ], axis = 1 ) ydev_cv = boot_df . iloc [ dev_index ][ \"Customers \\' Rating\" ] cv_lr = LinearRegression () cv_lr . fit ( xtrain_cv , ytrain_cv ) # evaluate on the train set y_train_pred_cv = cv_lr . predict ( xtrain_cv ) train_MSE_cv = mean_squared_error ( ytrain_cv , y_train_pred_cv ) round ( train_MSE_cv , 5 ) train_MSEs . append ( train_MSE_cv ) # evaluate on the dev set y_dev_pred_cv = cv_lr . predict ( xdev_cv ) dev_MSE_cv = mean_squared_error ( ydev_cv , y_dev_pred_cv ) round ( dev_MSE_cv , 5 ) dev_MSEs . append ( dev_MSE_cv ) bootstrap_coefs . append ( cv_lr . coef_ ) print ( \"train_MSEs:\" , np . mean ( train_MSEs )) print ( \"dev_MSEs:\" , np . mean ( dev_MSEs )) bootstrap_coefs = np . array ( bootstrap_coefs ) train_MSEs: 0.08578783379420153 dev_MSEs: 0.09414415195967932 We now see the property that we'd expect: the dev accuracy is worse than the training accuracy, although only by a little. We could probably benefit from having: more data observations (i.e., whiskeys) possibly different features possibly better modelling We can't make more data observations (whiskeys), and we've done a lot to sample our data so as to make our results more robust and meaningful. So, let's focus on trying different features , and then we can move on to other modelling choices! In [17]: ## PLOTS THE HISTOGRAMS OF BETA COEFFICIENTS fig , ax = plt . subplots ( nrows = 7 , ncols = 3 , figsize = ( 20 , 20 ), facecolor = 'w' , edgecolor = 'k' ) fig . subplots_adjust ( hspace = . 5 , wspace =. 2 ) ax = ax . ravel () fontsize = 10 for i in range ( 1 , len ( features )): betavals = bootstrap_coefs [:, i - 1 ] betavals . sort () x1 = np . percentile ( betavals , 2.5 ) x2 = np . percentile ( betavals , 97.5 ) x = np . linspace ( x1 , x2 , 500 ) counts , bins = np . histogram ( betavals ) y = counts . max () ax [ i - 1 ] . hist ( bootstrap_coefs [:, i - 1 ], bins = 10 , color = \"#FF7E79\" , alpha = 0.3 , edgecolor = 'black' , linewidth = 1 ) ax [ i - 1 ] . fill_between ( x , y , color = '#007D66' , alpha = 0.2 ) # prettify ax [ i - 1 ] . set_ylabel ( f 'Count' , fontsize = 10 ) ax [ i - 1 ] . set_xlabel ( r '$\\beta$ for ' + features [ i ], fontsize = 10 ) ax [ 19 ] . set_axis_off () ax [ 20 ] . set_axis_off () plt . xticks ( fontsize = 20 ) fig . suptitle ( f '95 % confidence interval for feature coefficients' , fontsize = 16 ) fig . subplots_adjust ( top = 0.95 ) sns . despine () In [18]: plt . figure ( figsize = ( 6 , 4 )) plt . hist ( train_MSEs , bins = 10 , alpha = 0.5 , label = \"Train split\" ) plt . hist ( dev_MSEs , bins = 10 , alpha = 0.5 , label = \"Dev split\" ) plt . xlabel ( \"MSE Values\" , size = 10 ) plt . ylabel ( \"Count\" , size = 10 ) plt . title ( \"Performance differences\" ) plt . legend ( loc = 'upper right' ) Out[18]: We see that the coefficients look even more normally distributed, and the MSE values for the training are more reasonable now -- compared to the dev set. The dev set performance has quite a span, which makes sense because we tried many splits (1,000 tests), and the data is small. Feature Selection with Simple Multi-linear Regression + Bootstrapping + Cross-validation Above, we threw the entire kitchen sink at the problem (aka, all features). Let's do forward passes to iteratively determine, in a greedy fashion, which combination of features yields the best results, on average. Keep in mind that the data is small, so there will be some noise. Specifically, let's start with just 1 feature (which performs best by itself), then keep adding 1 more feature, while keeping track of the performances. In [19]: num_bootstraps = 50 bootstrap_coefs = [] random . seed ( a = 21 ) bootstrap_seeds = random . sample ( range ( 100000 ), num_bootstraps ) # now we include all of the original 'train' # which encompasses the original 'dev' but excludes the 'test' merged = pd . concat ([ x_ , y_ ], axis = 1 , sort = False ) selected_features = [] candidate_features = features [ 1 :] mse_across_features = [] while len ( candidate_features ) > 0 : best_mse = 9999 best_feature = \"\" for i , feature in enumerate ( candidate_features ): dev_MSEs = [] for i in range ( num_bootstraps ): boot_df = merged . sample ( frac = 1 , replace = True , random_state = bootstrap_seeds [ i ]) kf = KFold ( n_splits = 10 , random_state = 270 ) #kf.get_n_splits(boot_df.drop([\"Customers\\' Rating\"], axis=1)) for train_index , dev_index in kf . split ( boot_df ): xtrain_cv = boot_df . iloc [ train_index ][ selected_features + [ feature ]] ytrain_cv = boot_df . iloc [ train_index ][ \"Customers \\' Rating\" ] xdev_cv = boot_df . iloc [ dev_index ][ selected_features + [ feature ]] ydev_cv = boot_df . iloc [ dev_index ][ \"Customers \\' Rating\" ] cv_lr = LinearRegression () cv_lr . fit ( xtrain_cv , ytrain_cv ) # evaluate on the dev set y_dev_pred_cv = cv_lr . predict ( xdev_cv ) dev_MSE_cv = mean_squared_error ( ydev_cv , y_dev_pred_cv ) round ( dev_MSE_cv , 5 ) dev_MSEs . append ( dev_MSE_cv ) cur_mse = np . mean ( dev_MSEs ) if cur_mse < best_mse : best_mse = cur_mse best_feature = feature mse_across_features . append ( best_mse ) print ( \"best feature:\" , best_feature ) selected_features . append ( best_feature ) candidate_features . remove ( best_feature ) best feature: Expert Score best feature: Price best feature: ABV % best feature: Age best feature: Tart best feature: Herbal best feature: Smoky best feature: Full-bodied best feature: Spicy best feature: Fruity best feature: Vanilla best feature: Sweet best feature: Rich best feature: Floral best feature: Oily best feature: Salty best feature: Briny best feature: Peaty best feature: Rare Let's plot the performance as we increased the # of features. In [20]: ind = [ _ + 1 for _ in range ( len ( features ) - 1 )] fig , ax = plt . subplots ( figsize = ( 6 , 5 )) ax . bar ( ind , mse_across_features , color = '#007D66' , alpha = 0.3 , edgecolor = 'k' ) ax . set_xticks ( sorted ( ind )); ax . set_xlabel ( r '# features used' , fontsize = 10 ) ax . set_ylabel ( 'Average MSE' , fontsize = 10 ) ax . set_title ( f 'Feature Selection Performance (Dev set)' , fontsize = 14 ) sns . despine () plt . tight_layout () In [21]: best_num_features = np . argmin ( mse_across_features ) print ( \"best_num_features:\" , best_num_features ) print ( \"features:\" , selected_features [ 0 : best_num_features ]) print ( \"mse_across_features:\" , np . min ( mse_across_features )) best_num_features: 6 features: ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal'] mse_across_features: 0.08898992695054489 The results plateau, and adding more features doesn't offer any significant change. In [22]: for i in range ( len ( selected_features )): print ( mse_across_features [ i ], selected_features [ 0 : i ]) 0.12909522321420297 [] 0.1044492865665544 ['Expert Score'] 0.09129133372424443 ['Expert Score', 'Price'] 0.08965466716306167 ['Expert Score', 'Price', 'ABV %'] 0.08923430250733025 ['Expert Score', 'Price', 'ABV %', 'Age'] 0.08899175542814948 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart'] 0.08898992695054489 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal'] 0.08907203953495638 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky'] 0.08904955558657023 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied'] 0.08912186240983001 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy'] 0.08922281913220305 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity'] 0.08939031344729367 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla'] 0.0895770795888184 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla', 'Sweet'] 0.089789108846626 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla', 'Sweet', 'Rich'] 0.08998644005302465 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla', 'Sweet', 'Rich', 'Floral'] 0.0902279116950343 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla', 'Sweet', 'Rich', 'Floral', 'Oily'] 0.09046982227650237 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla', 'Sweet', 'Rich', 'Floral', 'Oily', 'Salty'] 0.09078497439811871 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla', 'Sweet', 'Rich', 'Floral', 'Oily', 'Salty', 'Briny'] 0.09121076986135675 ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal', 'Smoky', 'Full-bodied', 'Spicy', 'Fruity', 'Vanilla', 'Sweet', 'Rich', 'Floral', 'Oily', 'Salty', 'Briny', 'Peaty'] An MSE of 0.09 seems pretty good to me! Let's compare it to other models. 3. kNN Let's start simple, as we did with Linear Regression. We begin by fitting a kNN regressor on 1 particular split (the original one). The purpose of starting small is to ensure we're doing it correctly and that the results make sense. In [33]: # create and fit a kNN regressor k = 5 knn_model = KNeighborsRegressor ( n_neighbors = k ) knn_model . fit ( xtrain . drop ([ 'Name' ], axis = 1 ), ytrain ) # evaluate on the training set y_train_pred = knn_model . predict ( xtrain . drop ([ 'Name' ], axis = 1 )) mse_train = mean_squared_error ( ytrain , y_train_pred ) # evaluate on the dev set y_dev_pred = knn_model . predict ( xdev . drop ([ 'Name' ], axis = 1 )) mse_dev = mean_squared_error ( ydev , y_dev_pred ) mse_dev print ( \"mse_train:\" , mse_train , \"mse_dev:\" , mse_dev ) mse_train: 0.14111582000000003 mse_dev: 0.1996808 Ok, it runs! However, those scores look much worse than what we started with when using linear regression. Hmm, hopefully it can improve. Let's vary our k value and see if we can do better for this original data split. In [34]: max_k = 20 train_MSEs = [] dev_MSEs = [] for k in range ( 1 , max_k ): knn_model = KNeighborsRegressor ( n_neighbors = k ) knn_model . fit ( xtrain . drop ([ 'Name' ], axis = 1 ), ytrain ) # evaluate on the training set y_train_pred = knn_model . predict ( xtrain . drop ([ 'Name' ], axis = 1 )) train_MSE = round ( mean_squared_error ( ytrain , y_train_pred ), 5 ) train_MSEs . append ( train_MSE ) # evaluate on the dev set y_dev_pred = knn_model . predict ( xdev . drop ([ 'Name' ], axis = 1 )) dev_MSE = round ( mean_squared_error ( ydev , y_dev_pred ), 5 ) dev_MSEs . append ( dev_MSE ) print ( \"k:\" , k , \"train_MSE:\" , train_MSE , \"dev_MSE:\" , dev_MSE ) print ( \"*** best result was k =\" , np . argmin ( dev_MSEs ), \":\" , np . min ( dev_MSEs )) k: 1 train_MSE: 0.00012 dev_MSE: 0.28597 k: 2 train_MSE: 0.08047 dev_MSE: 0.23643 k: 3 train_MSE: 0.11501 dev_MSE: 0.22813 k: 4 train_MSE: 0.13051 dev_MSE: 0.21735 k: 5 train_MSE: 0.14112 dev_MSE: 0.19968 k: 6 train_MSE: 0.14608 dev_MSE: 0.19611 k: 7 train_MSE: 0.15572 dev_MSE: 0.19327 k: 8 train_MSE: 0.16047 dev_MSE: 0.189 k: 9 train_MSE: 0.16409 dev_MSE: 0.19695 k: 10 train_MSE: 0.16373 dev_MSE: 0.19697 k: 11 train_MSE: 0.16789 dev_MSE: 0.19521 k: 12 train_MSE: 0.17082 dev_MSE: 0.19363 k: 13 train_MSE: 0.17434 dev_MSE: 0.19383 k: 14 train_MSE: 0.17493 dev_MSE: 0.19486 k: 15 train_MSE: 0.17601 dev_MSE: 0.19033 k: 16 train_MSE: 0.17767 dev_MSE: 0.18629 k: 17 train_MSE: 0.17972 dev_MSE: 0.18922 k: 18 train_MSE: 0.18203 dev_MSE: 0.18567 k: 19 train_MSE: 0.18227 dev_MSE: 0.18446 *** best result was k = 18 : 0.18446 In [35]: # let's visualize the trend ind = [ _ + 1 for _ in range ( max_k - 1 )] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 4 , 4 )) ax . plot ( ind , train_MSEs , 's-' , label = 'Train split' , color = '#02afed' , linewidth = 2 ) ax . plot ( ind , dev_MSEs , 's--' , label = 'Dev split' , color = '#355561' , linewidth = 2 ) ax . set_xlabel ( r '$K$ values' , fontsize = 10 ) ax . set_ylabel ( 'MSE' , fontsize = 10 ) ax . set_title ( r 'kNN Performance' ) ax . legend ( fontsize = 14 ) fig . tight_layout () Ah, that's not as good as linear regression! This is only over 1 split. Let's do the same procedure we did with linear regression, so that it's a fair comparison: feature selection + boostrapping + cross-validation. In [26]: best_num_features = np . argmin ( mse_across_features ) print ( \"best_num_features:\" , best_num_features ) print ( \"features:\" , selected_features [ 0 : best_num_features ]) print ( \"mse_across_features:\" , np . min ( mse_across_features )) best_num_features: 6 features: ['Expert Score', 'Price', 'ABV %', 'Age', 'Tart', 'Herbal'] mse_across_features: 0.08898992695054489 In [27]: min_k = 12 max_k = 18 num_bootstraps = 10 bootstrap_coefs = [] random . seed ( a = 21 ) bootstrap_seeds = random . sample ( range ( 100000 ), num_bootstraps ) # now we include all of the original 'train' # which encompasses the original 'dev' but excludes the 'test' merged = pd . concat ([ x_ , y_ ], axis = 1 , sort = False ) k_to_num_features = {} k_to_best_features = {} for k in range ( min_k , max_k ): selected_features = [] candidate_features = features [ 1 :] mse_across_features = [] while len ( candidate_features ) > 0 : best_mse = 9999 best_feature = \"\" for i , feature in enumerate ( candidate_features ): dev_MSEs = [] for i in range ( num_bootstraps ): boot_df = merged . sample ( frac = 1 , replace = True , random_state = bootstrap_seeds [ i ]) kf = KFold ( n_splits = 10 , random_state = 270 ) for train_index , dev_index in kf . split ( boot_df ): xtrain_cv = boot_df . iloc [ train_index ][ selected_features + [ feature ]] ytrain_cv = boot_df . iloc [ train_index ][ \"Customers \\' Rating\" ] xdev_cv = boot_df . iloc [ dev_index ][ selected_features + [ feature ]] ydev_cv = boot_df . iloc [ dev_index ][ \"Customers \\' Rating\" ] knn_model = KNeighborsRegressor ( n_neighbors = k ) knn_model . fit ( xtrain_cv , ytrain_cv ) # evaluate on the training set y_train_pred = knn_model . predict ( xtrain_cv ) train_MSE = round ( mean_squared_error ( ytrain_cv , y_train_pred ), 5 ) train_MSEs . append ( train_MSE ) # evaluate on the dev set y_dev_pred = knn_model . predict ( xdev_cv ) dev_MSE = round ( mean_squared_error ( ydev_cv , y_dev_pred ), 5 ) dev_MSEs . append ( dev_MSE ) cur_mse = np . mean ( dev_MSEs ) if cur_mse < best_mse : best_mse = cur_mse best_feature = feature mse_across_features . append ( np . min ( best_mse )) selected_features . append ( best_feature ) candidate_features . remove ( best_feature ) print ( \"k:\" , k , \"best_mse:\" , best_mse ) k_to_num_features [ k ] = mse_across_features k_to_best_features [ k ] = selected_features [ 0 : np . argmin ( mse_across_features ) + 1 ] k: 12 best_mse: 0.1869475999999999 k: 13 best_mse: 0.18727519999999998 k: 14 best_mse: 0.1882234 k: 15 best_mse: 0.1890631 k: 16 best_mse: 0.18984310000000001 k: 17 best_mse: 0.1905571 In [28]: # let's visualize the trend ind = [ _ + 1 for _ in range ( len ( features ) - 1 )] fig , ax = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) len ( k_to_num_features [ 12 ]) for k in range ( min_k , max_k ): ax . plot ( ind , k_to_num_features [ k ], 's-' , label = 'k=' + str ( k ), linewidth = 2 ) ax . set_xlabel ( r '# features' , fontsize = 10 ) ax . set_ylabel ( 'Avg MSE' , fontsize = 10 ) ax . set_title ( r 'Performance on the Dev Set' ) ax . legend ( fontsize = 10 ) xint = range ( min ( ind ), math . ceil ( max ( ind )) + 1 ) matplotlib . pyplot . xticks ( xint ) fig . tight_layout () In [29]: k_to_num_features Out[29]: {12: [0.13591640000000002, 0.10494430000000002, 0.09913929999999997, 0.09723290000000001, 0.09672829999999999, 0.09702670000000001, 0.10447569999999999, 0.11420819999999997, 0.12387369999999999, 0.1330116, 0.1400715, 0.14785450000000003, 0.15514190000000003, 0.1601279, 0.1669202, 0.17356739999999998, 0.1792516, 0.1819095, 0.1869475999999999], 13: [0.1343208, 0.10509180000000003, 0.09929230000000003, 0.09763709999999999, 0.097058, 0.09737620000000001, 0.10464460000000003, 0.11440120000000001, 0.12499389999999998, 0.13398030000000002, 0.1409857, 0.1484114, 0.1564101, 0.16226759999999998, 0.16834949999999996, 0.17425569999999996, 0.17974549999999997, 0.1829839, 0.18727519999999998], 14: [0.13382, 0.10542790000000002, 0.09966219999999999, 0.09805530000000001, 0.09768320000000003, 0.09808360000000002, 0.10480590000000004, 0.1145414, 0.1258255, 0.13452319999999998, 0.1418784, 0.1495025, 0.15825319999999998, 0.16450169999999997, 0.16951850000000004, 0.1748298, 0.1802204, 0.18430219999999997, 0.1882234], 15: [0.132942, 0.10539870000000001, 0.0999611, 0.09818810000000001, 0.09783450000000002, 0.09819739999999999, 0.10545439999999999, 0.11486910000000002, 0.1269057, 0.1350499, 0.142929, 0.1506209, 0.1598054, 0.16627439999999996, 0.1705622, 0.1755045, 0.18098350000000005, 0.1851066, 0.1890631], 16: [0.1323968, 0.10540299999999998, 0.10048880000000004, 0.09844439999999999, 0.0979636, 0.09835619999999999, 0.1058621, 0.1149024, 0.1274662, 0.13572700000000001, 0.1435734, 0.15104499999999998, 0.1613811, 0.1675207, 0.1721749, 0.17640419999999993, 0.18151380000000003, 0.1858502, 0.18984310000000001], 17: [0.13153670000000003, 0.10513460000000002, 0.1001751, 0.0982848, 0.09791950000000002, 0.09836379999999999, 0.10597120000000002, 0.11474799999999999, 0.12852940000000002, 0.1362188, 0.1448668, 0.1521721, 0.16188290000000002, 0.1695368, 0.17364449999999998, 0.1771931, 0.1826963, 0.187382, 0.1905571]} In [30]: best_k = 0 best_mse = 99999 for k in k_to_num_features . keys (): cur_num_features = np . argmin ( k_to_num_features [ k ]) + 1 cur_best_mse = np . min ( k_to_num_features [ k ]) if cur_best_mse < best_mse : best_mse = cur_best_mse best_k = k print ( \"best_k:\" , best_k , \"best_mse:\" , best_mse ) print ( \"best features:\" , k_to_best_features [ best_k ]) best_k: 12 best_mse: 0.09672829999999999 best features: ['Expert Score', 'ABV %', 'Price', 'Age', 'Rare'] QUESTION: The MSE when using kNN seems to come close to Linear Regression. Is there anything we could/should change about this kNN experiment? ANSWER: Scale our data! The features aren't on the same scale, so the distant metric used by kNN (e.g., Euclidean distance) doesn't account for this. The most useful features still include most of what we saw from Linear Regression, so this indicates that our lack of scaling is not a big problem, but it's definitely the proper thing to do. I'm surprised Price and Rare were able to make it into our list of top features, since their scales are just way smaller than the others that are from 0-100. In [ ]:","tags":"labs","url":"labs/lecture-27/notebook-2/"},{"title":"Lecture 27: Case Study 2","text":"Slides PDF | Lecture 27: Case Study 2 PPTX | Lecture 27: Case Study 2 Exercises Lecture 27: 1 Example Notebooks [Notebook] Lecture 27: 2 Example Notebooks [Notebook]","tags":"lectures","url":"lectures/lecture27/"},{"title":"S-Section 08: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost","text":"CS109A Introduction to Data Science Standard Section 8: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost. Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Hayden Joy In [ ]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) This section will work with a spam email dataset again. Our ultimate goal is to be able to build models so that we can predict whether an email is spam or not spam based on word characteristics within each email. We will review Decision Trees, Bagging, and Random Forest methods, and introduce Boosting: Ada Boost and XGBoost. Specifically, we will: Quick review of last week Trees in the context of the bias—variance tradeoff Rebuild the Decision Tree model, Bagging model, Random Forest Model just for comparison with Boosting. Theory: What is Boosting? Use the Adaboost on the Spam Dataset. Theory: What is Gradient Boosting and XGBoost? Use XGBoost on the Spam Dataset: Extreme Gradient Boosting Optional: Example to better understand Bias vs Variance tradeoff. 1. Quick review of last week ensemble: a group of items viewed as a whole rather than individually The Idea: Decision Trees are just flowcharts and interpretable! It turns out that simple flow charts can be formulated as mathematical models for classification and these models have the properties we desire; interpretable by humans have sufficiently complex decision boundaries the decision boundaries are locally linear, each component of the decision boundary is simple to describe mathematically. How to build Decision Trees (the Learning Algorithm in words): To learn a decision tree model, we take a greedy approach: Start with an empty decision tree (undivided feature space) Choose the ‘optimal' predictor on which to split and choose the ‘optimal' threshold value for splitting by applying a splitting criterion (1) Recurse on on each new node until stopping condition (2) is met So we need a (1) splitting criterion and a (2) stopping condition: (1) Splitting criterion (2) Stopping condition **Not stopping while building a deeper and deeper tree = 100% training accuracy; Yet we will overfit! To prevent the overfitting from happening, we should have stopping condition. How do we go from Classification to Regression? For classification, we return the majority class in the points of each leaf node. For regression we return the average of the outputs for the points in each leaf node. What is bagging? One way to adjust for the high variance of the output of an experiment is to perform the experiment multiple times and then average the results. Bootstrap: we generate multiple samples of training data, via bootstrapping. We train a full decision tree on each sample of data. AGgregatiING for a given input, we output the averaged outputs of all the models for that input. This method is called Bagging: B ootstrap + AGG regat ING . What is Random Forest? Many trees make a forest . Many random trees make a random forest . Random Forest is a modified form of bagging that creates ensembles of independent decision trees. To de-correlate the trees , we: train each tree on a separate bootstrap random sample of the full training set (same as in bagging) for each tree, at each split, we randomly select a set of 𝐽′ predictors from the full set of predictors. (not done in bagging) From amongst the 𝐽′ predictors, we select the optimal predictor and the optimal corresponding threshold for the split. Let's talk about decision trees, bagging, and random forest in the context of bias and variance. When is a decision tree underfit? When is a decision tree overfit? Let's think about this in the concept of tree depth. Bagging enjoys the benefits of High expressiveness (by using larger trees it is able to approximate complex functions and decision boundaries). Low by averaging the prediction of all the models thus reducing the in the final prediction. What is the weakness of bagging? In practice, the ensemble of trees tend to be highly ___ When could my bagging model be underfit? In what way does this apply to other ensemble methods? 2. Just re-building the tree models of last week Rebuild the Decision Tree model, Bagging model and Random Forest Model for comparison with Boosting methods In [ ]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns from tqdm import tqdm import sklearn.metrics as metrics import time from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn import tree from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.linear_model import LogisticRegressionCV from sklearn.model_selection import KFold from sklearn.metrics import confusion_matrix % matplotlib inline pd . set_option ( 'display.width' , 1500 ) pd . set_option ( 'display.max_columns' , 100 ) from sklearn.model_selection import learning_curve In [ ]: #Import Dataframe and Set Column Names spam_df = pd . read_csv ( 'data/spam.csv' , header = None ) columns = [ \"Column_\" + str ( i + 1 ) for i in range ( spam_df . shape [ 1 ] - 1 )] + [ 'Spam' ] spam_df . columns = columns display ( spam_df . head ()) In [ ]: #Let us split the dataset into a 70-30 split by using the following: #Split data into train and test np . random . seed ( 42 ) msk = np . random . rand ( len ( spam_df )) < 0.7 data_train = spam_df [ msk ] data_test = spam_df [ ~ msk ] #Split predictor and response columns x_train , y_train = data_train . drop ([ 'Spam' ], axis = 1 ), data_train [ 'Spam' ] x_test , y_test = data_test . drop ([ 'Spam' ] , axis = 1 ), data_test [ 'Spam' ] print ( \"Shape of Training Set :\" , data_train . shape ) print ( \"Shape of Testing Set :\" , data_test . shape ) In [ ]: #Check Percentage of Spam in Train and Test Set percentage_spam_training = 100 * y_train . sum () / len ( y_train ) percentage_spam_testing = 100 * y_test . sum () / len ( y_test ) print ( \"Percentage of Spam in Training Set \\t : {:0.2f} %.\" . format ( percentage_spam_training )) print ( \"Percentage of Spam in Testing Set \\t : {:0.2f} %.\" . format ( percentage_spam_testing )) Fitting an Optimal Single Decision Tree In [ ]: # Best depth for single decision trees of last week best_depth = 7 print ( \"The best depth was found to be:\" , best_depth ) In [ ]: #Evalaute the performance at the best depth model_tree = DecisionTreeClassifier ( max_depth = best_depth ) model_tree . fit ( x_train , y_train ) #Check Accuracy of Spam Detection in Train and Test Set acc_trees_training = accuracy_score ( y_train , model_tree . predict ( x_train )) acc_trees_testing = accuracy_score ( y_test , model_tree . predict ( x_test )) print ( \"Simple Decision Trees: Accuracy, Training Set \\t : {:.2%} \" . format ( acc_trees_training )) print ( \"Simple Decision Trees: Accuracy, Testing Set \\t : {:.2%} \" . format ( acc_trees_testing )) Fitting 100 Single Decision Trees while Bagging In [ ]: n_trees = 100 # we tried a variety of numbers here #Creating model np . random . seed ( 0 ) model = DecisionTreeClassifier ( max_depth = best_depth + 5 ) #Initializing variables predictions_train = np . zeros (( data_train . shape [ 0 ], n_trees )) predictions_test = np . zeros (( data_test . shape [ 0 ], n_trees )) #Conduct bootstraping iterations for i in range ( n_trees ): temp = data_train . sample ( frac = 1 , replace = True ) response_variable = temp [ 'Spam' ] temp = temp . drop ([ 'Spam' ], axis = 1 ) model . fit ( temp , response_variable ) predictions_train [:, i ] = model . predict ( x_train ) predictions_test [:, i ] = model . predict ( x_test ) #Make Predictions Dataframe columns = [ \"Bootstrap-Model_\" + str ( i + 1 ) for i in range ( n_trees )] predictions_train = pd . DataFrame ( predictions_train , columns = columns ) predictions_test = pd . DataFrame ( predictions_test , columns = columns ) In [ ]: #Function to ensemble the prediction of each bagged decision tree model def get_prediction ( df , count =- 1 ): count = df . shape [ 1 ] if count ==- 1 else count temp = df . iloc [:, 0 : count ] return np . mean ( temp , axis = 1 ) > 0.5 #Check Accuracy of Spam Detection in Train and Test Set acc_bagging_training = 100 * accuracy_score ( y_train , get_prediction ( predictions_train , count =- 1 )) acc_bagging_testing = 100 * accuracy_score ( y_test , get_prediction ( predictions_test , count =- 1 )) print ( \"Bagging: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_bagging_training )) print ( \"Bagging: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_bagging_testing )) Fitting Random Forest In [ ]: #Fit a Random Forest Model #Training model = RandomForestClassifier ( n_estimators = n_trees , max_depth = best_depth + 5 ) model . fit ( x_train , y_train ) #Predict y_pred_train = model . predict ( x_train ) y_pred_test = model . predict ( x_test ) #Performance Evaluation acc_random_forest_training = accuracy_score ( y_train , y_pred_train ) * 100 acc_random_forest_testing = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Random Forest: Accuracy, Training Set : {:0.2f} %\" . format ( acc_random_forest_training )) print ( \"Random Forest: Accuracy, Testing Set : {:0.2f} %\" . format ( acc_random_forest_testing )) Let's compare the performance of our 3 models: In [ ]: print ( \"Decision Trees: \\t Accuracy, Training Set \\t : {:.2%} \" . format ( acc_trees_training )) print ( \"Decision Trees: \\t Accuracy, Testing Set \\t : {:.2%} \" . format ( acc_trees_testing )) print ( \" \\n Bagging: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_bagging_training )) print ( \"Bagging: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_bagging_testing )) print ( \" \\n Random Forest: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_random_forest_training )) print ( \"Random Forest: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_random_forest_testing )) In [ ]: print ( len ( model . estimators_ [ 0 ] . tree_ . feature )) print ( len ( model . estimators_ [ 0 ] . tree_ . threshold )) Breakout Room 1: Exploring RandomForestClassifier class instances. For more resources on python classes (we're relying on them all the time via sklearn!) see this link. In [ ]: from functions import tree_pd import numpy as np import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns import sklearn.metrics as metrics from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn import tree from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.linear_model import LogisticRegressionCV from sklearn.model_selection import KFold from sklearn.metrics import confusion_matrix from sklearn import datasets from sklearn.ensemble import BaggingRegressor % matplotlib inline pd . set_option ( 'display.width' , 1500 ) pd . set_option ( 'display.max_columns' , 100 ) from sklearn.model_selection import learning_curve get_tree_pd = tree_pd . get_tree_pd cancer_scaled , target = tree_pd . load_cancer_dataset ( 10 , 4 ) ################################### Train Test split np . random . seed ( 40 ) #test_proportion test_prop = 0.2 msk = np . random . uniform ( 0 , 1 , len ( cancer_scaled )) > test_prop #Split predictor and response columns X_train , y_train = cancer_scaled [ msk ], target [ msk ] X_test , y_test = cancer_scaled [ ~ msk ], target [ ~ msk ] print ( \"Shape of Training Set :\" , X_train . shape ) print ( \"Shape of Testing Set :\" , X_test . shape ) ################################### Train a bagging and random forest model depth = 13 n_estimators = 100 best_rf_model = RandomForestClassifier ( max_depth = depth , random_state = 42 , n_estimators = n_estimators ) best_rf_model . fit ( X_train , y_train . reshape ( - 1 ,)) tree_rf_accuracy = best_rf_model . score ( X_test , y_test . reshape ( - 1 ,)) bagging_model = BaggingRegressor ( DecisionTreeClassifier ( max_depth = depth ), n_estimators = 100 , random_state = 42 ) . fit ( X_train , y_train . reshape ( - 1 ,)) Directions Run the cell below and look at the output. The .estimators attribute of a RandomForestClassifier class instance is a list of the individual DecisionTreeClassifier class instance estimators that make up the ensemble model. Calling .tree on the DecisionTreeClassifier will give you the individual tree estimator. Complete the function by extracting the impurity and feature attributes for each decision tree estimator at a specific decision node. Fix the creation of the dictionary at the bottom of the function and return a dataframe. In [ ]: help ( best_rf_model . estimators_ [ 0 ] . tree_ ) In [ ]: # %load \"../exercises/bo1.py\" def get_impurity_pd ( model , n = 0 ): \"\"\" This function returns a pandas dataframe with all of the nth nodes feature impurities. \"\"\" rf_estimators = model . estimators_ . copy () features = np . array ( X_train . columns ) node_impurities , node_features = [], [] for i , estimator in enumerate ( rf_estimators ): estimator_impurity = #TODO 0 estimator_feature = #TODO 1 node_impurities . append ( estimator_impurity ) node_features . append ( estimator_feature ) node_impurity_dict = { \"feature\" : #TODO \"impurity\" : #TODO df = #TODO return ( df ) In [ ]: # %load \"../solutions/impurity.py\" In [ ]: tree_node = 0 rf_df = get_impurity_pd ( best_rf_model , tree_node ) bagging_df = get_impurity_pd ( bagging_model , tree_node ) #plot fig , ax = plt . subplots ( 1 , 2 , figsize = ( 20 , 5 )) ax . ravel () sns . swarmplot ( x = \"feature\" , y = \"impurity\" , data = rf_df , ax = ax [ 0 ]) #sns.swarmplot(x = \"feature\", y = \"impurities\", data = rf_df, ax = ax[0]) ax [ 0 ] . tick_params ( labelrotation = 45 ) ax [ 0 ] . set_title ( \"Random Forest: Node 0 impurities after split\" ) sns . swarmplot ( x = \"feature\" , y = \"impurity\" , data = bagging_df , ax = ax [ 1 ]) ax [ 1 ] . set_title ( \"Bagging: Node 0 impurities after split\" ) plt . xticks ( rotation = 45 ); The limitations of random forest When can Random Forest overfit? Increasing the number of trees in RF generally doesn't increase the risk of overfitting, BUT if the number of trees in the ensemble is too large then the trees in the ensemble may become correlated, and therefore increase the variance. When can Random Forest fail? When we have a lot of predictors that are completely independent of the response and one overwhelmingly influential predictor . Why aren't random forests and bagging interpretable? How about a very deep decision tree? Bagging and random forest vs. Boosting Bagging and Random Forest: complex and deep trees overfit thus let's perform variance reduction on complex trees! Boosting: simple and shallow trees underfit thus let's perform bias reduction of simple trees! make the simple trees more expressive! Boosting attempts to improve the predictive flexibility of simple models. It trains a large number of \"weak\" learners in sequence . A weak learner is a constrained model (limit the max depth of each decision tree). Each one in the sequence focuses on learning from the mistakes of the one before it. By more heavily weighting in the mistakes in the next tree, our next tree will learn from the mistakes. A combining all the weak learners into a single strong learner = a boosted tree . Illustrative example (from source ) We built multiple trees consecutively: Tree 1 -> Tree 2 -> Tree 3 - > .... The size of the plus or minus signs indicates the weights of a data points for every Tree . How do we determine these weights? For each consecutive tree and iteration we do the following: The wrongly classified data points (\"mistakes\" = red circles) are identified and more heavily weighted in the next tree (green arrow) . Thus the size of the plus or minus changes in the next tree This change in weights will influence and change the next simple decision tree The correct predictions are identified and less heavily weighted in the next tree . We iterate this process for a certain number of times, stop and construct our final model: The ensemble ( \"Final: Combination\" ) is a linear combination of the simple trees, and is more expressive! The ensemble ( \"Final: Combination\" ) has indeed not just one simple decision boundary line, and fits the data better. What is Ada Boost? Ada Boost = Adaptive Boosting. AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers For an individual training point the loss can be defined as: $$\\text{ExpLoss_i} = \\begin{cases} e&#94;{\\hat{y}}, & \\text{if}\\ y=-1 \\\\ e&#94;{-\\hat{y}}, & \\text{} y=1 \\end{cases} $$ Illustrative Example (from slides) Step1. Start with equal distribition initially Step2. Fit a simple classifier Step3. Update the weights Step4. Update the classifier: First time trivial (we have no model yet.) Step2. Fit a simple classifier Step3. Update the weights: not shown. Step4. Update the classifier: 4. Use the Adaboost method to visualize Bias-Variance tradeoff. Now let's try Boosting! In [ ]: #Fit an Adaboost Model x_train , y_train = data_train . drop ([ 'Spam' ], axis = 1 ), data_train [ 'Spam' ] x_test , y_test = data_test . drop ([ 'Spam' ] , axis = 1 ), data_test [ 'Spam' ] #Training model = AdaBoostClassifier ( base_estimator = DecisionTreeClassifier ( max_depth = 3 ), n_estimators = 200 , learning_rate = 0.05 ) model . fit ( x_train , y_train ) #Predict y_pred_train = model . predict ( x_train ) y_pred_test = model . predict ( x_test ) #Performance Evaluation acc_boosting_training = accuracy_score ( y_train , y_pred_train ) * 100 acc_boosting_test = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Ada Boost: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_boosting_training )) print ( \"Ada Boost: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_boosting_test )) How does the test and training accuracy evolve with every iteration (tree)? In [ ]: #Plot Iteration based score train_scores = list ( model . staged_score ( x_train , y_train )) test_scores = list ( model . staged_score ( x_test , y_test )) plt . figure ( figsize = ( 10 , 7 )) plt . plot ( train_scores , label = 'train' ) plt . plot ( test_scores , label = 'test' ) plt . xlabel ( 'Iteration' ) plt . ylabel ( 'Accuracy' ) plt . title ( \"Variation of Accuracy with Iterations - ADA Boost\" ) plt . legend (); What about performance? In [ ]: print ( \"Decision Trees: \\t Accuracy, Testing Set \\t : {:.2%} \" . format ( acc_trees_testing )) print ( \"Bagging: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_bagging_testing )) print ( \"Random Forest: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_random_forest_testing )) print ( \"Ada Boost: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_boosting_test )) AdaBoost seems to be performing better than Simple Decision Trees and has a similar Test Set Accuracy performance compared to Random Forest. Random tip: If a \"for\"-loop takes som time and you want to know the progress while running the loop, use: tqdm() ( link ). No need for 1000's of print(i) outputs. Usage: for i in tqdm( range(start,finish) ): tqdm means \"progress\" in Arabic (taqadum, تقدّم) and tqdm is an abbreviation for \"I love you so much\" in Spanish (te quiero demasiado). What if we change the depth of our AdaBoost trees? In [ ]: ! pip install tqdm In [ ]: # Start Timer start = time . time () #Find Optimal Depth of trees for Boosting score_train , score_test , depth_start , depth_end = {}, {}, 2 , 30 for i in tqdm ( range ( depth_start , depth_end , 2 )): model = AdaBoostClassifier ( base_estimator = DecisionTreeClassifier ( max_depth = i ), n_estimators = 200 , learning_rate = 0.05 ) model . fit ( x_train , y_train ) score_train [ i ] = accuracy_score ( y_train , model . predict ( x_train )) score_test [ i ] = accuracy_score ( y_test , model . predict ( x_test )) # Stop Timer end = time . time () elapsed_adaboost = end - start In [ ]: #Plot lists1 = sorted ( score_train . items ()) lists2 = sorted ( score_test . items ()) x1 , y1 = zip ( * lists1 ) x2 , y2 = zip ( * lists2 ) plt . figure ( figsize = ( 10 , 7 )) plt . ylabel ( \"Accuracy\" ) plt . xlabel ( \"Depth\" ) plt . title ( 'Variation of Accuracy with Depth - ADA Boost Classifier' ) plt . plot ( x1 , y1 , 'b-' , label = 'Train' ) plt . plot ( x2 , y2 , 'g-' , label = 'Test' ) plt . legend () plt . show () Adaboost complexity depends on both the number of estimators and the base estimator. In the beginning as our model complexity increases (depth 2-3), we first observe a small increase in accuracy. But as we go further to the right of the graph ( deeper trees ), our model will overfit the data. REMINDER and validation: Boosting relies on simple trees! Breakout Room 2: Explore how changing the learning rate changes the training and testing accuracy. Use the Te Quero Demasiado (TQDM) wrap around your range as above. (Hint you will probably want to explore a range from $e&#94;{-6}$ to $e&#94;{-1}$ In [ ]: #TODO In [ ]: # %load \"../solutions/bo2.py\" Is this exercise useful? Food for Thought : Are boosted models independent of one another? Do they need to wait for the previous model's residuals? Are bagging or random forest models independent of each other , can they be trained in a parallel fashion? 5. Theory: What is Gradient Boosting and XGBoost? What is Gradient Boosting? To improve its predictions, gradient boosting looks at the difference between its current approximation, and the known correct target vector, which is called the residual . The mathematics: It may be assumed that there is some imperfect model $F_{m}$ The gradient boosting algorithm improves on $F_{m}$ constructing a new model that adds an estimator $h$ to provide a better model: $$F_{m+1}(x)=F_{m}(x)+h(x)$$ To find $h$, the gradient boosting solution starts with the observation that a perfect h would imply $$F_{m+1}(x)=F_{m}(x)+h(x)=y$$ or, equivalently solving for h, $$h(x)=y-F_{m}(x)$$ Therefore, gradient boosting will fit h to the residual $y-F_{m}(x)$ XGBoost: \"Long May She Reign!\" What is XGBoost and why is it so good!? Based on Gradient Boosting XGBoost = eXtreme Gradient Boosting ; refers to the engineering goal to push the limit of computations resources for boosted tree algorithm Accuracy: XGBoost however uses a more regularized model formalizaiton to control overfitting (=better performance) by both L1 and L2 regularization. Tree Pruning methods: more shallow tree will also prevent overfitting Improved convergence techniques (like early stopping when no improvement is made for X number of iterations) Built-in Cross-Validaiton Computing Speed: Special Vector and matrix type data structures for faster results. Parallelized tree building: using all of your CPU cores during training. Distributed Computing: for training very large models using a cluster of machines. Cache Optimization of data structures and algorithm: to make best use of hardware. XGBoost is building boosted trees in parallel? What? How? No: Xgboost doesn't run multiple trees in parallel, you need predictions after each tree to update gradients. Rather it does the parallelization WITHIN a single tree my using openMP to create branches independently. 6. Use XGBoost: Extreme Gradient Boosting In [ ]: # Let's install XGBoost ! pip install xgboost In [ ]: import xgboost as xgb # Create the training and test data dtrain = xgb . DMatrix ( x_train , label = y_train ) dtest = xgb . DMatrix ( x_test , label = y_test ) # Parameters param = { 'max_depth' : best_depth , # the maximum depth of each tree 'eta' : 0.3 , # the training step for each iteration 'silent' : 1 , # logging mode - quiet 'objective' : 'multi:softprob' , # error evaluation for multiclass training 'num_class' : 2 } # the number of classes that exist in this datset # Number of training iterations num_round = 200 # Start timer start = time . time () # Train XGBoost bst = xgb . train ( param , dtrain , num_round , evals = [( dtrain , 'train' )], early_stopping_rounds = 20 , # early stopping verbose_eval = 20 ) # Make prediction training set preds_train = bst . predict ( dtrain ) best_preds_train = np . asarray ([ np . argmax ( line ) for line in preds_train ]) # Make prediction test set preds_test = bst . predict ( dtest ) best_preds_test = np . asarray ([ np . argmax ( line ) for line in preds_test ]) # Performance Evaluation acc_XGBoost_training = accuracy_score ( y_train , best_preds_train ) * 100 acc_XGBoost_test = accuracy_score ( y_test , best_preds_test ) * 100 # Stop Timer end = time . time () elapsed_xgboost = end - start print ( \"XGBoost: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_XGBoost_training )) print ( \"XGBoost: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_XGBoost_test )) What about the accuracy performance: AdaBoost versus XGBoost? In [ ]: print ( \"Ada Boost: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_boosting_test )) print ( \"XGBoost: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_XGBoost_test )) What about the computing performance: AdaBoost versus XGBoost? In [ ]: print ( \"AdaBoost elapsed time: \\t {:0.2f} s\" . format ( elapsed_adaboost )) print ( \"XGBoost elapsed time: \\t {:0.2f} s\" . format ( elapsed_xgboost )) What if we change the depth of our XGBoost trees and compare to Ada Boost? In [ ]: def model_xgboost ( best_depth ): param = { 'max_depth' : best_depth , # the maximum depth of each tree 'eta' : 0.3 , # the training step for each iteration 'verbosity' : 0 , # logging mode - quiet 'objective' : 'multi:softprob' , # error evaluation for multiclass training 'num_class' : 2 } # the number of classes that exist in this datset # the number of training iterations num_round = 200 bst = xgb . train ( param , dtrain , num_round , evals = [( dtrain , 'train' )], early_stopping_rounds = 20 , verbose_eval = False ) preds_train = bst . predict ( dtrain ) best_preds_train = np . asarray ([ np . argmax ( line ) for line in preds_train ]) preds_test = bst . predict ( dtest ) best_preds_test = np . asarray ([ np . argmax ( line ) for line in preds_test ]) #Performance Evaluation XGBoost_training = accuracy_score ( y_train , best_preds_train ) XGBoost_test = accuracy_score ( y_test , best_preds_test ) return XGBoost_training , XGBoost_test In [ ]: #Find Optimal Depth of trees for Boosting score_train_xgb , score_test_xgb = {}, {} depth_start , depth_end = 2 , 30 for i in trange ( depth_start , depth_end , 2 ): XGBoost_training , XGBoost_test = model_xgboost ( i ) score_train_xgb [ i ] = XGBoost_training score_test_xgb [ i ] = XGBoost_test In [ ]: #Plot lists1 = sorted ( score_train_xgb . items ()) lists2 = sorted ( score_test_xgb . items ()) x3 , y3 = zip ( * lists1 ) x4 , y4 = zip ( * lists2 ) plt . figure ( figsize = ( 10 , 7 )) plt . ylabel ( \"Accuracy\" ) plt . xlabel ( \"Depth\" ) plt . title ( 'Variation of Accuracy with Depth - Adaboost & XGBoost Classifier' ) plt . plot ( x1 , y1 , label = 'Train Accuracy Ada Boost' ) plt . plot ( x2 , y2 , label = 'Test Accuracy Ada Boost' ) plt . plot ( x3 , y3 , label = 'Train Accuracy XGBoost' ) plt . plot ( x4 , y4 , label = 'Test Accuracy XGBoost' ) plt . legend () plt . show () Interesting : No real optimal depth of the simple tree for XGBoost, probably a lot of regularization, pruning, or early stopping when using a deep tree at the start. XGBoost does not seem to overfit when the depth of the tree increases, as opposed to Ada Boost. All the accuracy performances: In [ ]: print ( \"Decision Trees: \\t Accuracy, Testing Set \\t : {:.2%} \" . format ( acc_trees_testing )) print ( \"Bagging: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_bagging_testing )) print ( \"Random Forest: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_random_forest_testing )) print ( \"Ada Boost: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_boosting_test )) print ( \"XGBoost: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_XGBoost_test )) Overview of all the tree algorithms: Source End of Section Optional: Example to better understand Bias vs Variance tradeoff. A central notion underlying what we've been learning in lectures and sections so far is the trade-off between overfitting and underfitting. If you remember back to Homework 3, we had a model that seemed to represent our data accurately. However, we saw that as we made it more and more accurate on the training set, it did not generalize well to unobserved data. As a different example, in face recognition algorithms, such as that on the iPhone X, a too-accurate model would be unable to identity someone who styled their hair differently that day. The reason is that our model may learn irrelevant features in the training data. On the contrary, an insufficiently trained model would not generalize well either. For example, it was recently reported that a face mask could sufficiently fool the iPhone X. A widely used solution in statistics to reduce overfitting consists of adding structure to the model, with something like regularization. This method favors simpler models during training. The bias-variance dilemma is closely related. The bias of a model quantifies how precise a model is across training sets. The variance quantifies how sensitive the model is to small changes in the training set. A robust model is not overly sensitive to small changes. The dilemma involves minimizing both bias and variance ; we want a precise and robust model. Simpler models tend to be less accurate but more robust. Complex models tend to be more accurate but less robust. How to reduce bias: Use more complex models, more features, less regularization, ... Boosting: attempts to improve the predictive flexibility of simple models. Boosting uses simple base models and tries to \"boost\" their aggregate complexity. How to reduce variance: Early Stopping: Its rules provide us with guidance as to how many iterations can be run before the learner begins to over-fit. Pruning: Pruning is extensively used while building related models. It simply removes the nodes which add little predictive power for the problem in hand. Regularization: It introduces a cost term for bringing in more features with the objective function. Hence it tries to push the coefficients for many variables to zero and hence reduce cost term. Train with more data: It won't work every time, but training with more data can help algorithms detect the signal better. Ensembling: Ensembles are machine learning methods for combining predictions from multiple separate models. For example: Bagging attempts to reduce the chance of overfitting complex models: Bagging uses complex base models and tries to \"smooth out\" their predictions. Interesting Piazza post: why randomness in simple decision tree? \"Hi there. I notice that there is a parameter called \"random_state\" in decision tree function and I wonder why we need randomness in simple decision tree. If we add randomness in such case, isn't it the same as random forest?\" The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node . Such algorithms cannot guarantee to return the globally optimal decision tree . This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement (Bagging). For example: What is the defaulth DecisionTreeClassifier behaviour when there are 2 or more best features for a certain split (a tie among \"splitters\")? (after a deep dive and internet search link ): The current default behaviour when splitter=\"best\" is to shuffle the features at each step and take the best feature to split. In case there is a tie, we take a random one.","tags":"sections","url":"sections/sec_8/"},{"title":"S-Section 08: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost","text":"Jupyter Notebooks S-Section 8: Review Trees and Boosting including Ada Boosting Gradient Boosting and XGBoost","tags":"sections","url":"sections/section8/"},{"title":"Advanced Section 5: Stacking and mixture of experts","text":"Slides A-Section 5: Stacking [PDF]","tags":"a-sections","url":"a-sections/a-section5/"},{"title":"Lecture 26: Boosting Methods for Classification","text":"In [1]: # Import necessary libraries import matplotlib.pyplot as plt from matplotlib.colors import ListedColormap import numpy as np import seaborn as sns sns . set_style ( 'white' ) import pandas as pd from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import AdaBoostClassifier from helper import plot_decision_boundary % matplotlib inline In [2]: # Read the dataset as a pandas dataframe df = pd . read_csv ( \"boostingclassifier.csv\" ) # Read the columns latitude and longitude as the predictor variables X = df [[ 'latitude' , 'longitude' ]] . values # Landtype is the response variable y = df [ 'landtype' ] . values In [3]: # AdaBoost algorithm implementation from scratch def AdaBoost_scratch ( X , y , M = 10 , learning_rate = 1 ): #Initialization of utility variables N = len ( y ) estimator_list , y_predict_list , estimator_error_list , estimator_weight_list , sample_weight_list = [], [],[],[],[] #Initialize the sample weights sample_weight = np . ones ( N ) / N # Cooy the sample weights to another list sample_weight_list . append ( sample_weight . copy ()) #For m = 1 to M where M is the number of stumps for m in range ( M ): #Fit a Decision Tree classifier stump with a maximum of 2 leaf nodes estimator = ___ # Fit the model on the entire data with the sample weight initialise before estimator . fit ( ___ ) # Predict on the entire data y_predict = estimator . predict ( X ) # Compute the number of misclassifications incorrect = ( y_predict != y ) # Compute the error as the mean of the weighted sum of the number of incorrect predictions given the sample weight estimator_error = ___ # Compute the new weights using the learning rate and estimator error estimator_weight = ___ # Boost the sample weights sample_weight *= np . exp ( estimator_weight * incorrect * (( sample_weight > 0 ) | ( estimator_weight < 0 ))) # Save the iteration values estimator_list . append ( estimator ) y_predict_list . append ( y_predict . copy ()) estimator_error_list . append ( estimator_error . copy ()) estimator_weight_list . append ( estimator_weight . copy ()) sample_weight_list . append ( sample_weight . copy ()) #Convert to np array for convenience estimator_list = np . asarray ( estimator_list ) y_predict_list = np . asarray ( y_predict_list ) estimator_error_list = np . asarray ( estimator_error_list ) estimator_weight_list = np . asarray ( estimator_weight_list ) sample_weight_list = np . asarray ( sample_weight_list ) # Compute the predictions preds = ( np . array ([ np . sign (( y_predict_list [:, point ] * estimator_weight_list ) . sum ()) for point in range ( N )])) # Return the model, estimated weights and sample weights return estimator_list , estimator_weight_list , sample_weight_list In [4]: ### edTest(test_adaboost) ### # Call the AdaBoost function to perform boosting classification estimator_list , estimator_weight_list , sample_weight_list = AdaBoost_scratch ( X , y , M = 6 , learning_rate = 1 ) In [0]: # Helper code to plot the AdaBoost Decision Boundary stumps fig = plt . figure ( figsize = ( 14 , 14 )) for m in range ( 0 , 6 ): fig . add_subplot ( 3 , 2 , m + 1 ) s_weights = ( sample_weight_list [ m ,:] / sample_weight_list [ m ,:] . sum () ) * 300 plot_decision_boundary ( estimator_list [ m ], X , y , N = 50 , scatter_weights = s_weights , counter = m ) plt . tight_layout () In [5]: # Use sklearn's AdaBoostClassifier to take a look at the final decision boundary # Initialise the model with Decision Tree classifier as the base model same as above # Use SAMME as the algorithm and 6 estimators with learning rate as 1 boost = AdaBoostClassifier ( base_estimator = DecisionTreeClassifier ( max_depth = 1 , max_leaf_nodes = 2 ), algorithm = 'SAMME' , n_estimators = 6 , learning_rate = 1.0 ) # Fit on the entire data boost . fit ( X , y ) # Call the plot_decision_boundary function to plot the decision boundary of the model plot_decision_boundary ( boost , X , y , N = 50 ) plt . title ( 'AdaBoost Decision Boundary' , fontsize = 16 ) plt . show () Mindchow 🍲 Use the helper code below to visualize the sequential growth of trees using Adaboost Play around with the learning_rate and the num_estimators and see how it affects the trees Your answer here","tags":"labs","url":"labs/lecture-26/notebook/"},{"title":"Lecture 26: Boosting Methods for Classification","text":"In [0]: # Import necessary libraries # Evaluate bagging ensemble for regression import numpy as np from sklearn.ensemble import BaggingRegressor from sklearn.ensemble import GradientBoostingRegressor import matplotlib.pyplot as plt import pandas as pd import itertools from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split % matplotlib inline In [0]: # Read the dataset airquality.csv df = pd . read_csv ( \"airquality.csv\" ) In [0]: # Take a quick look at the data # Remove rows with missing values df = df [ df . Ozone . notna ()] df . head () In [0]: # Assign \"x\" column as the predictor variable and \"y\" as the # We only use Ozone as a predictor for this exercise and Temp as the response x , y = df [ 'Ozone' ] . values , df [ 'Temp' ] . values # Fancy way of sorting on X # We do this now because we will not split the data # into train/val/test in this part of the exercise x , y = list ( zip ( * sorted ( zip ( x , y )))) x , y = np . array ( x ) . reshape ( - 1 , 1 ), np . array ( y ) Part A: Gradient Boosting by hand In [0]: # Fit a single decision tree stump on the entire data basemodel = ___ ___ # Predict on the entire data y_pred = ___ In [0]: # Helper code to plot the data plt . figure ( figsize = ( 10 , 6 )) xrange = np . linspace ( x . min (), x . max (), 100 ) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_first_residuals) ### # Calculate the Error Residuals residuals = ___ In [0]: # Helper code to plot the data with the residuals plt . figure ( figsize = ( 10 , 6 )) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . plot ( x , residuals , '.-' , color = '#faa0a6' , markersize = 6 , label = \"Residuals\" ) plt . plot ([ x . min (), x . max ()],[ 0 , 0 ], '--' ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'center right' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_fitted_residuals) ### # Fit another tree stump on the residuals dtr = ___ ___ # Predict on the entire data y_pred_residuals = ___ In [0]: # Helper code to add the fit of the residuals to the original plot plt . figure ( figsize = ( 10 , 6 )) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . plot ( x , residuals , '.-' , color = '#faa0a6' , markersize = 6 , label = \"Residuals\" ) plt . plot ([ x . min (), x . max ()],[ 0 , 0 ], '--' ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . plot ( x , y_pred_residuals , alpha = 0.7 , linewidth = 3 , color = 'red' , label = 'Residual Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'center right' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_new_pred) ### # Set a lambda value and compute the predictions based on the residuals lambda_ = ___ y_pred_new = ___ In [0]: # Helper code to plot the boosted tree plt . figure ( figsize = ( 10 , 8 )) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . plot ( x , residuals , '.-' , color = '#faa0a6' , markersize = 6 , label = \"Residuals\" ) plt . plot ([ x . min (), x . max ()],[ 0 , 0 ], '--' ) plt . xlim () plt . plot ( x , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'First Tree' ) plt . plot ( x , y_pred_residuals , alpha = 0.7 , linewidth = 3 , color = 'red' , label = 'Residual Tree' ) plt . plot ( x , y_pred_new , alpha = 0.7 , linewidth = 3 , color = 'k' , label = 'Boosted Tree' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'center right' , fontsize = 12 ) plt . show () Mindchow 🍲 You can continue doing this! Try at least one more interation. Part 2: Comparison with Bagging To compare the two methods, we will be using sklearn's methods and not our own implementation from above. In [0]: # Split the data into train and test sets with train size as 0.8 # and random_state as 102 # The default value for shuffle is True for train_test_split, so the ordering we # did above is not a problem. x_train , x_test , y_train , y_test = train_test_split ( x , y , train_size = 0.8 , random_state = 102 ) In [0]: ### edTest(test_boosting) ### # Set a learning rate l_rate = ___ # Initialise a Boosting model using sklearn's boosting model # Use 5000 estimators, depth of 1 and learning rate as defined above boosted_model = ___ # Fit on the train data ___ # Predict on the test data y_pred = ___ In [0]: # Train a bagging model # Specify the number of bootstraps num_bootstraps = 30 # Specify the maximum depth of the decision tree max_depth = 100 # Define the Bagging Regressor Model # Use Decision Tree as your base estimator with depth as mentioned in max_depth # Initialise number of estimators using the num_bootstraps value # Set max_samples as 0.8 and random_state as 3 bagging_model = ___ # Fit the model on the train data ___ # Predict on the test data y_pred_bagging = ___ In [0]: # Helper code to plot the bagging and boosting model predictions plt . figure ( figsize = ( 10 , 8 )) xrange = np . linspace ( x . min (), x . max (), 100 ) . reshape ( - 1 , 1 ) y_pred_boost = boosted_model . predict ( xrange ) y_pred_bag = bagging_model . predict ( xrange ) plt . plot ( x , y , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"True Data\" ) plt . xlim () plt . plot ( xrange , y_pred_boost , alpha = 0.7 , linewidth = 3 , color = '#77c2fc' , label = 'Bagging' ) plt . plot ( xrange , y_pred_bag , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'Boosting' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show () In [0]: ### edTest(test_mse) ### # Compute the MSE of the Boosting model prediction on the test data boost_mse = ___ print ( \"The MSE of the Boosting model is\" , boost_mse ) In [0]: # Compute the MSE of the Bagging model prediction on the test data bag_mse = ___ print ( \"The MSE of the Bagging model is\" , bag_mse ) Mindchow 🍲 To be fair, we should fine tune the hyper-parameters for both models. Go back and play with the learning rate , n_estimators and max_depth for Boosting and n_estimators and max_depth for Bagging. How does RF compare? Your answer here","tags":"labs","url":"labs/lecture-26/notebook-2/"},{"title":"Lecture 26: Boosting Methods for Classification","text":"Slides PDF | Lecture 26: Gradient Boosting PPTX | Lecture 26: Gradient Boosting PDF | Lecture 26: AdaBoositng PPTX | Lecture 26: AdaBoositng Exercises Lecture 26: Regression with Boosting [Notebook] Lecture 26: Boosting Classification [Notebook]","tags":"lectures","url":"lectures/lecture26/"},{"title":"Lecture 25: Boosting Methods for Regression","text":"In [0]: # Import the necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance from sklearn.tree import DecisionTreeClassifier from helper import plot_permute_importance , plot_feature_importance % matplotlib inline In [0]: # Read the dataset and take a quick look df = pd . read_csv ( \"heart.csv\" ) df . head () In [0]: # Assign the predictor and response variables. # 'AHD' is the response and all the other columns are the predictors X = ___ y = ___ In [0]: # Set the parameters # The random state is fized for testing purposes random_state = 44 # Choose a `max_depth` for your trees max_depth = ___ SINGLE TREE In [0]: ### edTest(test_decision_tree) ### # Define a Decision Tree classifier with random_state as the above defined variable # Set the maximum depth to be max_depth tree = __ # Fit the model on the entire data tree . fit ( X , y ); # Using Permutation Importance to get the importance of features for the Decision Tree # With random_state as the above defined variable tree_result = ___ RANDOM FOREST In [0]: ### edTest(test_random_forest) ### # Define a Random Forest classifier with random_state as the above defined variable # Set the maximum depth to be max_depth and use 10 estimators forest = ___ # Fit the model on the entire data forest . fit ( X , y ); # Use Permutation Importance to get the importance of features for the Random Forest model # With random_state as the above defined variable forest_result = ___ PLOTTING THE FEATURE RANKING In [0]: # Use the helper code given to visualize the feature importance using 'MDI' plot_feature_importance ( tree , forest , X , y ); # Use the helper code given to visualize the feature importance using 'permutation feature importance' plot_permute_importance ( tree_result , forest_result , X , y ); Mindchow 🍲 Q1. A common criticism for the MDI method is that it assigns a lot of importance to noisy features (more here ). Did you make such an observation in the plots above? Your answer here Q2. After marking, change the max_depth for your classifiers to a very low value such as $3$, and see if you see a change in the relative importance of predictors. Your answer here","tags":"labs","url":"labs/lecture-25/notebook/"},{"title":"Lecture 25: Boosting Methods for Regression","text":"In [0]: # Import the main packages import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import f1_score from sklearn.metrics import roc_auc_score from imblearn.over_sampling import SMOTE from prettytable import PrettyTable % matplotlib inline In [0]: # Read the dataset and take a quick look df = pd . read_csv ( 'diabetes.csv' ) df . head () In [0]: # On checking the response variable ('Outcome') value counts, you will notice that the number of diabetics are less than the number of non-diabetics df [ 'Outcome' ] . value_counts () In [0]: ### edTest(test_imbalance) ### # To estimate the amount of data imbalance, find the ratio of class 1(Diabetics) to the size of the dataset. imbalance_ratio = ___ print ( f 'The percentage of diabetics in the dataset is only { ( imbalance_ratio ) * 100 : .2f } %' ) In [0]: # Assign the predictor and response variables. # Outcome is the response and all the other columns are the predictors X = ___ y = ___ In [0]: # Fix a random_state and split the data into train and validation sets random_state = 22 X_train , X_val , y_train , y_val = train_test_split ( X , y , train_size = 0.8 , random_state = random_state ) In [0]: # We fix the max_depth variable to 20 for all trees, you can come back and change this to investigate performance of RandomForest max_depth = 20 Strategy 1 - Vanilla Random Forest No correction for imbalance In [0]: # Define a Random Forest classifier with random_state as above # Set the maximum depth to be max_depth and use 10 estimators random_forest = ___ # Fit the model on the training set random_forest . ___ In [0]: # We make predictions on the validation set predictions = ___ # We also compute two metrics that better represent misclassification of minority classes i.e `f1 score` and `AUC` # Compute the f1-score and assign it to variable score1 score1 = ___ # Compute the `auc` and assign it to variable auc1 auc1 = ___ Strategy 2 - Random Forest with class weighting Balancing the class imbalance in each bootstrap In [0]: # Define a Random Forest classifier with random_state as above # Set the maximum depth to be max_depth and use 10 estimators # Specify `class_weight='balanced_subsample' random_forest = ___ # Fit the model on the training data random_forest . ___ In [0]: # We make predictions on the validation set predictions = ___ # Again we also compute two metrics that better represent misclassification of minority classes i.e `f1 score` and `AUC` # Compute the f1-score and assign it to variable score2 score2 = ___ # Compute the `auc` and assign it to variable auc2 auc2 = ___ Strategy 3 - RandomForest with SMOTE We can use SMOTE along with the previous method to further improve our metrics. Read more about imblearn's SMOTE here . In [0]: # Run this cell below to use SMOTE to balance our dataset sm = SMOTE ( random_state = 3 ) X_train_res , y_train_res = sm . fit_sample ( X_train , y_train . ravel ()) #If you now see the shape, you will see that X_train_res has more number of points than X_train print ( f 'Number of points in balanced dataset is { X_train_res . shape [ 0 ] } ' ) In [0]: # Again Define a Random Forest classifier with random_state as above # Set the maximum depth to be max_depth and use 10 estimators # Again specify `class_weight='balanced_subsample' random_forest = ___ # Fit the model on the new training data created above with SMOTE random_forest . ___ In [0]: ### edTest(test_smote) ### # We make predictions on the validation set predictions = ___ # Again we also compute two metrics that better represent misclassification of minority classes i.e `f1 score` and `AUC` # Compute the f1-score and assign it to variable score3 score3 = ___ # Compute the `auc` and assign it to variable auc3 auc3 = ___ In [0]: # Finally, we compare the results from the three implementations above # Just run the cells below to see your results pt = PrettyTable () pt . field_names = [ \"Strategy\" , \"F1 Score\" , \"AUC score\" ] pt . add_row ([ \"Random Forest - no correction\" , score1 , auc1 ]) pt . add_row ([ \"Random Forest - class weighting\" , score2 , auc2 ]) pt . add_row ([ \"Random Forest - SMOTE upsampling\" , score3 , auc3 ]) print ( pt ) Mindchow 🍲 Go back and change the learning_rate parameter and n_estimators for Adaboost. Do you see an improvement in results? Your answer here","tags":"labs","url":"labs/lecture-25/notebook-2/"},{"title":"Lecture 25: Boosting Methods for Regression","text":"Slides PDF | Lecture 25: RF Feature Importance PPTX | Lecture 25: RF Feature Importance PDF | Lecture 25: Boositng PPTX | Lecture 25: Boositng Exercises Lecture 25: Feature Importance [Notebook] Lecture 25: Random Forest vs SMOTE Classification [Notebook]","tags":"lectures","url":"lectures/lecture25/"},{"title":"Lecture 24: Tuning Hyperparameters","text":"In [1]: # Import the necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import train_test_split from sklearn.inspection import permutation_importance from sklearn.tree import DecisionTreeClassifier % matplotlib inline In [2]: # Read the dataset and take a quick look df = pd . read_csv ( \"diabetes.csv\" ) df . head () In [3]: # Assign the predictor and response variables. # Outcome is the response and all the other columns are the predictors X = df . drop ( \"Outcome\" , axis = 1 ) y = df [ 'Outcome' ] X . shape , y . shape In [4]: # set the seed for reproducibility of results seed = 0 # split in train and test X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = seed ) X_train . shape , y_train . shape , X_test . shape , y_test . shape Vanila random forest Start by training a Random Forest Classifier using the default parameters and calculate the Receiver Operating Characteristic Area Under the Curve (ROC AUC). As we know, this metric is better than accuracy for a classification problem, since it covers the case of an imbalanced dataset. In [5]: ### edTest(test_vanilla) ### from sklearn.metrics import roc_auc_score # Define a Random Forest classifier with randon_state = seed vanilla_rf = __ # Fit the model on the entire data vanilla_rf . fit ( __ , __ ); # Calculate AUC/ROC on the test set y_proba = __ [:, 1 ] auc = np . round ( roc_auc_score ( y_test , y_proba ), 2 ) print ( f 'Plain RF AUC on test set: { auc } ' ) In [6]: # number of samples and features num_features = X_train . shape [ 1 ] num_samples = X_train . shape [ 0 ] num_samples , num_features 1. Number of trees, num_iterators , default = 100 The number of trees needs to be large enough for the $oob$ error to stabilize in its lowest possible value. Plot the $oob$ error of a random forest as a function of the number of trees. Trees in a RF are called estimators . A good start is 10 times the number of features, however, adjusting other hyperparameters will influence the optimum number of trees. In [7]: %%time from collections import OrderedDict clf = RandomForestClassifier ( warm_start = True , oob_score = True , min_samples_leaf = 40 , max_depth = 10 , random_state = seed ) error_rate = {} # Range of `n_estimators` values to explore. min_estimators = 150 max_estimators = 500 for i in range ( min_estimators , max_estimators + 1 ): clf . set_params ( n_estimators = i ) clf . fit ( X_train , y_train ) # Record the OOB error for each `n_estimators=i` setting. oob_error = 1 - clf . oob_score_ error_rate [ i ] = oob_error In [8]: %%time # Generate the \"OOB error rate\" vs. \"n_estimators\" plot. # OOB error rate = num_missclassified/total observations (%)\\ xs = [] ys = [] for label , clf_err in error_rate . items (): xs . append ( label ) ys . append ( clf_err ) plt . plot ( xs , ys ) plt . xlim ( min_estimators , max_estimators ) plt . xlabel ( \"n_estimators\" ) plt . ylabel ( \"OOB error rate\" ) plt . show () 2. min_samples_leaf , default = 1 The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression. We will plot various values of the min_samples_leaf with num_iterators . In [9]: %%time from collections import OrderedDict ensemble_clfs = [ ( 1 , RandomForestClassifier ( warm_start = True , min_samples_leaf = 1 , oob_score = True , max_depth = 10 , random_state = seed )), ( 5 , RandomForestClassifier ( warm_start = True , min_samples_leaf = 5 , oob_score = True , max_depth = 10 , random_state = seed )) ] # Map a label (the value of `min_samples_leaf`) to a list of (model, oob error) tuples. error_rate = OrderedDict (( label , []) for label , _ in ensemble_clfs ) min_estimators = 80 max_estimators = 500 for label , clf in ensemble_clfs : for i in range ( min_estimators , max_estimators + 1 ): clf . set_params ( n_estimators = i ) clf . fit ( X_train , y_train ) # Record the OOB error for each model. Error is 1 - oob_score # oob_score: score of the training dataset obtained using an # out-of-bag estimate. # OOB error rate is % of num_missclassified/total observations oob_error = 1 - clf . oob_score_ error_rate [ label ] . append (( i , oob_error )) for label , clf_err in error_rate . items (): xs , ys = zip ( * clf_err ) plt . plot ( xs , ys , label = f 'min_samples_leaf= { label } ' ) plt . xlim ( min_estimators , max_estimators ) plt . xlabel ( \"n_estimators\" ) plt . ylabel ( \"OOB error rate\" ) plt . legend ( loc = \"upper right\" ) plt . show () In [10]: err = 100 best_num_estimators = 0 for label , clf_err in error_rate . items (): num_estimators , error = min ( clf_err , key = lambda n : ( n [ 1 ], - n [ 0 ])) if error < err : err = error ; best_num_estimators = num_estimators ; best_leaf = label print ( f 'Optimum num of estimators: { best_num_estimators } \\n min_samples_leaf: { best_leaf } ' ) Re-train the Random Forest Classifier using the new values for the parameters and calculate the AUC/ROC. Include another parameter, the max_features , the number of features to consider when looking for the best split. In [11]: ### edTest(test_estimators) ### estimators_rf = RandomForestClassifier ( n_estimators = best_num_estimators , random_state = seed , oob_score = True , min_samples_leaf = best_leaf , max_features = 'sqrt' ) # Fit the model on the entire data estimators_rf . fit ( X_train , y_train ); # Calculate AUC/ROC on the test set y_proba = __ [:, 1 ] estimators_auc = np . round ( roc_auc_score ( y_test , y_proba ), 2 ) print ( f 'Educated RF AUC on test set: { estimators_auc } ' ) Look at the model's parameters In [12]: estimators_rf . get_params () 3. Performing a cross-validation search After we have some idea of the range of optimum values for the number of trees and maybe a couple of other parameters, and have enough computing power, you may perform an exhaustive search over other parameter values. In [13]: %%time from sklearn.model_selection import GridSearchCV do_grid_search = True if do_grid_search : rf = RandomForestClassifier ( n_jobs =- 1 , n_estimators = best_num_estimators , oob_score = True , max_features = 'sqrt' , min_samples_leaf = best_leaf , random_state = seed ) . fit ( X_train , y_train ) param_grid = { 'min_samples_split' : [ 2 , 5 , None ]} scoring = { 'AUC' : 'roc_auc' } grid_search = GridSearchCV ( rf , param_grid , scoring = scoring , refit = 'AUC' , return_train_score = True , n_jobs =- 1 ) results = grid_search . fit ( X_train , y_train ) print ( results . best_estimator_ . get_params ()) best_rf = results . best_estimator_ # Calculate AUC/ROC y_proba = best_rf . predict_proba ( X_test )[:, 1 ] auc = np . round ( roc_auc_score ( y_test , y_proba ), 2 ) print ( f 'GridSearchCV RF AUC on test set: { auc } ' )","tags":"labs","url":"labs/lecture-24/notebook/"},{"title":"Lecture 24: Tuning Hyperparameters","text":"In [1]: #!pip install -qq dtreeviz import os , sys sys . path . append ( f \" { os . getcwd () } /../\" ) In [2]: # Import the main packages import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import BaggingClassifier from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from dtreeviz.trees import dtreeviz % matplotlib inline colors = [ None , # 0 classes None , # 1 class [ '#FFF4E5' , '#D2E3EF' ], # 2 classes ] from IPython.display import Markdown , display def printmd ( string ): display ( Markdown ( string )) In [3]: # Read the dataset and take a quick look df = pd . read_csv ( \"diabetes.csv\" ) df . head () In [4]: ### edTest(test_assign) ### # Assign the predictor and response variables. # \"Outcome\" is the response and all the other columns are the predictors X = __ y = __ In [5]: # Fix a random_state and split the data # into train and validation sets random_state = 144 X_train , X_val , y_train , y_val = train_test_split ( __ , __ , train_size = 0.8 , random_state = random_state ) Bagging Implementation In [6]: # Define a Bagging classifier with randon_state as above # and with a DecisionClassifier as a basemodel # We fix the max_depth variable to 20 for all trees max_depth = 20 # Set the maximum depth to be max_depth and use 100 estimators n_estimators = 1000 basemodel = __ ( max_depth = __ , random_state = __ ) bagging = BaggingClassifier ( base_estimator = basemodel , n_estimators = n_estimators ) # Fit the model on the training set bagging . fit ( __ , __ ) In [7]: ### edTest(test_bagging) ### # We make predictions on the validation set predictions = bagging . predict ( X_val ) # compute the accuracy on the validation set acc_bag = round ( accuracy_score ( predictions , y_val ), 2 ) print ( f 'For Bagging, the accuracy on the validation set is { acc_bag } ' ) Random Forest implementation In [8]: # Define a Random Forest classifier with randon_state as above # Set the maximum depth to be max_depth and use 100 estimators random_forest = __ ( max_depth = max_depth , random_state = random_state , n_estimators = n_estimators ) # Fit the model on the training set random_forest . fit ( __ , __ ) In [9]: ### edTest(test_RF) ### # We make predictions on the validation set predictions = random_forest . predict ( X_val ) # compute the accuracy on the validation set acc_rf = round ( accuracy_score ( predictions , y_val ), 2 ) print ( f 'For Random Forest, the accuracy on the validation set is { acc_rf } ' ) Visualizing the trees - Bagging In [10]: # Reducing the max_depth for visualization max_depth = 3 basemodel = DecisionTreeClassifier ( max_depth = max_depth , random_state = random_state ) bagging = BaggingClassifier ( base_estimator = basemodel , n_estimators = 1000 ) # Fit the model on the training set bagging . fit ( X_train , y_train ) # Selecting two trees at random bagvati1 = bagging . estimators_ [ 0 ] bagvati2 = bagging . estimators_ [ 100 ] In [11]: vizA = dtreeviz ( bagvati1 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , ) printmd ( ' Bagging Tree 1 ' ) vizA In [12]: vizB = dtreeviz ( bagvati2 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , scale = 1.1 ) printmd ( ' Bagging Tree 2 ' ) vizB Visualizing the trees - Random Forest In [13]: # Reducing the max_depth for visualization max_depth = 3 random_forest = RandomForestClassifier ( max_depth = max_depth , random_state = random_state , n_estimators = 1000 , max_features = \"sqrt\" ) # Fit the model on the training set random_forest . fit ( X_train , y_train ) # Selecting two trees at random forestvati1 = random_forest . estimators_ [ 0 ] forestvati2 = random_forest . estimators_ [ __ ] In [14]: vizC = dtreeviz ( forestvati1 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , scale = 1.1 ) printmd ( ' Random Forest Tree 1 ' ) vizC In [15]: vizD = dtreeviz ( forestvati2 , df . iloc [:,: 8 ], df . Outcome , feature_names = df . columns [: 8 ], target_name = 'Diabetes' , class_names = [ 'No' , 'Yes' ] , orientation = 'TD' , colors = { 'classes' : colors }, label_fontsize = 14 , ticks_fontsize = 10 , scale = 1.1 ) printmd ( ' Random Forest Tree 2 ' ) vizD Mindchow 🍲 Change the max_depth of Bagging and Random Forest to see different trees. Which one gives different trees? Change the max_features in RandomForestClassifier to 8. How is it affecting the correlation between the trees?","tags":"labs","url":"labs/lecture-24/notebook-2/"},{"title":"Lecture 24: Tuning Hyperparameters","text":"Slides PDF | Lecture 24: Tuning Hyperparameters PPTX | Lecture 24: Tuning Hyperparameters Exercises Lecture 24: Bagging vs Random Forest (Tree correlation) [Notebook] Lecture 24: Hyperparameter tuning [Notebook]","tags":"lectures","url":"lectures/lecture24/"},{"title":"S-Section 07: Bagging and Random Forest","text":"CS109A Introduction to Data Science Standard Section 7: Bagging and Random Forest Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Hayden Joy In [ ]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) This section will work with a spam email dataset. Our ultimate goal is to be able to build models so that we can predict whether an email is spam or not spam based on word characteristics within each email. We will cover Decision Trees, Bagging, and Random Forest methods and allow you to apply it to the homework. Specifically, we will: 1. Load in the spam dataset and split the data into train and test. 2. Find the optimal depth for the Decision Tree model and evaluate performance. 3. Fit the Bagging model using multiple bootstrapped datasets and do majority voting. 4. Fit the Random Forest Model and compare with Bagging. Hopefully after this section you will be able to answer the following questions: What are decision tree models? How do we construct them? How do we visualize them? What is bagging? Why does bagging help with overfitting? Why does bagging help to built more expressive trees? The Idea: Decision Trees are just flowcharts and interpretable! It turns out that simple flow charts can be formulated as mathematical models for classification and these models have the properties we desire; interpretable by humans have sufficiently complex decision boundaries the decision boundaries are locally linear, each component of the decision boundary is simple to describe mathematically. Let's review some theory: How to build Decision Trees (the Learning Algorithm in words): To learn a decision tree model, we take a greedy approach: Start with an empty decision tree (undivided feature space) Choose the ‘optimal' predictor on which to split and choose the ‘optimal' threshold value for splitting by applying a splitting criterion (1) Recurse on on each new node until stopping condition (2) is met For classification, we label each region in the model with the label of the class to which the majority of the points within the region belong. So we need a (1) splitting criterion and a (2) stopping condition: (1) Splitting criterion (2) Stopping condition If we don't terminate the decision tree learning algorithm manually, the tree will continue to grow until each region defined by the model possibly contains exactly one training point (and the model attains 100% training accuracy). Not stopping while building a deeper and deeper tree = 100% training accuracy; What will your test accuracy be? What can we do to fix this? To prevent the overfitting from happening, we could Stop the algorithm at a particular depth. (= not too deep ) Don't split a region if all instances in the region belong to the same class. (= stop when subtree is pure ) Don't split a region if the number of instances in the sub-region will fall below pre-defined threshold (min_samples_leaf). (= not too specific/small subtree ) Don't use to many splits in the tree (= not too many splits / not too complex global tree ) Be content with <100% accuracy training set... Done with theory, let's get started In [ ]: import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns import sklearn.metrics as metrics from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn import tree from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.linear_model import LogisticRegressionCV from sklearn.model_selection import KFold from sklearn.metrics import confusion_matrix from sklearn import datasets % matplotlib inline pd . set_option ( 'display.width' , 1500 ) pd . set_option ( 'display.max_columns' , 100 ) from sklearn.model_selection import learning_curve Part 1 : Introduction to the Spam Dataset We will be working with a spam email dataset. The dataset has 57 predictors with a response variable called Spam that indicates whether an email is spam or not spam. The goal is to be able to create a classifier or method that acts as a spam filter. In [ ]: #Import Dataframe and Set Column Names spam_df = pd . read_csv ( 'data/spam.csv' , header = None ) columns = [ \"Column_\" + str ( i + 1 ) for i in range ( spam_df . shape [ 1 ] - 1 )] + [ 'Spam' ] spam_df . columns = columns display ( spam_df . head ()) The predictor variabes are all continuous. They represent certain features like the frequency of the word \" discount \". The exact specification and description of each predictor can be found online. We are not so much interested in the exact inference of each predictor so we will omit the exact names of each of the predictors. We are more interested in the prediction of the algorithm so we will treat each as predictor without going into too much exact detail in each. Link to description : https://archive.ics.uci.edu/ml/datasets/spambase Let us split the dataset into a 70-30 split by using the following: Note : While you will use train_test_split in your homeworks, the code below should help you visualize splitting/masking of a dataframe which will be helpful in general. In [ ]: #Split data into train and test np . random . seed ( 42 ) msk = np . random . rand ( len ( spam_df )) < 0.7 data_train = spam_df [ msk ] data_test = spam_df [ ~ msk ] #Split predictor and response columns x_train , y_train = data_train . drop ([ 'Spam' ], axis = 1 ), data_train [ 'Spam' ] x_test , y_test = data_test . drop ([ 'Spam' ] , axis = 1 ), data_test [ 'Spam' ] print ( \"Shape of Training Set :\" , data_train . shape ) print ( \"Shape of Testing Set :\" , data_test . shape ) In [ ]: spam_df . iloc [ np . arange ( 10 )] We can check that the number of spam cases is roughly evenly represented in both the training and test set. In [ ]: #Check Percentage of Spam in Train and Test Set percentage_spam_training = 100 * y_train . sum () / len ( y_train ) percentage_spam_testing = 100 * y_test . sum () / len ( y_test ) print ( \"Percentage of Spam in Training Set \\t : {:0.2f} %.\" . format ( percentage_spam_training )) print ( \"Percentage of Spam in Testing Set \\t : {:0.2f} %.\" . format ( percentage_spam_testing )) Part 2 : Fitting an Optimal Single Decision Tree (by Depth) : We fit here a single tree to our spam dataset and perform 5-fold cross validation on the training set. For EACH depth of the tree, we fit a tree and then compute the 5-fold CV scores. These scores are then averaged and compared across different depths. In [ ]: #Find optimal depth of trees mean_CV_acc = {} all_CV_acc = {} tree_depth_start , tree_depth_end , steps = 3 , 31 , 4 for i in range ( tree_depth_start , tree_depth_end + 1 , steps ): model = DecisionTreeClassifier ( max_depth = i ) score = cross_val_score ( estimator = model , X = x_train , y = y_train , cv = 5 , n_jobs =- 1 ) all_CV_acc [ i ] = score mean_CV_acc [ i ] = score . mean () In [ ]: mean_CV_acc Some dictionary manipulations for our x,y construction for the plot below: In [ ]: x = list ( mean_CV_acc . keys ()) y = list ( mean_CV_acc . values ()) x , y In [ ]: lists = sorted ( mean_CV_acc . items ()) x , y = zip ( * lists ) In [ ]: #Plot plt . ylabel ( \"Cross Validation Accuracy\" ) plt . xlabel ( \"Maximum Depth\" ) plt . title ( 'Variation of Accuracy with Depth - Simple Decision Tree' ) plt . plot ( x , y , 'b-' , marker = 'o' ) plt . show () As we can see, the optimal depth is found to be a depth of 7. Although, does it makes sense to choose 6? Also, if we wanted to get the Confidence Bands of these results, how would we? It's as simple as a combination of getting variance using scores.std() and plt.fill_between() . In [ ]: stds = np . array ([ np . std ( score ) for score in all_CV_acc . values () ]) stds In [ ]: plt . fill_between ( x , y + stds , y - stds , alpha = 0.2 ) #Plot plt . ylabel ( \"Cross Validation Accuracy\" ) plt . xlabel ( \"Maximum Depth\" ) plt . title ( 'Variation of Accuracy with Depth - Simple Decision Tree' ) plt . plot ( x , y , 'b-' , marker = 'o' ) plt . show () If we want to display it as a boxplot we first construct a dataframe with all the scores and second we use sns.boxplot(...) In [ ]: # Making a numpy array with all the CV acc scores scores_numpy = np . array ( list ( all_CV_acc . values ())) # Making a datafr trees = pd . DataFrame ({ 'Max Depth' : x + x + x + x + x , 'CV Accuracy Score' : list ( scores_numpy [:, 0 ]) + list ( scores_numpy [:, 1 ]) + list ( scores_numpy [:, 2 ]) + list ( scores_numpy [:, 3 ]) + list ( scores_numpy [:, 4 ])}) trees . head () In [ ]: # plotting the boxplot plt . figure ( figsize = ( 12 , 3 )) plt . title ( 'Variation of Accuracy with Depth - Simple Decision Tree' ) sns . boxplot ( x = \"Max Depth\" , y = \"CV Accuracy Score\" , data = trees ); In [ ]: # plotting the boxplot without outliers (showfliers = False) plt . figure ( figsize = ( 12 , 3 )) plt . title ( 'Variation of Accuracy with Depth - Simple Decision Tree' ) sns . boxplot ( x = \"Max Depth\" , y = \"CV Accuracy Score\" , data = trees , showfliers = False ); Let's extract the best_depth value from this dictionary: We create the new variable best_depth . Can you see why we coded the best depth parameter as we did below? (Hint: Think about reproducibility.) How to sort using your own function with key parameter? If you want your own implementation for sorting, sorted() also accepts a key function as an optional parameter. Based on the results of the key function, you can sort the given iterable. sorted(iterable, key=len) In [ ]: mean_CV_acc In [ ]: # What does this do? Is this the result we want? sorted ( mean_CV_acc , reverse = False ) In [ ]: # What does this do? sorted ( mean_CV_acc , key = mean_CV_acc . get , reverse = True ) In [ ]: #Make best depth a variable best_depth = sorted ( mean_CV_acc , key = mean_CV_acc . get , reverse = True )[ 0 ] print ( \"The best depth was found to be:\" , best_depth ) In [ ]: #Evalaute the performance at the best depth model_tree = DecisionTreeClassifier ( max_depth = best_depth ) model_tree . fit ( x_train , y_train ) #Check Accuracy of Spam Detection in Train and Test Set acc_trees_training = accuracy_score ( y_train , model_tree . predict ( x_train )) acc_trees_testing = accuracy_score ( y_test , model_tree . predict ( x_test )) print ( \"Simple Decision Trees: Accuracy, Training Set \\t : {:.2%} \" . format ( acc_trees_training )) print ( \"Simple Decision Trees: Accuracy, Testing Set \\t : {:.2%} \" . format ( acc_trees_testing )) In [ ]: #Get Performance by Class (Lookup Confusion Matrix) pd . crosstab ( y_test , model_tree . predict ( x_test ), margins = True , rownames = [ 'Actual' ], colnames = [ 'Predicted' ]) How to visualize a Decision Tree with pydot Question: Do you think this tree is interpretable? What do you think about a the maximal depth of the tree? Let's first store the decision tree in a text format: decision_tree.dot In [ ]: from sklearn import tree file_name = \"results/decision_tree.dot\" tree . export_graphviz ( model_tree , out_file = file_name ) Let's look at the resulting text decision_tree.dot In [ ]: ! head results/decision_tree.dot Let's convert our (hard to read) written decision tree ( decision_tree.dot ) into an intuitive image file format: image_tree.png **NOTE:** You might need to install the pydot package by typing the following command in your terminal: pip install pydot or you can install from within the jupyter notebook by running the following cell: ! pip install pydot In [ ]: ! pip install pydot ! conda install graphiv -y In [ ]: import pydot ( graph ,) = pydot . graph_from_dot_file ( file_name ) graph . write_png ( 'results/image_tree.png' ) Let's display the image_tree.png in markdown: Markdown: ![title](results/image_tree.png) . The result: Repost Question: Do you think this tree is interpretable? Break Out Room 1 *Let's try out decision tree models on a cancer dataset. Click here for another example with this dataset and more information. This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets. Features: radius (mean of distances from center to points on the perimeter) texture (standard deviation of gray-scale values) perimeter area smoothness (local variation in radius lengths) compactness (perimeter&#94;2 / area - 1.0) concavity (severity of concave portions of the contour) concave points (number of concave portions of the contour) symmetry fractal dimension (\"coastline approximation\" - 1) Target: 2) Diagnosis (M = malignant, B = benign) In [ ]: from functions import tree_pd import numpy as np import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns import sklearn.metrics as metrics from sklearn.model_selection import cross_val_score from sklearn.metrics import accuracy_score from sklearn import tree from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.linear_model import LogisticRegressionCV from sklearn.model_selection import KFold from sklearn.metrics import confusion_matrix from sklearn import datasets % matplotlib inline pd . set_option ( 'display.width' , 1500 ) pd . set_option ( 'display.max_columns' , 100 ) from sklearn.model_selection import learning_curve get_tree_pd = tree_pd . get_tree_pd cancer_scaled , target = tree_pd . load_cancer_dataset ( 10 , 4 ) In [ ]: ################################### Train Test split np . random . seed ( 40 ) #test_proportion test_prop = 0.2 msk = np . random . uniform ( 0 , 1 , len ( cancer_scaled )) > test_prop #Split predictor and response columns x_train , y_train = cancer_scaled [ msk ], target [ msk ] x_test , y_test = cancer_scaled [ ~ msk ], target [ ~ msk ] print ( \"Shape of Training Set :\" , x_train . shape ) print ( \"Shape of Testing Set :\" , x_test . shape ) Your tasks are as follows: 1) Use the get_tree_pd function to assign a dataframe of cross-validation scores for different depths of a DecisionTreeClassifier . Specifically feed the function a class instance with random_state=42. This function takes four arguments (x_train, y_train, model, tree_depth_range). 2) Visualize the mean cross validation accuracy scores using sns.boxenplot or another function of your choice similar to sns.catplot 3) Use pandas groupby function to to get the mean cross-validation accuracy for specific depths. Assign to a new dataframe cv_acc_mean . 4) Visualize the mean cross validation accuracy scores using sns.lineplot in combination with cv_acc_mean . Discuss what you see with your group. In [ ]: #Your code here In [ ]: # %load '../solutions/breakout1.py' once you have successfully completed the code above, this code should run without a problem. In [ ]: max_idx = cv_acc_mean [ \"cv_acc_score\" ] . idxmax () best_depth = cv_acc_mean [ \"depth\" ][ max_idx ] print ( \"best depth {:} \" . format ( best_depth )) model = DecisionTreeClassifier ( max_depth = best_depth ) model . fit ( x_train , y_train ) cancer_tree_accuracy = model . score ( x_test , y_test ) print ( \"Testing set accuracy {:.4f} \" . format ( cancer_tree_accuracy )) Now answer the following questions: $\\bullet$ Why is the best depth the value that it is? $\\bullet$ Why might the deeper trees be over or under fitting on this particular dataset? In [ ]: # %load '../solutions/breakout1.py' Try feeding the get_tree_pd an additional \"bootstraps\" argument and comment on what happens to the validation accuracy. In [ ]: #TODO tree . export_graphviz ( model , \"example_tree.dot\" ) import pydot ( graph ,) = pydot . graph_from_dot_file ( \"example_tree.dot\" ) graph . write_png ( 'results/small_tree.png' ) Let's display the image_tree.png in markdown: Markdown: ![title](results/small_tree.png) . The result: Repost Question: Do you think this tree is interpretable? Part 3: Bagging and Voting QUESTION: Where does the word \"Bagging\" come from? Some Theory: What is bagging? Bootstrapping: resample with replacements to get different datasets and built different models. Do something smart to combine the different models. One way to adjust for the high variance of the output of an experiment is to perform the experiment multiple times and then average the results. Bootstrap: we generate multiple samples of training data, via bootstrapping. We train a full decision tree on each sample of data. AGgregatiING for a given input, we output the averaged outputs of all the models for that input. This method is called Bagging: B ootstrap + AGG regat ING . Let's bootstrap our training dataset to create multiple datasets and fit Decision Tree models to each. (Resampling: we showed live that different samples give different results for things like sums, varying more when the things we sum over have high variance themselves.) In [ ]: # Stat on all data data_train . mean ( axis = 0 ) . to_frame ( 'mean' ) . T In [ ]: x_train , y_train = data_train . drop ([ 'Spam' ], axis = 1 ), data_train [ 'Spam' ] x_test , y_test = data_test . drop ([ 'Spam' ] , axis = 1 ), data_test [ 'Spam' ] In [ ]: data_train . sample ( frac = 1. , replace = True ) . mean ( axis = 0 ) . to_frame ( 'mean' ) . T Now we actually fit the samples In [ ]: n_trees = 100 # we tried a variety of numbers here choosen_depth = 5 In [ ]: #Creating model np . random . seed ( 0 ) model = DecisionTreeClassifier ( max_depth = choosen_depth ) #Initializing variables predictions_train = np . zeros (( data_train . shape [ 0 ], n_trees )) predictions_test = np . zeros (( data_test . shape [ 0 ], n_trees )) #Conduct bootstraping iterations for i in range ( n_trees ): temp = data_train . sample ( frac = 1 , replace = True ) response_variable = temp [ 'Spam' ] temp = temp . drop ([ 'Spam' ], axis = 1 ) model . fit ( temp , response_variable ) predictions_train [:, i ] = model . predict ( x_train ) predictions_test [:, i ] = model . predict ( x_test ) #Make Predictions Dataframe columns = [ \"Bootstrap-Model_\" + str ( i + 1 ) for i in range ( n_trees )] predictions_train = pd . DataFrame ( predictions_train , columns = columns ) predictions_test = pd . DataFrame ( predictions_test , columns = columns ) In [ ]: y_train = data_train [ 'Spam' ] . values y_test = data_test [ 'Spam' ] . values In [ ]: ## Example Bolean for locating the Non Spam y == 0 ## Example Bolean for locating the Spam y == 1 In [ ]: num_to_avg = 100 fig , axs = plt . subplots ( 1 , 2 , figsize = ( 16 , 7 )) for ( ax , label , predictions , y ) in [ ( axs [ 0 ], 'Training Set' , predictions_train , y_train ), ( axs [ 1 ], 'Test Set' , predictions_test , y_test ) ]: # Take the average mean_predictions = predictions . iloc [:,: num_to_avg ] . mean ( axis = 1 ) # Plot the Spam mean_predictions [ y == 1 ] . hist ( density = True , histtype = 'step' , range = [ 0 , 1 ], label = 'Spam' , lw = 2 , ax = ax ) # Plot the non Spam mean_predictions [ y == 0 ] . hist ( density = True , histtype = 'step' , range = [ 0 , 1 ], label = 'Not-Spam' , lw = 2 , ax = ax ) ax . legend ( loc = 'upper center' ); ax . set_xlabel ( \"Mean of ensemble predictions (0.5=50 True - 50 False)\" ) ax . set_title ( label ) And now get final predictions: majority voting! In [ ]: #Function to ensemble the prediction of each bagged decision tree model def get_prediction ( df , count =- 1 ): count = df . shape [ 1 ] if count ==- 1 else count temp = df . iloc [:, 0 : count ] return np . mean ( temp , axis = 1 ) > 0.5 #Check Accuracy of Spam Detection in Train and Test Set acc_bagging_training = 100 * accuracy_score ( y_train , get_prediction ( predictions_train , count =- 1 )) acc_bagging_testing = 100 * accuracy_score ( y_test , get_prediction ( predictions_test , count =- 1 )) print ( \"Bagging: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_bagging_training )) print ( \"Bagging: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_bagging_testing )) Count in the above code can be use to define the number of models the voting in the dataframe should be based on. In [ ]: #Get Performance by Class (Lookup Confusion Matrix) pd . crosstab ( np . array ( y_test ), model . predict ( x_test ), margins = True , rownames = [ 'Actual' ], colnames = [ 'Predicted' ]) Food for Thought : Are these bagging models independent of each other, can they be trained in a parallel fashion? Part 4 : Random Forest vs Bagging What is Random Forest? Many trees make a forest . Many random trees make a random forest . Random Forest is a modified form of bagging that creates ensembles of independent decision trees. To de-correlate the trees , we: train each tree on a separate bootstrap random sample of the full training set (same as in bagging) for each tree, at each split, we randomly select a set of 𝐽′ predictors from the full set of predictors. (not done in bagging) From amongst the 𝐽′ predictors, we select the optimal predictor and the optimal corresponding threshold for the split. Question: Why would this second step help (only considering random sub-group of features)? Now, we will fit an ensemble method, the Random Forest technique, which is different from the decision tree. Refer to the lectures slides for a full treatment on how they are different. Let's use n_estimators = predictor_count/2 and max_depth = best_depth . In [ ]: #Fit a Random Forest Model best_depth = 7 #Training model = RandomForestClassifier ( n_estimators = int ( x_train . shape [ 1 ] / 2 ), max_depth = best_depth ) model . fit ( x_train , y_train ) #Predict y_pred_train = model . predict ( x_train ) y_pred_test = model . predict ( x_test ) #Perfromance Evaluation acc_random_forest_training = accuracy_score ( y_train , y_pred_train ) * 100 acc_random_forest_testing = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Random Forest: Accuracy, Training Set : {:0.2f} %\" . format ( acc_random_forest_training )) print ( \"Random Forest: Accuracy, Testing Set : {:0.2f} %\" . format ( acc_random_forest_testing )) Let's compare the performance of our 3 models: In [ ]: print ( \"Decision Trees: \\t Accuracy, Training Set \\t : {:.2%} \" . format ( acc_trees_training )) print ( \"Decision Trees: \\t Accuracy, Testing Set \\t : {:.2%} \" . format ( acc_trees_testing )) print ( \" \\n Bagging: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_bagging_training )) print ( \"Bagging: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_bagging_testing )) print ( \" \\n Random Forest: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_random_forest_training )) print ( \"Random Forest: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_random_forest_testing )) As we see above, the performance of both Bagging and Random Forest was similar, so what is the difference? Do both overfit the data just as much? Hints : What is the only extra parameter we declared when defining a Random Forest Model vs Bagging? Does it have an impact on overfitting? In [ ]: #Fit a Random Forest Model new_depth = best_depth + 20 #Training model = RandomForestClassifier ( n_estimators = int ( x_train . shape [ 1 ] / 2 ), max_depth = new_depth ) model . fit ( x_train , y_train ) #Predict y_pred_train = model . predict ( x_train ) y_pred_test = model . predict ( x_test ) #Perfromance Evaluation acc_random_forest_deeper_training = accuracy_score ( y_train , y_pred_train ) * 100 acc_random_forest_deeper_testing = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Random Forest: Accuracy, Training Set (Deeper): {:0.2f} %\" . format ( acc_random_forest_deeper_training )) print ( \"Random Forest: Accuracy, Testing Set (Deeper): {:0.2f} %\" . format ( acc_random_forest_deeper_testing )) Training accuracies: In [ ]: print ( \"Training Accuracies:\" ) print ( \"Decision Trees: \\t Accuracy, Training Set \\t : {:.2%} \" . format ( acc_trees_training )) print ( \"Bagging: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_bagging_training )) print ( \"Random Forest: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_random_forest_training )) print ( \"RF Deeper: \\t Accuracy, Training Set \\t : {:0.2f} %\" . format ( acc_random_forest_deeper_training )) Testing accuracies: In [ ]: print ( \"Testing Accuracies:\" ) print ( \"Decision Trees: \\t Accuracy, Testing Set \\t : {:.2%} \" . format ( acc_trees_testing )) print ( \"Bagging: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_bagging_testing )) print ( \"Random Forest: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_random_forest_testing )) print ( \"RF Deeper: \\t Accuracy, Testing Set \\t : {:0.2f} %\" . format ( acc_random_forest_deeper_testing )) Feature Importance Random Forest gives the above values as feature_importance where it normalizes the impact of a predictor to the number of times it is useful and thus gives overvall significance for free. Explore the attributes of the Random Forest model object for the best nodes. In [ ]: #Top Features feature_importance = model . feature_importances_ feature_importance = 100.0 * ( feature_importance / feature_importance . max ()) sorted_idx = np . argsort ( feature_importance ) pos = np . arange ( sorted_idx . shape [ 0 ]) + . 5 #Plot plt . figure ( figsize = ( 10 , 12 )) plt . barh ( pos , feature_importance [ sorted_idx ], align = 'center' ) plt . yticks ( pos , x_train . columns [ sorted_idx ]) plt . xlabel ( 'Relative Importance' ) plt . title ( 'Variable Importance' ) plt . show () Our Initial Questions: What are decision tree models? How do we construct them? How do we vizualize them? What is the idea of bagging? Why does bagging help with overfitting? Why does bagging help to built more expressive trees? End of Standard Section. What about next week? Gradient Boosting etc.. building upon decision trees. Why should we care? Source Medion: \"XGBoost Algorithm: Long May She Reign!\" Break Out Room 2: Random Forests *Let's try to improve our accuracy scores on the cancer dataset. In [ ]: ################################### Train Test split np . random . seed ( 40 ) #test_proportion test_prop = 0.2 msk = np . random . uniform ( 0 , 1 , len ( cancer_scaled )) > test_prop #Split predictor and response columns x_train , y_train = cancer_scaled [ msk ], target [ msk ] x_test , y_test = cancer_scaled [ ~ msk ], target [ ~ msk ] print ( \"Shape of Training Set :\" , x_train . shape ) print ( \"Shape of Testing Set :\" , x_test . shape ) Your tasks: 1) Use the get_tree_pd function to assign a dataframe rf_val_acc using a class instance of RandomForestClassifier . As a reminder this function takes four arguments (x_train, y_train, model, tree_depth_range). This time don't feed a random state. 2) Use pandas groupby function to to get the mean cross-validation accuracy for specific depths. Assign to a new dataframe rf_mean_acc . 3) Visualize the mean cross validation accuracy scores by running the cell provided. Answer the subsequent questions. 4) Plot the feature importance of the best random forest model. In [ ]: #Your code here In [ ]: # %load '../solutions/breakout2.py' Run this code when you are finished with the first exercise to compare random forests and simple decision trees. More questions and one final task lie below. In [ ]: ### add a decision tree classifier for comparison. tree_val_acc = get_tree_pd ( x_train , y_train , DecisionTreeClassifier (), tree_depth_range ) tree_mean_acc = tree_val_acc . groupby ( \"depth\" ) . mean () tree_mean_acc [ \"depth\" ] = list ( tree_depth_range ) ### Make the plot plt . figure ( figsize = ( 12 , 3 )) plt . title ( 'Variation of Accuracy on Validation set with Depth - Simple Decision Tree' ) sns . lineplot ( x = \"depth\" , y = \"cv_acc_score\" , data = rf_val_acc , label = \"random forest\" ); sns . lineplot ( x = \"depth\" , y = \"cv_acc_score\" , data = tree_val_acc , label = \"simple decision tree\" ); plt . xlabel ( \"max_depth\" ) plt . ylabel ( \"validation set accuracy score\" ) max_idx = tree_mean_acc [ \"cv_acc_score\" ] . idxmax () best_depth_tree = tree_mean_acc [ \"depth\" ][ max_idx ] best_tree_model = DecisionTreeClassifier ( max_depth = best_depth ) best_tree_model . fit ( x_train , y_train ) tree_test_accuracy = best_tree_model . score ( x_test , y_test . reshape ( - 1 ,)) max_idx = rf_mean_acc [ \"cv_acc_score\" ] . idxmax () best_depth_rf = rf_mean_acc [ \"depth\" ][ max_idx ] best_rf_model = RandomForestClassifier ( max_depth = best_depth_rf , random_state = 42 ) best_rf_model . fit ( x_train , y_train . reshape ( - 1 ,)) tree_rf_accuracy = best_rf_model . score ( x_test , y_test . reshape ( - 1 ,)) print ( \"Decision Tree max depth {:} \" . format ( best_depth )) print ( \"Random Forest max depth {:} \" . format ( best_depth_rf )) print ( \"Best Decision Tree test set accuracy: {:0.2f} %\" . format ( tree_test_accuracy * 100 )) print ( \"Best Random Forest test set accuracy: {:0.2f} %\" . format ( tree_rf_accuracy * 100 )) $\\bullet$ Why doesn't the random forest accuracy score deteriorate in the same way that the decision tree does for deeper trees? $\\bullet$ What are the two kinds of stochasticity that lead to the robustness of random forests? $\\bullet$ How do random forests differ from Bagging? Now plot the feature importance of your best random forest model: In [ ]: #Your code here In [ ]: # %load '../solutions/importance.py'","tags":"sections","url":"sections/sec_7/"},{"title":"S-Section 07: Bagging and Random Forest","text":"Jupyter Notebooks S-Section 7: Bagging and Random Forest","tags":"sections","url":"sections/section7/"},{"title":"Lecture 23: Regression Trees, Bagging, and RF","text":"Bagging Classification In [2]: # Import required libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.tree import DecisionTreeClassifier from sklearn.model_selection import train_test_split from sklearn import metrics import scipy.optimize as opt from sklearn.metrics import accuracy_score # to be used for plotting later from matplotlib.colors import ListedColormap cmap_light = ListedColormap ([ '#FFF4E5' , '#D2E3EF' ]) cmap_bold = ListedColormap ([ '#F7345E' , '#80C3BD' ]) In [7]: # Read the file 'agriland.csv' and take a quick look at your data df = pd . read_csv ( 'agriland.csv' ) # Note that the latitude & longitude values are normalized df . head () Out[7]: latitude longitude land_type 0 -0.071860 -1.297410 1.0 1 -0.179482 -0.874892 1.0 2 -1.217428 -1.352105 0.0 3 1.143306 -0.894172 1.0 4 -3.033199 0.818646 0.0 In [0]: # Set your predictor variables(latitude &longitude) as 'X' and response variable as y and make sure to use .values X = ___ y = ___ In [0]: #split data in train an test, with test size = 0.2 and randomstate=44 X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.2 , random_state = 44 ) In [0]: # Define the max_depth of your decision tree and set the random_state variable as 44 max_depth = ___ # Lets create and train our model clf = DecisionTreeClassifier ( max_depth = max_depth , random_state = 44 ) clf . fit ( X_train , y_train ) In [0]: # Predict on the test set and calculate the accuracy of a single decision tree prediction = ___ single_acc = ___ print ( f 'Single tree Accuracy is { single_acc * 100 } %' ) In [0]: # Complete the function below to get the prediction by bagging # Inputs: X_train, y_train to train your data # X_to_evaluate: Samples that you are goin to predict (evaluate) # num_bootstraps: how many trees you want to train # Output: An array of predicted classes for X_to_evaluate def prediction_by_bagging ( X_train , y_train , X_to_evaluate , num_bootstraps ): # list to store every array of predictions predictions = [] #generate num_bootstraps number of trees for i in range ( num_bootstraps ): # sample data to perform first bootstrap, here, we actually bootstrap indices, because we want the same subset for X_train and y_train resample_indexes = np . random . choice ( np . arange ( y_train . shape [ 0 ]), size = y_train . shape [ 0 ]) # get bootstrapped set for 'X' and 'y' using the above indices X_boot = X_train [ ___ ] y_boot = y_train [ ___ ] # train decision tree on bootstrap set, use the same max_depth and random_state as above clf = ___ # fit the model on bootstrapped training set clf . fit ( ___ , ___ ) # make predictions on X_to_evaluate samples pred = clf . predict ( ___ ) predictions . append ( pred ) # Now we have a list of predictions like [prediction_array_0, prediction_array_1, ..., prediction_array_n] # To get the majority vote for each sample, we can find the average prediction and threshold them by 0.5 average_prediction = ___ return average_prediction In [0]: ### edTest(test_bag_acc) ### # now we print the accuracy of the bagging with decision trees #Define the number of bootstraps to be used num_bootstraps = 200 y_pred = prediction_by_bagging ( X_train , y_train , X_test , num_bootstraps = num_bootstraps ) # Compare the average predictions to the true test set values and compute the accuracy bagging_accuracy = ___ print ( f 'Accuracy with Bootstrapped Aggregation is { bagging_accuracy * 100 } %' ) In [0]: # To visualize, lets plot accuracy as a function of the number of trees in the Bagging # Run the helper code below, and if your function is well defined above, you should see a plot of accuracy vs number of bagged trees n = np . linspace ( 1 , 250 , 250 ) . astype ( int ) acc = [] for n_i in n : acc . append ( np . mean ( prediction_by_bagging ( X_train , y_train , X_test , n_i ) == y_test )) plt . figure ( figsize = ( 10 , 8 )) plt . plot ( n , acc , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'Model Prediction' ) plt . title ( 'Accuracy vs. Number of trees in Bagging ' , fontsize = 24 ) plt . xlabel ( 'Number of trees' , fontsize = 16 ) plt . ylabel ( 'Accuracy' , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show () Bagging Visualization Bagging does well to reduce overfitting, but only upto a certain extent. Vary the max_depth and numboot variables to see how Bagging helps reduce overfitting with the help of the visualization below In [0]: # We will make plots for three different values of `max_depth` fig , axes = plt . subplots ( 1 , 3 , figsize = ( 20 , 6 )) # Make a list of three max_depths to investigate max_depth = [ 2 , 5 , 100 ] # Fix the number of bootstraps numboot = 100 for index , ax in enumerate ( axes ): for i in range ( numboot ): df_new = df . sample ( frac = 1 , replace = True ) y = df_new . land_type . values X = df_new [[ 'latitude' , 'longitude' ]] . values dtree = DecisionTreeClassifier ( max_depth = max_depth [ index ]) dtree . fit ( X , y ) ax . scatter ( X [:, 0 ], X [:, 1 ], c = y - 1 , s = 50 , alpha = 0.5 , edgecolor = \"k\" , cmap = cmap_bold ) plot_step_x1 = 0.1 plot_step_x2 = 0.1 x1min , x1max = X [:, 0 ] . min (), X [:, 0 ] . max () x2min , x2max = X [:, 1 ] . min (), X [:, 1 ] . max () x1 , x2 = np . meshgrid ( np . arange ( x1min , x1max , plot_step_x1 ), np . arange ( x2min , x2max , plot_step_x2 ) ) # Re-cast every coordinate in the meshgrid as a 2D point Xplot = np . c_ [ x1 . ravel (), x2 . ravel ()] # Predict the class y = dtree . predict ( Xplot ) y = y . reshape ( x1 . shape ) cs = ax . contourf ( x1 , x2 , y , alpha = 0.02 ) ax . set_xlabel ( 'Latitude' , fontsize = 14 ) ax . set_ylabel ( 'Longitude' , fontsize = 14 ) ax . set_title ( f 'Max depth = { max_depth [ index ] } ' , fontsize = 20 ) Mindchow 🍲 Play around with the following parameters: max_depth numboot Based on your observations, answer the questions below: How does the plot change with varying max_depth How does the plot change with varying numboot How are the three plots essentially different? Does more bootstraps reduce overfitting for High depth Low depth","tags":"labs","url":"labs/lecture-23/notebook/"},{"title":"Lecture 23: Regression Trees, Bagging, and RF","text":"In [2]: # Import necessary libraries import numpy as np from numpy import mean from numpy import std from sklearn.datasets import make_regression from sklearn.ensemble import BaggingRegressor import matplotlib.pyplot as plt import pandas as pd import itertools from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split % matplotlib inline In [3]: # Read the dataset df = pd . read_csv ( \"airquality.csv\" , index_col = 0 ) In [10]: # Take a quick look at the data df . head ( 10 ) In [12]: # We will only use Ozone for this exerice. Drop any notnas df = df [ df . Ozone . notna ()] In [17]: # Assign \"x\" column as the predictor variable, only use Ozone, and \"y\" as the x = df [[ 'Ozone' ]] . values y = df [ 'Temp' ] In [20]: # Split the data into train and test sets with train size as 0.8 and random_state as 102 x_train , x_test , y_train , y_test = train_test_split ( x , y , train_size = 0.8 , random_state = 102 ) Bagging Regressor In [21]: # Specify the number of bootstraps as 30 num_bootstraps = 30 # Specify the maximum depth of the decision tree as 3 max_depth = 3 # Define the Bagging Regressor Model # Use Decision Tree as your base estimator with depth as mentioned in max_depth # Initialise number of estimators using the num_bootstraps value model = ___ # Fit the model on the train data ___ Out[21]: Ozone Solar.R Wind Temp Month Day 1 41.0 190.0 7.4 67 5 1 2 36.0 118.0 8.0 72 5 2 3 12.0 149.0 12.6 74 5 3 4 18.0 313.0 11.5 62 5 4 6 28.0 NaN 14.9 66 5 6 7 23.0 299.0 8.6 65 5 7 8 19.0 99.0 13.8 59 5 8 9 8.0 19.0 20.1 61 5 9 11 7.0 NaN 6.9 74 5 11 12 16.0 256.0 9.7 69 5 12 13 11.0 290.0 9.2 66 5 13 14 14.0 274.0 10.9 68 5 14 15 18.0 65.0 13.2 58 5 15 16 14.0 334.0 11.5 64 5 16 17 34.0 307.0 12.0 66 5 17 18 6.0 78.0 18.4 57 5 18 19 30.0 322.0 11.5 68 5 19 20 11.0 44.0 9.7 62 5 20 21 1.0 8.0 9.7 59 5 21 22 11.0 320.0 16.6 73 5 22 23 4.0 25.0 9.7 61 5 23 24 32.0 92.0 12.0 61 5 24 28 23.0 13.0 12.0 67 5 28 In [24]: # Helper code to plot the predictions of individual estimators and plt . figure ( figsize = ( 10 , 8 )) xrange = np . linspace ( x . min (), x . max (), 80 ) . reshape ( - 1 , 1 ) plt . plot ( x_train , y_train , 'o' , color = '#EFAEA4' , markersize = 6 , label = \"Train Data\" ) plt . plot ( x_test , y_test , 'o' , color = '#F6345E' , markersize = 6 , label = \"Test Data\" ) plt . xlim () for i in model . estimators_ : y_pred1 = i . predict ( xrange ) plt . plot ( xrange , y_pred1 , alpha = 0.5 , linewidth = 0.5 , color = '#ABCCE3' ) plt . plot ( xrange , y_pred1 , alpha = 0.6 , linewidth = 1 , color = '#ABCCE3' , label = \"Prediction of Individual Estimators\" ) y_pred = model . predict ( xrange ) plt . plot ( xrange , y_pred , alpha = 0.7 , linewidth = 3 , color = '#50AEA4' , label = 'Model Prediction' ) plt . xlabel ( \"Ozone\" , fontsize = 16 ) plt . ylabel ( \"Temperature\" , fontsize = 16 ) plt . xticks ( fontsize = 12 ) plt . yticks ( fontsize = 12 ) plt . legend ( loc = 'best' , fontsize = 12 ) plt . show () In [25]: # Compute the test MSE of the prediction of individual estimator y_pred1 = ___ print ( \"The test MSE of one estimator in the model is\" , round ( mean_squared_error ( y_test , y_pred1 ), 2 )) In [0]: ### edTest(test_mse) ### # Compute the test MSE of the model prediction y_pred = ___ print ( \"The test MSE of the model is\" , round ( mean_squared_error ( y_test , y_pred ), 2 )) Mindchow 🍲 After marking, go back and change the number of bootstraps and the maximum depth of the tree. Do you see any relation between them? How does the variance change with change in maximum depth? How does the variance change with change in number of bootstraps? Your answer here In [0]:","tags":"labs","url":"labs/lecture-23/notebook-2/"},{"title":"Lecture 23: Regression Trees, Bagging, and RF","text":"Slides PDF | Lecture 23: Bagging PPTX | Lecture 23: Bagging Exercises Lecture 23: Regression with Bagging [Notebook] Lecture 23: Bagging Classification with Decision Boundary [Notebook]","tags":"lectures","url":"lectures/lecture23/"},{"title":"Lecture 21: PCA & Missing Data","text":"CS-109A Introduction to Data Science Lecture 21: More on Classification and PCA Harvard University Fall 2020 Instructors: Pavlos Protopapas, Kevin Rader, Chris Tanner Contributors: Kevin Rader, Eleni Kaxiras, Chris Tanner, Will Claybaugh, David Sondak In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Learning Goals In this lab, we will look at how to use PCA to reduce a dataset to a smaller number of dimensions. The goals are: Better Understand the multiclass setting Understand what PCA is and why it's useful Feel comfortable performing PCA on a new dataset Understand what it means for each component to capture variance from the original dataset Be able to extract the `variance explained` by components Perform modelling with the PCA components In [25]: % matplotlib inline import numpy as np import scipy as sp import matplotlib.pyplot as plt import seaborn as sns import pandas as pd from sklearn.linear_model import LogisticRegressionCV from sklearn.linear_model import LassoCV from sklearn.metrics import accuracy_score pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) pd . set_option ( 'display.notebook_repr_html' , True ) from sklearn.model_selection import train_test_split Part 0: Introduction What is PCA? PCA is a deterministic technique to transform data to a lowered dimensionality, whereby each feature/dimension captures the most variance possible. Why do we care to use it? Visualizating the components can be useful Allows for more efficient use of resources (time, memory) Statistical reasons: fewer dimensions -> better generalization noise removal / collinearity (improving data quality) Imagine some dataset where we have two features that are pretty redundant. For example, maybe we have data concerning elite runners. Two of the features may include VO2 max and heartrate . These are highly correlated. We probably don't need both, as they don't offer much additional information from each other. Using a great visual example from online , let's say that this unlabelled graph (always label your axis) represents those two features: Let's say that this is our entire dataset, just 2 dimensions. If we wish to reduce the dimensions, we can only reduce it to just 1 dimension. A straight line is just 1 dimension (to help clarify this: imagine your straight line as being the x-axis, and values can be somewhere on this axis, but that's it. There is no y-axis dimension for a straight line). So, how should PCA select a straight line through this data? Below, the image shows all possible projects that are centered in the data: PCA picks the line that: captures the most variance possible minimizes the distance of the transformed points (distance from the original to the new space) The animation suggests that these two aspects are actually the same. In fact, this is objectively true, but the proof for which is beyond the scope of the material for now. Feel free to read more at this explanation and via Andrew Ng's notes . In short, PCA is a math technique that works with the covariance matrix -- the matrix that describes how all pairwise features are correlated with one another. Covariance of two variables measures the degree to which they moved/vary in the same directin; how much one variable affects the other. A positive covariance means they are positively related (i.e., x1 increases as x2 does); negative means negative correlation (x1 decreases as x2 increases). In data science and machine learning, our models are often just finding patterns in the data this is easier if the data is spread out across each dimension and for the data features to be independent from one another (imagine if there's no variance at all. We couldn't do anything). Can we transform the data into a new set that is a linear combination of those original features? PCA finds new dimensions (set of basis vectors) such that all the dimensions are orthogonal and hence linearly independent, and ranked according to the variance (eigenvalue). That is, the first component is the most important, as it captures the most variance. Part 1a: The Wine Quality Dataset Imagine that a wine sommelier has tasted and rated 1,000 distinct wines, and now that she's highly experienced, she is curious if she can more efficiently rate wines without even trying them. That is, perhaps her tasting preferences follow a pattern, allowing her to predict the rating a new wine without even trying it! The dataset contains 11 chemical features, along with a quality scale from 1-10; however, only values of 3-9 are actually used in the data. The ever-elusive perfect wine has yet to be tasted. NOTE: While this dataset involves the topic of alcohol, we, the CS109A staff, along with Harvard at large is in no way encouraging alcohol use, and this example should not be intrepreted as any endorsement for such; it is merely a pedagogical example. I apologize if this example offends anyone or is off-putting. Read-in and checking First, let's read-in the data and verify it: In [61]: wines_df = pd . read_csv ( \"data/wines.csv\" ) wines_df . head () In [62]: wines_df . describe () For this exercise, let's say that the wine expert is curious if she can predict, as a rough approximation, the categorical quality -- bad, average, or great. Let's define those categories as follows: bad is when for wines that have a quality <= 5 average is when a wine has a quality of 6 great is when a wine has a quality of >= 7 Q1.1 For sanity, let's see how many wines will end up in each quality category (create the barplot) In [63]: counts = np . unique ( wines_df [ 'quality' ], return_counts = True ) ### use seaborn to create a barplot sns . barplot ( x = ___ , y = ___ ); In [64]: ##### copy the original data so that we're free to make changes wines_df_recode = wines_df . copy () # use the 'cut' function to reduce a variable down to the aforementioned bins (inclusive boundaries) wines_df_recode [ 'quality' ] = pd . cut ( wines_df_recode [ 'quality' ],[ 0 , 5 , 6 , 10 ], labels = [ 0 , 1 , 2 ]) wines_df_recode . loc [ wines_df_recode [ 'quality' ] == 1 ] # drop the un-needed columns x_data = wines_df_recode . drop ([ 'quality' ], axis = 1 ) y_data = wines_df_recode [ 'quality' ] x_train , x_test , y_train , y_test = train_test_split ( x_data , y_data , test_size =. 2 , random_state = 8 , stratify = y_data ) # previews our data to check if we correctly constructed the labels (we did) print ( wines_df [ 'quality' ] . head ()) print ( wines_df_recode [ 'quality' ] . head ()) In [65]: y_data . value_counts () Now that we've split the data, let's look to see if there are any obvious patterns (correlations between different variables). In [66]: from pandas.plotting import scatter_matrix scatter_matrix ( wines_df_recode , figsize = ( 30 , 20 )); It looks like there aren't any particularly strong correlations among the predictors (maybe total sulfur dioxide and free sulfur dioxide ) so we're safe to keep them all. Part 1b: Baseline Prediction Models Before we do anything too fancy, it's always best to start with something simple. Baseline The baseline estimate is barely a model -- it simple returns the single label/class occurs the most often (in the training data). In other words, whichever label/class is most popular, it will always preduct that as its answer, completely independent of any x-data. Above, we saw that the most popular label was average , represented by 432 of our 1,000 wines. Thus, our MLE should yield average for all inputs: In [67]: baseline_class = y_data . value_counts () . idxmax () baseline_train_accuracy = len ( y_train . loc [ y_train == baseline_class ]) / len ( y_train ) baseline_test_accuracy = len ( y_test . loc [ y_test == baseline_class ]) / len ( y_test ) scores = [[ baseline_train_accuracy , baseline_test_accuracy ]] names = [ 'baseline' ] df_results = pd . DataFrame ( scores , index = names , columns = [ 'Train Accuracy' , 'Test Accuracy' ]) df_results Baseline gives a predictive accuracy is 43.3% on the test set. Hopefully we can do better than this. Logistic Regression Logistic regression is used for predicting categorical outputs, which is exactly what our task concerns. So, let's create a multiclass logistic regression model ( ovr : one-vs.-rest): In [68]: from sklearn.linear_model import LogisticRegression lr = LogisticRegression ( C = 1000000 , solver = 'lbfgs' , multi_class = 'ovr' , max_iter = 10000 ) . fit ( x_train , y_train ) print ( \"Coefficients:\" , lr . coef_ ) print ( \"Intercepts:\" , lr . intercept_ ) Q1.2 What is stored in .coef_ and .intercept_ ? Why are there so many of them? your answer here Q1.3 Determine measure its performance as measured by classific ation accuracy: In [69]: ### edTest(test_acc) ### lr_train_accuracy = lr . score ( ___ , ___ ) lr_test_accuracy = lr . score ( ___ , ___ ) # appends results to our dataframe names . append ( 'Logistic Regression' ) scores . append ([ lr_train_accuracy , lr_test_accuracy ]) df_results = pd . DataFrame ( scores , index = names , columns = [ 'Train Accuracy' , 'Test Accuracy' ]) df_results Yay, that's better than our baseline performance. Can we do better with cross-validation? Part 1c: Tuning our Prediction Model Summary Logistic regression extends OLS to work naturally with a dependent variable that's only ever 0 and 1. In fact, Logistic regression is even more general and is used for predicting the probability of an example belonging to each of $N$ classes. The code for the two cases is identical and just like LinearRegression : .fit , .score , and all the rest Significant predictors does not imply that the model actually works well. Signifigance can be driven by data size alone. The data aren't enough to do what we want Warning : Logistic regression tries to hand back valid probabilities. As with all models, you can't trust the results until you validate them- if you're going to use raw probabilities instead of just predicted class, take the time to verify that if you pool all cases where the model says \"I'm 30% confident it's class A\" 30% of them actually are class A. Q1.4 Use LogisticRegressionCV to build a better tuned ovr (lasso-based) model based on all of these predictors. In [70]: ### edTest(test_logitCV) ### logit_regr_lasso = LogisticRegressionCV ( solver = 'liblinear' , multi_class = ___ , penalty = ___ , max_iter = ___ , cv = 10 ) logit_regr_lasso . fit ( x_train , y_train ) # fit y_train_pred_lasso = ___ # predict the test set y_test_pred_lasso = ___ # predict the test set train_score_lasso = ___ # get train accuracy test_score_lasso = ___ # get test accuracy names . append ( 'Logistic Regression w/ CV + Lasso' ) scores . append ([ train_score_lasso , test_score_lasso ]) df_results = pd . DataFrame ( scores , index = names , columns = [ 'Train Accuracy' , 'Test Accuracy' ]) df_results Q1.5 : Hmm, cross-validation didn't seem to offer much improved results. Is this correct? Is it possible for cross-validation to not yield much better results than non-cross-validation? If so, why is that happening here? What else should we consider? your answer here Part 2: Dimensionality Reduction In attempt to improve performance, we may wonder if some of our features are redundant and are posing difficulties for our logistic regression model. Q2.1 : Let's PCA decompose our predictors to shrink the problem down to 2 dimensions (with as little loss as possible) and see if that gives us a clue about what makes this problem tough. In [50]: from sklearn.decomposition import PCA from sklearn.preprocessing import StandardScaler num_components = 2 # scale the datasets scale_transformer = StandardScaler ( copy = True ) . fit ( x_train ) x_train_scaled = scale_transformer . transform ( x_train ) x_test_scaled = scale_transformer . transform ( x_test ) # reduce dimensions pca_transformer = PCA ( ___ ) . fit ( ___ ) x_train_2d = pca_transformer . transform ( ___ ) x_test_2d = pca_transformer . transform ( ___ ) print ( x_train_2d . shape ) x_train_2d [ 0 : 5 ,:] NOTE: Both scaling and reducing dimension follow the same pattern: we fit the object to the training data, then use .transform() to convert the training and test data. This ensures that, for instance, we scale the test data using the training mean and variance, not its own mean and variance We need to equalize the variance of each feature before applying PCA; otherwise, certain dimensions will dominate the scaling. Our PCA dimensions would just be the features with the largest spread. Q2.2 : Why didn't we scale the y-values (class labels) or transform them with PCA? Is this a mistake? your answer here Q2.3 : Our data only has 2 dimensions/features now. What do these features represent? In [52]: # your code here your answer here Since our data only has 2 dimensions now, we can easily visualize the entire dataset. If we choose to color each datum with respect to its associated label/class, it allows us to see how separable the data is. That is, it gives an indication as to how easy/difficult it is for a model to fit the new, transformed data. Q2.4 : Create the scatterplot with color-coded points for the 3 classes of wine quaslity, and put PCA vector 1 on the x-axis and PCA vector 2 on the y-axis. In [53]: # notice that we set up lists to track each group's plotting color and label colors = [ 'r' , 'c' , 'b' ] label_text = [ \"Bad Wines\" , \"Average Wines\" , \"Great Wines\" ] # and we loop over the different groups for cur_quality in [ 0 , 1 , 2 ]: cur_df = x_train_2d [ y_train == cur_quality ] plt . scatter ( ___ , ___ , c = colors [ cur_quality ], label = label_text [ cur_quality ]) # all plots need labels plt . xlabel ( \"PCA Dimension 1\" ) plt . ylabel ( \"PCA Dimention 2\" ) plt . legend (); Well, that gives us some idea of why the problem is difficult! The bad, average, and great wines are all on top of one another. Not only are there few great wines, which we knew from the beginning, but there is no line that can easily separate the classes of wines. Q2.5 : What critique can you make against the plot above? Why does this plot not prove that the different wines are hopelessly similar? The wine data we've used so far consist entirely of continuous predictors. Would PCA work with categorical data? Looking at our PCA plot above, we see something peculiar: we have two disjoint clusters, both of which have immense overlap in the qualities of wines.What could cause this? What does this mean? your answer here Let's plot the same PCA'd data, but let's color code them according to if the wine is red or white In [56]: # notice that we set up lists to track each group's plotting color and label colors = [ 'r' , 'c' , 'b' ] label_text = [ \"Reds\" , \"Whites\" ] # and we loop over the different groups for cur_color in [ 0 , 1 ]: cur_df = x_train_2d [ x_train [ 'red' ] == cur_color ] plt . scatter ( ___ , ___ , c = colors [ cur_color ], label = label_text [ cur_color ]) # all plots need labels plt . xlabel ( \"PCA Dimension 1\" ) plt . ylabel ( \"PCA Dimention 2\" ) plt . legend (); Q2.6 : Wow. Look at that separation. Too bad we aren't trying to predict if a wine is red or white. Does this graph help you answer our previous question? Does it change your thoughts? your answer here Part 2b. Evaluating PCA: Variance Explained and Predictions One of the criticisms we made of the PCA plot was that it's lost something from the original data. Heck, we're only using 2 dimensions, we it's perfectly reasonable and expected for us to lose some information -- our goal was that the information we were discarding was noise. Let's investigate how much of the original data's structure the 2-D PCA captures. We'll look at the explained_variance_ratio_ portion of the PCA fit. This lists, in order, the percentage of the x data's total variance that is captured by the nth PCA dimension. In [58]: var_explained = pca_transformer . explained_variance_ratio_ print ( \"Variance explained by each PCA component:\" , var_explained ) print ( \"Total Variance Explained:\" , np . sum ( var_explained )) The first PCA dimension captures 33% of the variance in the data, and the second PCA dimension adds another 20%. Together, we've captured about half of the total variation in the training data with just these two dimensions. So far, we've used PCA to transform our data, we've visualized our newly transformed data, and we've looked at the variance that it captures from the original dataset. That's a good amount of inspection; now let's actually use our transformed data to make predictions. Q2.7 Use Logistic Regression (with and without cross-validation) on the PCA-transformed data. Do you expect this to outperform our original accuracy? What are your results? Does this seem reasonable? In [59]: ### edTest(test_lrPCA) ### # do the non-CV approach here (create CV on your own if you'd like) lr_pca = ___ lr_pca_train_accuracy = ___ lr_pca_test_accuracy = ___ names . append ( 'Logistic Regression w/ PCA' ) ___ ___ df_results We're only using 2 dimensions. What if we increase our data to the full PCA components? Q2.8 : How can we incorporate the type of wine (Red vs. White) into this analysis (hink of at least 2 different approaches)? Can these both approaches be used for the PCA and non-PCA models? Part 3: Going further with PCA Q3.1 : Fit a PCA that finds the complete PCA decomposition of our training data predictors Use `np.cumsum()` to print out the variance we'd be able to explain by using n PCA dimensions for all choices of components. Does the full-dimension PCA agree with the 2d PCA on how much variance the first components explain? **Do the full and 2d PCAs find the same first two dimensions? Why or why not?** Make a plot of number of PCA dimensions against total variance explained. What PCA dimension looks good to you? Use cross-validation to determine how many of the PCA copmonents add to the predictive nature of the PCR. Hint: np.cumsum stands for 'cumulative sum', so np.cumsum([1,3,2,-1,2]) is [1,4,6,5,7] In [60]: #your code here The plot above can be used to inform of us when we reach diminishing returns on variance explained. That is, the 'elbow' of the line is probably an ideal number of dimensions to use, at least with respect to the amount of variance explained. Q3.2 Looking at your graph, what is the 'elbow' point / how many PCA components do you think we should use? Does this number of components agree with the cross-validated predictive performance of the PCR? Explain. your answer here Part 3b: PCA Debriefing: PCA maps a high-dimensional space into a lower dimensional space. The PCA dimensions are ordered by how much of the original data's variance they capture There are other cool and useful properties of the PCA dimensions (orthogonal, etc.). See a textbook . PCA on a given dataset always gives the same dimensions in the same order. You can select the number of dimensions by fitting a big PCA and examining a plot of the cumulative variance explained. PCA is not guaranteed to improve predictive performance at all. As you've learned in class now, none of our models are guaranteed to always outperform others on all datasets; analyses are a roll of the dice. The goal is to have a suite of tools to allow us to gather, process, disect, model, and visualize the data -- and to learn which tools are better suited to which conditions. Sometimes our data isn't the most conducive to certain tools, and that's okay. What can we do about it? Be honest about the methods and the null result. Lots of analyses fail. Collect a dataset you think has a better chance of success. Maybe we collected the wrong chemical signals... Keep trying new approaches. Just beware of overfitting the data you're validating on. Always have a test set locked away for when the final model is built. Change the question. Maybe something you noticed during analysis seems interesting or useful (classifying red versus white). But again, you the more you try, the more you might overfit, so have test data locked away. Just move on. If the odds of success start to seem small, maybe you need a new project. In [0]:","tags":"labs","url":"labs/lecture-21/notebook/"},{"title":"Lecture 21: PCA & Missing Data","text":"See lecture 20 slides Exercises Lecture 21: Multi-class Classification and PCA [Notebook]","tags":"lectures","url":"lectures/lecture21/"},{"title":"S-Section 06: PCA and Logistic Regression","text":"CS109A Introduction to Data Science Standard Section 6: PCA and Logistic Regression Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Hayden Joy In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: For this section, our goal is to get you familiarized with Dimensionality Reduction using Principal Components Analysis (PCA) and to recap Logistic Regression from the last homework. This medium article was referenced extensively while creating this notebook. Specifically, we will: Define the terms big data and high-dimensionality Learn what PCA is. What is the motivation to use PCA. Learn about the sklearn PCA library and its nuances Get familiar with the Linear Algebra of PCA Use PCA to visualize high-dimensional data in 2-dimensions Meet the MNIST handwritten digit dataset (and hopefully stay friends for a while) Use PCA in order to improve model training time and understand the speed-accuracy trade-off Discuss when to use PCA and when not to use it Assumptions behind PCA In [5]: # Data and stats packages import numpy as np import pandas as pd # Visualization packages % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns sns . set () # from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler # NEW PACKAGES from sklearn.decomposition import PCA from time import time # Other packages pd . set_option ( 'display.max_columns' , 50 ) import warnings warnings . filterwarnings ( \"ignore\" ) Motivation Principal Components Analysis helps us deal with high-dimensionality in big-data. But first... High-dimensionality is the case when p is large i.e. there are a lot of predictors. This is sometimes a problem because: Our models may be overfit There may be multi-collinearity Matrices may not be invertible (in the case of OLS) Our challenge is to represent the p dimensions by a smaller number (m) of dimensions without losing much information. Then, we can fit a model using the m predictors, which addresses the three problems listed above. Here's where PCA comes into play. What is Principal Components Analysis (PCA)? A Framework For Dimensionality Reduction Consider that the data are described by a Linear Regression Model: $$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p + \\epsilon, $$ where $p$ is very large, i.e. high dimensionality . LASSO for dimensional reduction: We can use LASSO: which will drop some of the predictors by forcing $\\beta_j=0$. That effectively reduces the dimensions. Are we happy with that? No! Because we totally lose the information kept by the $X_j$ predictors PCA for dimensional reduction: Considering a new system of coordinates, namely a new set of predictors, denoted by $Z_1$, $Z_2$,$\\dots$, $Z_m$, where $m \\leq p$ and where each $Z_i$ is a linear combination of the original $p$ predictors, $X_1, \\dots~ X_p$, thus: $$ Z_i = \\sum_{j=1}&#94;{p} c_{ij} X_i $$ for some fixed coefficients $c_{ij}$ (PCA will determine them). For example: $$ Z_1 = 3.3 X_1 + 4 X_2 + 0 X_3 + \\dots + 1.2 X_p. $$ Then we can build a linear regression model using the new predictors as follows: $$ Y = \\theta_0 + \\theta_1 Z_1 + \\theta_2 Z_2 + \\dots + \\theta_m Z_m + \\epsilon $$ Notice: For $m=p$ the number of predictors is the same, hence, we have not lost any information. We just transform the coordinate systems of the data-space . Determine the PCA coefficients $c_{ij}$: PCA identifies a new set of predictors, as linear combinations of the original ones, that captures the 'maximum amount' of variance in the observed data. In other words, PCA determines the $c_{ij}$ such as the data varies most along the new axis $Z_i$, which are called principal components . PCA sorts the axis such as the largest variance goes along $Z_1$, the second largest variance goes to $Z_2$ and so on. Comments: The basic assumption in PCA is that higher variance indicates more importance The principal components consist an $m$-dimensional orthonormal system of coordinates We see that the \"best line\" is the one where there is maximal variance along the line. Source here . In principle, we could explore all the rotations, that is, rotating the original coordinate system under all the angles, and find which rotation yields the maximum variance. However, when the dimensionality (p) is large this is an extremely time consuming and inefficient technique. In that case we may use PCA which is systematic way to find the best rotation or the best coordinate system. PCA is a mathematical method based on linear algebra, for more details and rigorous formulation check the advanced section for PCA. Have we reduced the dimensions yet? No yet... So far we have defined new predictors that linearly depend on the original predictors. Hence, we still have all the information stored in the data. Reducing the dimensions: Since the $Z_j$ are sorted with respect how much information the carry , the larger the $j$ the less important the $Z_j$. Hence, we can keep just a few of the principal components and drop the rest. For instance, keeping only the first two components $(m=2)$, we obtain two predictors that contain information from all the original $p$ predictors. How can we choose the m? PCA takes care of that too. Applications of PCA One major application of PCA is to address the issue of high-dimensionality (reduce the number of predictors). Another major application of PCA is in visualization . Specifically, if we have an N-dimensional dataset (N>3), how do we visualize it? One option : A more practical option : use PCA to get the top 2-3 principal components and plot these components on 2D or 3D plots. PCA for Visualization Data Source: MTCars Dataset Here are a few resources that use this dataset and apply PCA for visualization. This notebook references this PCA tutorial in R , these lecture notes from CMU , this blog , and this blog which has some nice visualizations of PCA. Loading in The Cars Dataset and carry out EDA This dataset consists of data on 32 models of car, taken from an American motoring magazine (1974 Motor Trend magazine). For each car, you have 11 features , expressed in varying units (US units), They are as follows ( source ): mpg : Fuel consumption (Miles per (US) gallon): more powerful and heavier cars tend to consume more fuel. cyl : Number of cylinders: more powerful cars often have more cylinders disp : Displacement (cu.in.): the combined volume of the engine's cylinders hp : Gross horsepower: this is a measure of the power generated by the car drat : Rear axle ratio: this describes how a turn of the drive shaft corresponds to a turn of the wheels. Higher values will decrease fuel efficiency. wt : Weight (1000 lbs): pretty self-explanatory! qsec : 1/4 mile time: the car's speed and acceleration vs : Engine block: this denotes whether the vehicle's engine is shaped like a \"V\", or is a more common straight shape. am : Transmission: this denotes whether the car's transmission is automatic (0) or manual (1). gear : Number of forward gears: sports cars tend to have more gears. carb : Number of carburetors: associated with more powerful engines Note that the units used vary and occupy different scales. Dropping the categorical variables vs and am and only keeping in the continuous predictors . In [3]: cars_df = pd . read_csv ( '../data/mtcars.csv' ) cars_df = cars_df [ cars_df . columns . difference ([ 'am' , 'vs' ])] cars_df . head () Out[3]: carb cyl disp drat gear hp model mpg qsec wt 0 4 6 160.0 3.90 4 110 Mazda RX4 21.0 16.46 2.620 1 4 6 160.0 3.90 4 110 Mazda RX4 Wag 21.0 17.02 2.875 2 1 4 108.0 3.85 4 93 Datsun 710 22.8 18.61 2.320 3 1 6 258.0 3.08 3 110 Hornet 4 Drive 21.4 19.44 3.215 4 2 8 360.0 3.15 3 175 Hornet Sportabout 18.7 17.02 3.440 In [102]: cars_df . describe () Out[102]: carb cyl disp drat gear hp mpg qsec wt count 32.0000 32.000000 32.000000 32.000000 32.000000 32.000000 32.000000 32.000000 32.000000 mean 2.8125 6.187500 230.721875 3.596563 3.687500 146.687500 20.090625 17.848750 3.217250 std 1.6152 1.785922 123.938694 0.534679 0.737804 68.562868 6.026948 1.786943 0.978457 min 1.0000 4.000000 71.100000 2.760000 3.000000 52.000000 10.400000 14.500000 1.513000 25% 2.0000 4.000000 120.825000 3.080000 3.000000 96.500000 15.425000 16.892500 2.581250 50% 2.0000 6.000000 196.300000 3.695000 4.000000 123.000000 19.200000 17.710000 3.325000 75% 4.0000 8.000000 326.000000 3.920000 4.000000 180.000000 22.800000 18.900000 3.610000 max 8.0000 8.000000 472.000000 4.930000 5.000000 335.000000 33.900000 22.900000 5.424000 Our task is to try to visualize this data in a meaningful way. Obviously we can't make a 9-dimensional plot, but we can try to make several different plots using the pairplot function from seaborn. In [6]: sns . pairplot ( cars_df ); But there are numerous variables and numerous more relationships between these variables. We can do better through PCA. Visualization by using PCA Standardizing Variables Standardization is a crucial step in PCA formulation (more details in advanced section) In [6]: # separating the quantitative predictors from the model of the car (a string) model = cars_df [ 'model' ] quant_df = cars_df [ cars_df . columns . difference ([ 'model' ])] # Standardization quant_scaled = StandardScaler () . fit_transform ( quant_df ) cars_df_scaled = pd . DataFrame ( quant_scaled , columns = quant_df . columns ) # We can bring back the model variable, although we do not need it cars_df_scaled [ 'model' ] = cars_df [ 'model' ] cars_df_scaled . head () Out[6]: carb cyl disp drat gear hp mpg qsec wt model 0 0.746967 -0.106668 -0.579750 0.576594 0.430331 -0.543655 0.153299 -0.789601 -0.620167 Mazda RX4 1 0.746967 -0.106668 -0.579750 0.576594 0.430331 -0.543655 0.153299 -0.471202 -0.355382 Mazda RX4 Wag 2 -1.140108 -1.244457 -1.006026 0.481584 0.430331 -0.795570 0.456737 0.432823 -0.931678 Datsun 710 3 -1.140108 -0.106668 0.223615 -0.981576 -0.946729 -0.543655 0.220730 0.904736 -0.002336 Hornet 4 Drive 4 -0.511083 1.031121 1.059772 -0.848562 -0.946729 0.419550 -0.234427 -0.471202 0.231297 Hornet Sportabout In [7]: cars_df_scaled . describe () Out[7]: carb cyl disp drat gear hp mpg qsec wt count 3.200000e+01 3.200000e+01 3.200000e+01 3.200000e+01 3.200000e+01 3.200000e+01 3.200000e+01 3.200000e+01 3.200000e+01 mean -6.938894e-18 4.163336e-17 1.387779e-16 -3.122502e-16 -1.144917e-16 -1.734723e-17 -5.481726e-16 -1.469311e-15 4.683753e-17 std 1.016001e+00 1.016001e+00 1.016001e+00 1.016001e+00 1.016001e+00 1.016001e+00 1.016001e+00 1.016001e+00 1.016001e+00 min -1.140108e+00 -1.244457e+00 -1.308518e+00 -1.589643e+00 -9.467293e-01 -1.403130e+00 -1.633610e+00 -1.903996e+00 -1.769642e+00 25% -5.110827e-01 -1.244457e+00 -9.008917e-01 -9.815764e-01 -9.467293e-01 -7.437050e-01 -7.865141e-01 -5.436944e-01 -6.604034e-01 50% -5.110827e-01 -1.066677e-01 -2.821771e-01 1.870518e-01 4.303315e-01 -3.510140e-01 -1.501383e-01 -7.888899e-02 1.118844e-01 75% 7.469671e-01 1.031121e+00 7.810529e-01 6.145986e-01 4.303315e-01 4.936423e-01 4.567366e-01 5.977084e-01 4.078199e-01 max 3.263067e+00 1.031121e+00 1.977904e+00 2.533809e+00 1.807392e+00 2.790515e+00 2.327934e+00 2.871986e+00 2.291423e+00 Carrying out PCA Sklearn PCA documentation In [8]: # drop again the model predictor quant_df = cars_df_scaled [ cars_df_scaled . columns . difference ([ 'model' ])] # fitting the PCA object onto our dataframe (excluding the model name column) pca = PCA () . fit ( quant_df ) # transforming the dataframe quant_df_pca = pca . transform ( quant_df ) print ( 'Original dimensions:' , quant_df . shape ) print ( 'PCA dimensions: ' , quant_df_pca . shape ) Original dimensions: (32, 9) PCA dimensions: (32, 9) Let us examine some of the attributes we obtain from PCA. explained_variance_ : The amount of variance explained by each of the selected principal components. explained_variance_ratio_ : Percentage of variance explained by each of the selected principal components. By default, unless n_components is specified all components will be stored and the sum of the ratios will be 1.0. In [9]: fig , ax = plt . subplots ( ncols = 2 , figsize = ( 20 , 6 )) ax1 , ax2 = ax . ravel () ratio = pca . explained_variance_ratio_ ax1 . bar ( range ( len ( ratio )), ratio , color = 'purple' , alpha = 0.8 ) ax1 . set_title ( 'Explained Variance Ratio PCA' , fontsize = 20 ) ax1 . set_xticks ( range ( len ( ratio ))) ax1 . set_xticklabels ([ 'PC {} ' . format ( i + 1 ) for i in range ( len ( ratio ))]) ax1 . set_ylabel ( 'Explained Variance Ratio' ) # ratio[0]=0 ratio = pca . explained_variance_ratio_ ax2 . plot ( np . cumsum ( ratio ), 'o-' ) ax2 . set_title ( 'Cumulative Sum of Explained Variance Ratio PCA' , fontsize = 20 ) ax2 . set_ylim ( 0 , 1.1 ) ax2 . set_xticks ( range ( len ( ratio ))) ax2 . set_xticklabels ([ 'PC {} ' . format ( i + 1 ) for i in range ( len ( ratio ))]) ax2 . set_ylabel ( 'Cumulative Sum of Explained Variance Ratio' ); We see that over 85% of the variance is explained by the first 2 principal components! components_ : This represents the principal components i.e. directions of maximum variance in the data. The components are sorted by explained_variance_ . Let us write the equation for all the principal components using our formulation of the principal components above: $$ Z_i = \\sum_{j=1}&#94;{p} w_{ij} X_i $$ In [10]: for i , comp in enumerate ( pca . components_ ): expression = 'Z_ {} = ' . format ( i + 1 ) for c , x in zip ( comp , quant_df . columns ): if c < 0 : expression += str ( np . round ( c , 2 )) + '*' + x + ' ' else : expression += '+' + str ( np . round ( c , 2 )) + '*' + x + ' ' print ( expression + ' \\n ' ) Z_1 = -0.24*carb -0.4*cyl -0.4*disp +0.31*drat +0.21*gear -0.37*hp +0.39*mpg +0.22*qsec -0.37*wt Z_2 = +0.48*carb +0.02*cyl -0.09*disp +0.34*drat +0.55*gear +0.27*hp +0.03*mpg -0.48*qsec -0.17*wt Z_3 = +0.46*carb -0.25*cyl -0.08*disp +0.15*drat +0.21*gear -0.02*hp -0.22*mpg +0.63*qsec +0.45*wt Z_4 = -0.21*carb +0.04*cyl +0.34*disp +0.85*drat -0.28*gear +0.07*hp -0.01*mpg -0.03*qsec +0.19*wt Z_5 = -0.4*carb -0.12*cyl +0.49*disp -0.16*drat +0.56*gear +0.29*hp +0.32*mpg +0.15*qsec +0.19*wt Z_6 = +0.36*carb +0.22*cyl -0.02*disp -0.02*drat -0.32*gear +0.35*hp +0.72*mpg +0.26*qsec -0.08*wt Z_7 = -0.21*carb -0.16*cyl -0.18*disp +0.05*drat -0.09*gear +0.7*hp -0.38*mpg +0.28*qsec -0.43*wt Z_8 = -0.11*carb +0.81*cyl -0.06*disp +0.14*drat +0.32*gear -0.17*hp -0.12*mpg +0.36*qsec -0.2*wt Z_9 = -0.32*carb +0.16*cyl -0.66*disp +0.04*drat +0.05*gear +0.25*hp +0.11*mpg -0.17*qsec +0.57*wt Using the printed equations above, we can create vectors showing where each feature has a high value. Let us do this for the first 2 principal components (using $v$ to denote a vector): $$ \\begin{aligned} v_{carb} = \\begin{pmatrix}-0.24 \\\\ 0.48 \\end{pmatrix}, \\; v_{cyl} = \\begin{pmatrix}-0.4 \\\\ 0.02 \\end{pmatrix}, \\; v_{disp} = \\begin{pmatrix}-0.4 \\\\ -0.09 \\end{pmatrix}, \\\\ v_{drat} = \\begin{pmatrix}0.31 \\\\ 0.34 \\end{pmatrix}, \\; v_{gear} = \\begin{pmatrix}0.21 \\\\ 0.55 \\end{pmatrix}, \\; v_{hp} = \\begin{pmatrix}-0.37 \\\\ 0.27 \\end{pmatrix}, \\\\ v_{mpg} = \\begin{pmatrix}0.39 \\\\ 0.03 \\end{pmatrix}, \\; v_{qsec} = \\begin{pmatrix}0.22 \\\\ -0.48 \\end{pmatrix}, \\; v_{wt} = \\begin{pmatrix}-0.37 \\\\ -0.17 \\end{pmatrix} \\end{aligned} $$ Checking if our vectors are orthonormal Orthonormal vectors are the vectors which are orthogonal (zero dot product) with length equal to one (unit vectors). Orthogonal: We use the dot product between two vectors to check if the vectors are orthogonal or not. If the dot product is 0, that means that the two vectors are orthogonal. The dot product between two vectors is (geometrically): $$ \\textbf{a} \\cdot \\textbf{b} = ||\\textbf{a}|| ||\\textbf{b}|| \\cos(\\theta) $$ Where $\\theta$ is the angle between the two vectors and $||\\cdot||$ denotes the norm of the vector. Since we assume that the norm of a vector is non-zero, the only way the dot product of two vectors to be zero is when the angle between them is 90 degrees ($\\cos(90) = 0$). Thus, the dot product is a way to check if two vectors are perpendicular. Unit vectors In order to calculate the length $||\\textbf{a}||$ of a vector we can take the dot product of a vector with itself, namely $$ ||\\textbf{a}|| = \\textbf{a}\\cdot \\textbf{a} $$ In [11]: vec1 = pca . components_ [ 0 ] vec2 = pca . components_ [ 1 ] # print(np.dot(vec1.T, vec2)) print ( 'The dot product between the first two principal components is ' , np . round ( np . dot ( vec1 , vec2 ), 5 )) print ( 'The length of the first principal component is ' , np . round ( np . dot ( vec1 , vec1 ), 5 )) The dot product between the first two principal components is 0.0 The length of the first principal component is 1.0 We see that the first two principal components are orthogonal and the first principal component is also a unit vector. You can check other pairs of principal components in order to convince yourself that all principal components are always pairwise orthogonal unit vectors. Visualizing PCA results In [12]: # to plot vectors from the center vecs = pca . components_ [ 0 : 10 ] . T * 2 fig , ax = plt . subplots ( figsize = ( 16 , 8 )) ax . plot ( quant_df_pca [:, 0 ], quant_df_pca [:, 1 ], 'ok' , markersize = 4 ) ax . set_xlabel ( 'Principal Component 1' ) ax . set_ylabel ( 'Principal Component 2' ) ax . set_title ( 'Cars Dataset plotted using first 2 Principal Components' , fontsize = 20 ) # plotting arrowheads of the original axes projected on the 2D PCA space for i , vec in enumerate ( vecs ): ax . arrow ( 0 , 0 , vec [ 0 ], vec [ 1 ], color = 'brown' , head_width = 0.1 ) s = 1.3 ax . annotate ( quant_df . columns [ i ], ( s * vec [ 0 ], s * vec [ 1 ]), color = 'brown' ) # annotating text for i , txt in enumerate ( cars_df_scaled [ 'model' ]): ax . annotate ( txt , ( quant_df_pca [:, 0 ][ i ], quant_df_pca [:, 1 ][ i ]), size = 12 ) Any patterns of interest? Let us examine the geography more closely. Source: this blog . In [13]: country = [ \"Japan\" , \"US\" , \"EU\" , \"US\" , \"EU\" , \"Japan\" , \"US\" , \"EU\" , \"US\" , \"EU\" ] times = [ 3 , 4 , 7 , 3 , 1 , 3 , 4 , 3 , 1 , 3 ] country_list = np . array ( sum (([ x ] * y for x , y in zip ( country , times )),[])) In [14]: fig , ax = plt . subplots ( figsize = ( 16 , 8 )) # main plot ax . plot ( quant_df_pca [:, 0 ], quant_df_pca [:, 1 ], 'ok' , markersize = 4 ) ax . set_xlabel ( 'Principal Component 1' ) ax . set_ylabel ( 'Principal Component 2' ) ax . set_title ( 'Cars Dataset plotted using first 2 Principal Components' , fontsize = 20 ) # plotting arrowheads for i , vec in enumerate ( vecs ): ax . arrow ( 0 , 0 , vec [ 0 ], vec [ 1 ], color = 'brown' , head_width = 0.05 ) s = 1.3 ax . annotate ( quant_df . columns [ i ], ( s * vec [ 0 ], s * vec [ 1 ]), color = 'brown' ) # plotting names cs = [ sns . xkcd_rgb [ \"magenta\" ], sns . xkcd_rgb [ \"denim blue\" ], sns . xkcd_rgb [ \"medium green\" ]] colors = { \"Japan\" : cs [ 0 ], \"US\" : cs [ 1 ], \"EU\" : cs [ 2 ]} # dummy plots to show up in the legend ax . plot ( 0 , 0 , color = cs [ 0 ], label = 'Japan' ) ax . plot ( 0 , 0 , color = cs [ 1 ], label = 'US' ) ax . plot ( 0 , 0 , color = cs [ 2 ], label = 'EU' ) # plotting text with color for i , txt in enumerate ( cars_df_scaled [ 'model' ]): country = country_list [ i ] ax . annotate ( txt , ( quant_df_pca [:, 0 ][ i ], quant_df_pca [:, 1 ][ i ]), color = colors [ country ], size = 12 ) ax . legend ( fontsize = 15 ); What patterns do you see now? For example, looking at the axes, we see that the American cars are characterized by high values for number of cyl , disp , and wt . Thus they are more powerful and heavier cars. Japanese cars, on the other hand, are characterized by higher fuel efficieny ( mpg ). European cars are somewhat in the middle and less tightly clustered than either group, but are average more efficient than American cars We can draw conclusions visually. Not any modeling so far. Addressing n_components PCA takes in 1 parameter: n_components . This defines the number of principal components that PCA will use. By default, the number of the used principal components is the minimum of the number of rows and the number of columns in the dataset min(p, n) . This is the maximum number of dimensions that PCA transform is allowed to use; For more details check the advanced section. Note : Setting the default parameter for n_components and taking the top k principal components is equivalent to setting n_components=k . The former is computationally more expensive though. Let's check this. In [15]: old_components = pca . components_ [ 0 : 2 ] # doing pca with 2 components pca2 = PCA ( n_components = 2 ) . fit ( quant_df ) new_components = pca2 . components_ # checking equivalence print ( new_components . all () == old_components . all ()) True Break Out Room 1 Visualizing high-dimensional data in a 2D plot Perform PCA Explore the \"Explained Variance Ratio\" Keep the first two principal components and visualize the data in this low-dimensional 2D space. Explore the \"cumulative explained variance ratio\" In [145]: dataset = datasets . load_iris () print ( 'classes ' , dataset . target_names ) print ( 'predictors' , dataset . feature_names ) classes ['setosa' 'versicolor' 'virginica'] predictors ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] In [146]: # pd.DataFrame(dataset.data).describe() In [147]: iris_scaled = StandardScaler () . fit_transform ( dataset . data ) In [148]: # pd.DataFrame(iris_scaled).describe() In [150]: # %load '../solutions/breakout_1_sol.py' As a reminder here is a plot from Section 5, where we outlined the data by using two predictors. Think about the difference between LASSO and PCA as dimensional reduction methods. PCA to speed up classification of Handwritten Digits This example, using the MNIST dataset , was borrowed from this Towards Data Science blog post . In this example, we will be classifying hand-written digits. Data Loading and EDA IMPORTANT: Unless you have the \"TF version 2.0.0\" try the following pip install --upgrade pip pip install tensorflow==2.0.0 pip3 install keras OR conda install tensorflow=2.0.0 Verify the installation by displaying the package information: In [200]: ! pip3 show keras Name: Keras Version: 2.3.0 Summary: Deep Learning for humans Home-page: https://github.com/keras-team/keras Author: Francois Chollet Author-email: francois.chollet@gmail.com License: MIT Location: /home/marios/anaconda3/envs/cs109a/lib/python3.7/site-packages Requires: six, scipy, keras-applications, pyyaml, h5py, numpy, keras-preprocessing Required-by: In [117]: # we'll use keras a lot more in the last few weeks of the course from keras.datasets import mnist ( x_train , y_train ), ( x_test , y_test ) = mnist . load_data () print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) An alternative way to load the MNIST dataset In [4]: # unzip the minst.zip file ! unzip mnist.zip #the data are saved in a dictionary with pickle. You can import the data as follows: import pickle with open ( 'mnist.pickle' , 'rb' ) as handle : mnist_data = pickle . load ( handle ) ( x_train , y_train ), ( x_test , y_test ) = mnist_data [ \"data\" ] print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) Archive: mnist.zip inflating: mnist.pickle (60000, 28, 28) (60000,) (10000, 28, 28) (10000,) Our training set ( x_train ) contains 60000 images of size 28 by 28. Our training labels ( y_train ) are numbers from 0 to 9. Let's examine one of these values below. In [118]: print ( y_train [ 0 ]) fig , ax = plt . subplots () ax . grid ( None ) ax . imshow ( x_train [ 0 ], cmap = 'gray' ); 5 Our task is to classify the test set digits as accurately as possible. The shape of the training set is $6000 \\times 28 \\times 28$. We have not dealt with 3D arrays before. We will deal with images in greater detail (and not only!!!) in the follow-up course, CS109b , if you are interested in doing more of this kind of stuff you should take this course. For now, we will reshape the array into a 2D array of shape $6000\\times 784$. In [119]: x_train = x_train . reshape ( x_train . shape [ 0 ], 784 ) x_test = x_test . reshape ( x_test . shape [ 0 ], 784 ) # check if the shapes are ok print ( x_train . shape , y_train . shape , x_test . shape , y_test . shape ) (60000, 784) (60000,) (10000, 784) (10000,) Normalizing Data Image data is usually between 0 and 255 (0 indicates a black pixel and 255 indicates a white pixel). We can normalize these values by dividing by 255. Why do we prefer normalization instead of standardization in this case? In [120]: # checking the min and max of x_train and x_test print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) x_train = ( x_train - x_train . min ()) / ( x_train . max () - x_train . min ()) x_test = ( x_test - x_train . min ()) / ( x_train . max () - x_train . min ()) print ( x_train . min (), x_train . max (), x_test . min (), x_test . max ()) 0 255 0 255 0.0 1.0 0.0 255.0 Modeling using Logistic Regression In [90]: start = time () #‘lbfgs' solver handles multinomial loss in multiclass problems logreg_model = LogisticRegression ( solver = 'lbfgs' ) . fit ( x_train , y_train ) end = time () full_logreg_time = end - start print ( 'Time to fit: {} s' . format ( full_logreg_time )) Time to fit: 13.416306972503662s In [121]: y_preds_train = logreg_model . predict ( x_train ) y_preds_test = logreg_model . predict ( x_test ) full_logreg_score_train = accuracy_score ( y_train , y_preds_train ) full_logreg_score_test = accuracy_score ( y_test , y_preds_test ) # Evaluation print ( 'Training Set Score: {} ' . format ( full_logreg_score_train )) print ( 'Test Set Score: {} ' . format ( full_logreg_score_test )) Training Set Score: 0.9350666666666667 Test Set Score: 0.8699 In [122]: # get performance by class pd . crosstab ( y_test , y_preds_test , margins = True , rownames = [ 'Actual' ], colnames = [ 'Predicted' ]) Out[122]: Predicted 0 1 2 3 4 5 6 7 8 9 All Actual 0 968 0 0 2 0 0 4 1 5 0 980 1 0 1025 2 4 0 0 3 2 99 0 1135 2 11 4 872 22 5 0 16 3 95 4 1032 3 5 0 9 916 1 1 3 7 62 6 1010 4 3 0 5 3 874 0 12 1 35 49 982 5 17 1 1 81 7 410 18 3 338 16 892 6 13 2 3 3 5 1 918 1 12 0 958 7 4 4 20 17 8 0 0 839 28 108 1028 8 6 1 2 12 5 1 5 1 939 2 974 9 10 1 0 8 12 0 0 0 40 938 1009 All 1037 1038 914 1068 917 413 979 858 1653 1123 10000 We get a high training and test set score but it takes a relatively long time to fit a model. Let us see if we can speed things up when using PCA Logistic Regression Model after PCA In [140]: # Do PCA onto our training set and inspect pca = PCA ( n_components = 100 ) . fit ( x_train ) fig , ax = plt . subplots ( ncols = 2 , figsize = ( 20 , 6 )) ax1 , ax2 = ax . ravel () ratio = pca . explained_variance_ratio_ ax1 . plot ( range ( 1 , len ( ratio ) + 1 ), ratio , 'o-' ) ax1 . set_title ( 'Explained Variance Ratio PCA' , fontsize = 20 ) ax1 . set_ylabel ( 'Explained Variance Ratio' ) ratio = pca . explained_variance_ratio_ ax2 . plot ( range ( 1 , len ( ratio ) + 1 ), np . cumsum ( ratio ), 'o-' ) ax2 . set_title ( 'Cumulative Sum of Explained Variance Ratio PCA' , fontsize = 20 ) ax2 . set_ylabel ( 'Cumulative Sum of Explained Variance Ratio' ); We see that the first 100 principal components hold over 90% of the variance and the first 50 principal components hold over 80% of the variance! We have significantly reduced the dimensionality of our problem! Let us use PCA to find the first 100 principal components of our dataset and transform our x_train and x_test accordingly. In [124]: x_train_pca = pca . transform ( x_train ) x_test_pca = pca . transform ( x_test ) print ( x_train_pca . shape , x_test_pca . shape ) (60000, 100) (10000, 100) In [125]: start = time () logreg_model_pca = LogisticRegression ( solver = 'lbfgs' ) . fit ( x_train_pca , y_train ) end = time () print ( 'Time to fit model (100 PCs): {} s' . format ( end - start )) print ( 'Time to fit model (full dataset): {} s' . format ( full_logreg_time )) print ( 'So to fit the model with the full dataset is about' , np . round ( full_logreg_time / ( end - start ), 0 ), ' times slower than using PCA' ) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . bar ( 0 , full_logreg_time , width = 0.5 ) ax . bar ( 1 , end - start , width = 0.5 ) ax . set_xlabel ( 'Model' ) ax . set_xticks ([ 0 , 1 ]) ax . set_xticklabels ([ 'Full Dataset' , '100 PCs' ]) ax . set_ylabel ( 'Time to Fit Model (s)' ) ax . set_title ( 'Time taken to fit different models (s)' ); Time to fit model (100 PCs): 3.536935567855835s Time to fit model (full dataset): 13.416306972503662s So to fit the model with the full dataset is about 4.0 times slower than using PCA Note: The time taken to fit our model is considerably smaller! Now let us check our accuracy In [126]: y_preds_train_pca = logreg_model_pca . predict ( x_train_pca ) y_preds_test_pca = logreg_model_pca . predict ( x_test_pca ) # Evaluation print ( 'Training Set Score (100 PCs): {} ' . format ( accuracy_score ( y_train , y_preds_train_pca ))) print ( 'Test Set Score (100 PCs): {} \\n ' . format ( accuracy_score ( y_test , y_preds_test_pca ))) print ( 'Training Set Score (full dataset): {} ' . format ( full_logreg_score_train )) print ( 'Test Set Score (full dataset): {} ' . format ( full_logreg_score_test )) Training Set Score (100 PCs): 0.9221 Test Set Score (100 PCs): 0.8543 Training Set Score (full dataset): 0.9350666666666667 Test Set Score (full dataset): 0.8699 In [127]: # get performance by class pd . crosstab ( y_test , y_preds_test_pca , margins = True , rownames = [ 'Actual' ], colnames = [ 'Predicted' ]) Out[127]: Predicted 0 1 2 3 4 5 6 7 8 9 All Actual 0 968 0 0 1 0 0 4 1 6 0 980 1 0 1003 1 4 0 0 4 1 121 1 1135 2 16 5 850 17 7 0 14 5 112 6 1032 3 6 1 6 901 0 0 2 6 84 4 1010 4 3 0 3 2 889 0 11 1 37 36 982 5 35 1 4 77 12 320 22 3 407 11 892 6 24 2 5 1 7 1 906 1 11 0 958 7 4 3 17 15 8 0 0 865 30 86 1028 8 5 0 0 13 4 0 4 1 946 1 974 9 10 3 0 6 31 0 0 3 61 895 1009 All 1071 1018 886 1037 958 321 967 887 1815 1040 10000 Not a significant drop in accuracy!! But, since we are losing information by not accounting for all the variance, we are faced with a speed accuracy tradeoff. You should explore the case of keeping less principal components. Plotting PCA Plotting the Reconstructed Image In [138]: fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 8 , 4 )) ax1 , ax2 = ax . ravel () ax1 . imshow ( x_train [ 0 ] . reshape ( 28 , 28 ), cmap = 'gray' ) ax1 . grid ( None ) ax1 . set_title ( 'Original Image with 784 components' ) ax2 . imshow ( x_train_pca [ 1 ] . reshape ( 10 , 10 ), cmap = 'gray' ) ax2 . grid ( None ) ax2 . set_title ( 'Image after PCA with 100 components' ) fig . tight_layout () Uhhh... this is terrible. But we can use PCA to carry out an inverse transform in order to get a reconstructed image. Let's try again, using pca.inverse_transform() ! Source: this github repo In [144]: img_reconstructed = pca . inverse_transform ( x_train_pca [ 0 ]) fig , ax = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 8 , 4 )) ax1 , ax2 = ax . ravel () ax1 . imshow ( x_train [ 0 ] . reshape ( 28 , 28 ), cmap = 'gray' ) ax1 . grid ( None ) ax1 . set_title ( 'Original Image with 784 components' ) ax2 . imshow ( img_reconstructed . reshape ( 28 , 28 ), cmap = 'gray' ) ax2 . grid ( None ) ax2 . set_title ( 'Reconstructed Image after PCA with 100 components' ) fig . tight_layout () Plotting all our points on a 2-dimensional plot given by the first 2 principal components of PCA This towards data science article has a few similar plots that are pretty cool! In [142]: pca_2 = PCA ( n_components = 2 ) . fit ( x_train ) x_train_2 = pca_2 . transform ( x_train ) print ( x_train_2 . shape ) (60000, 2) In [143]: fig , ax = plt . subplots ( figsize = ( 16 , 8 )) for i in range ( 10 ): indices = np . where ( y_train == i )[ 0 ] data = x_train_2 [ indices ] ax . plot ( data [:, 0 ], data [:, 1 ], 'o' , label = ' {} ' . format ( i ), alpha = 0.5 ) ax . set_title ( 'First 2 Principal Components of MNIST Data' , fontsize = 20 ) ax . set_xlabel ( 'Principal Component 1' ) ax . set_ylabel ( 'Principal Component 2' ) ax . legend (); Any patterns of interest? Break Out Room 2 Lets perform classification by using reduced dimensional data-space. Specifically compare the classification score when we use: (a) all the predictors; (b) 2 of the original predictors; (c) 2 principal components as predictors. Use all the 4 predictors In [132]: X_train , X_test , y_train_iris , y_test_iris = train_test_split ( iris_scaled , dataset . target , test_size = 0.4 , random_state = 42 ) print ( 'Shapes for X and y training sets:' , X_train . shape , y_train_iris . shape ) print ( 'Shapes for X and y testing sets:' , X_test . shape , y_test_iris . shape ) #Training model_logistic = LogisticRegression ( C = 100 ) . fit ( X_train , y_train_iris ) #Predict y_pred_train = model_logistic . predict ( X_train ) y_pred_test = model_logistic . predict ( X_test ) #Performance Evaluation train_score = accuracy_score ( y_train_iris , y_pred_train ) * 100 test_score = accuracy_score ( y_test_iris , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) Shapes for X and y training sets: (90, 4) (90,) Shapes for X and y testing sets: (60, 4) (60,) Training Set Accuracy: 98.88888888888889% Testing Set Accuracy: 98.33333333333333% Use the 2 predictors In [133]: # Choose the first two predictors (sepal length and sepal width) iris_2predictors = iris_scaled [:, 0 : 2 ] X_train , X_test , y_train_iris , y_test_iris = train_test_split ( iris_2predictors , dataset . target , test_size = 0.4 , random_state = 42 ) print ( 'Shapes for X and y training sets:' , X_train . shape , y_train_iris . shape ) print ( 'Shapes for X and y testing sets:' , X_test . shape , y_test_iris . shape ) #Training model_logistic = LogisticRegression ( C = 100 ) . fit ( X_train , y_train_iris ) #Predict y_pred_train = model_logistic . predict ( X_train ) y_pred_test = model_logistic . predict ( X_test ) #Perfromance Evaluation train_score = accuracy_score ( y_train_iris , y_pred_train ) * 100 test_score = accuracy_score ( y_test_iris , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) Shapes for X and y training sets: (90, 2) (90,) Shapes for X and y testing sets: (60, 2) (60,) Training Set Accuracy: 78.88888888888889% Testing Set Accuracy: 85.0% Use all the 2 principal componets as predictors Perform a PCA transform with 2 principal components Use the 2 principal components for a logistic regression Print the score for the training and testing sets Compare with the previous two cases In [134]: # Your code here In [137]: # %load '../solutions/breakout_2_sol.py' Should I always use PCA? PCA is great for: Speeding up the training of a model without significantly decreasing the predictive ability relative to a model with all p predictors. Visualizing how predictive your features can be of your response, especially in the case of classification. Reducing multicollinearity, and thus potentially improving the computational time required to fit models. Reducing dimensionality in very high dimensional settings. PCA is not so good in certain situations because: Interpretation of coefficients in PCA is completely lost. So do not do PCA if interpretation is important. When the predictors' distribution deviates significantly from a multivariable Normal distribution. When the high variance does not indicate high importance. When the hidden dimensions are not orthonormal. Assumptions of PCA Linear change of basis: PCA is a linear transformation from a Euclidean basis (defined by the original predictors) to an abstract orthonormal basis. Hence, PCA assumes that such a linear change of basis is sufficient for identifying degrees of freedom and conducting dimensionality reduction Mean/variance are sufficient (data are approximately multi-variate gaussian): In applying PCA to our data, we are only using the means (for standardizing) and the covariance matrix that are associated with our predictors. Thus, PCA assumes that such statistics are sufficient for describing the distributions of the predictor variables. This is true only if the predictors are drawn jointly from a multivariable Normal distribution. When the predictor distributions heavily violate this assumption, PCA components may not be as informative. High variance indicates high importance: This fundamental assumption is intuitively reasonable, since components corresponding to low variability likely say little about the data, but this is not always true. Principal components are orthogonal: PCA explicitly assumes that intrinsic (abstract) dimensions are orthogonal, which may not be true. However, this allowes us to use techniques from linear algebra such as the spectral decomposition and thereby, simplify our calculations. End of Standard Section","tags":"sections","url":"sections/sec_6/"},{"title":"S-Section 06: PCA and Logistic Regression","text":"Jupyter Notebooks S-Section 6: PCA and Logistic Regression","tags":"sections","url":"sections/section6/"},{"title":"Advanced Section 4: Mathematical Foundations  of PCA","text":"Slides A-Section 4: PCA [PDF] A-Section 4: PCA [PPTX] Notes A-Section 4: PCA (Notes) [PDF]","tags":"a-sections","url":"a-sections/a-section4/"},{"title":"Lecture 20: PCA","text":"In [0]: % matplotlib inline import sys import numpy as np import pylab as pl import pandas as pd import sklearn as sk import statsmodels.api as sm import matplotlib.pyplot as plt import matplotlib import seaborn as sns from sklearn.linear_model import LogisticRegression from sklearn.decomposition import PCA import sklearn.metrics as met pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) PCA Part 0: Reading the data In this notebook, we will be using the same Heart dataset from last lecture. As a reminder the variables we will be using today include: AHD : whether or not the patient presents atherosclerotic heart disease (a heart attack): Yes or No Sex : a binary indicator for whether the patient is male (Sex=1) or female (Sex=0) Age : age of patient, in years MaxHR : the maximum heart rate of patient based on exercise testing RestBP : the resting systolic blood pressure of the patient Chol : the HDL cholesterol level of the patient Oldpeak : ST depression induced by exercise relative to rest (on an ECG) Slope : the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping) Ca : number of major vessels (0-3) colored by flourosopy For further information on the dataset, please see the UC Irvine Machine Learning Repository . In [0]: df_heart = pd . read_csv ( 'Heart.csv' ) # Force the response into a binary indicator: df_heart [ 'AHD' ] = 1 * ( df_heart [ 'AHD' ] == \"Yes\" ) print ( df_heart . shape ) df_heart . head () Part 1: Principal Components Analysis (PCA) Q1.1 Just a sidebar (and a curiosity), what happens when two of the identical predictor is used in logistic regression? Is an error created? Should one be? Investigate by predicting AHD from two copies of Age , and compare to the simple logistic regression model with Age alone. In [0]: y = df_heart [ 'AHD' ] logit1 = LogisticRegression ( penalty = \"none\" , solver = \"lbfgs\" ) . fit ( df_heart [[ 'Age' ]], y ) # investigating what happens when two identical predictors of 'Age' are used logit2 = LogisticRegression ( penalty = \"none\" , solver = \"lbfgs\" ) . fit ( ___ , y ) print ( \"The coef estimate for Age (when in the model once):\" , logit1 . coef_ ) print ( \"The coef estimates for Age (when in the model twice):\" , logit2 . coef_ ) your answer here We will apply PCA to the heart dataset when there are just 4 predictors considered (remember: PCA is used when dimensionality is high (lots of predictors), but this will help us get our heads around what is going on): In [0]: # For pedagogical purposes, let's simplify our lives and use just 7 predictors X = df_heart [[ 'Age' , 'RestBP' , 'Chol' , 'MaxHR' , 'Sex' , 'Oldpeak' , 'Slope' ]] y = df_heart [ 'AHD' ] X . describe () In [0]: # Here is the table of correlations between our predictors. This will be useful later on X . corr () Q1.2 Is there any evidence of multicollinearity in the set of predictors? How do you know? How will PCA handle these correlations? your answer here Next we apply the PCA transformation in a few steps, and show some of the results below: In [0]: # create/fit the 'full' pca transformation pca = PCA () . fit ( X ) # apply the pca transformation to the full predictor set pcaX = pca . transform ( X ) # convert to a data frame pcaX_df = pd . DataFrame ( pcaX , columns = [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' , 'PCA7' ]]) # here are the weighting (eigen-vectors) of the variables (first 2 at least) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 ,:]) print ( \"Second PCA Component (w2):\" , pca . components_ [ 1 ,:]) # here is the variance explained: print ( \"Variance explained by each component:\" , pca . explained_variance_ratio_ ) Q1.3 Now try the PCA decompositon on the standardized version of X instead. In [0]: ### edTest(test_pcaZ) ### # create/fit the standardized version of X Z = sk . preprocessing . StandardScaler () . fit ( ___ ) . transform ( ___ ) # create/fit the 'full' pca transformation on Z pca_standard = PCA () . fit ( ___ ) pcaZ = pca_standard . transform ( ___ ) # convert to a data frame pcaZ_df = pd . DataFrame ( ___ , columns = [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' , 'PCA7' ]]) In [0]: # Let's look at them to see what they are comprised of: pd . DataFrame . from_dict ({ 'Variable' : X . columns , 'PCA1' : pca . components_ [ 0 ], 'PCA2' : pca . components_ [ 1 ], 'PCA-Z1' : pca_standard . components_ [ 0 ], 'PCA-Z2' : pca_standard . components_ [ 1 ]}) Q1.3 Interpret the results above. What doss $w_1$ represent based on the untransformed data? What doss $w_1$ represent based on the standardized data? Which is a better representation of the data? your answer here In [0]: np . sum ( pca . components_ [ 0 ,:] ** 2 ) Q1.4 It is common for a model with high dimensional data (lots of predictors) to be plotted along the first 2 PCA components (with the classification boundaries added). Below is the scatter plot for these data (without a classificaiton boundary, since we do not have a model yet) for the unstandardized PCA. Repeat this for the standardized PCA: In [0]: # Plot the response over the first 2 PCA component vectors fig ,( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 12 , 5 )) ax1 . scatter ( pcaX_df [[ 'PCA1' ]][ y == 0 ], pcaX_df [[ 'PCA2' ]][ y == 0 ]) ax1 . scatter ( pcaX_df [[ 'PCA1' ]][ y == 1 ], pcaX_df [[ 'PCA2' ]][ y == 1 ]) ax1 . legend ([ \"AHD = No\" , \"AHD = Yes\" ]) ax1 . set_xlabel ( \"First PCA Component Vector\" ) ax1 . set_ylabel ( \"Second PCA Component Vector\" ); Q2.4 Does there appear to be good potential here? Which form of the data appear to provide better information as to seperate the classes in the response? What at would a classification boundary look like if a logistic regression model were fit using the first 2 principal components as the predictors? your answer here Part 2: PCA in Regression (PCR) First let's fit the full logistic regression model to predict AHD from the 7 predictors above. Remember: PCA is an approach to handling the predictors (unsupervised), so it does not matter if we are using it for a regression or classification type problem. In [0]: #fit the 'full' model on the 7 predictors. and print out the coefficients logit_full = LogisticRegression ( penalty = \"none\" , solver = \"lbfgs\" , max_iter = 2000 ) . fit ( X , y ) beta = logit_full . coef_ [ 0 ] print ( beta ) Q2.1 Fit the logistic model on the first PCA vector below and get some relevant output In [0]: ### edTest(test_pcr1) ### logit_pcr1 = LogisticRegression ( penalty = 'none' , solver = \"lbfgs\" ) . fit ( ___ , y ) print ( \"Intercept from simple PCR-Logistic:\" , logit_pcr1 . intercept_ ) print ( \"'Slope' from simple PCR-Logistic:\" , logit_pcr1 . coef_ ) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 : 1 ,:]) Q2.2 What does this PCR-1 model tell us about how the predictors relate to the response (aka, estimate the coefficient(s) in the original predictor space)? Is it truly a simple logistic regression model in the original predictor space? In [0]: # your code here: do a multiplication of pcr_1's coefficients times the first component vector from PCA ( logit_pcr1 . coef_ * pca . components_ [ 0 : 1 ,:]) your answer here Here is the above claculation fora few up to the 7th PCR logistic regression, and then plotted on a 'pretty' plot: In [0]: # Fit the other 3 PCRs on the rest of the 7 predictors #pcaX_df.iloc[:,np.arange(0,5)].head() logit_pcr2 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' ]], y ) logit_pcr3 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' ]], y ) logit_pcr4 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' ]], y ) logit_pcr5 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' ]], y ) logit_pcr6 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' ]], y ) logit_pcr7 = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaX_df , y ) pcr1 = ( logit_pcr1 . coef_ * np . transpose ( pca . components_ [ 0 : 1 ,:])) . sum ( axis = 1 ) pcr2 = ( logit_pcr2 . coef_ * np . transpose ( pca . components_ [ 0 : 2 ,:])) . sum ( axis = 1 ) pcr3 = ( logit_pcr3 . coef_ * np . transpose ( pca . components_ [ 0 : 3 ,:])) . sum ( axis = 1 ) pcr4 = ( logit_pcr4 . coef_ * np . transpose ( pca . components_ [ 0 : 4 ,:])) . sum ( axis = 1 ) pcr5 = ( logit_pcr5 . coef_ * np . transpose ( pca . components_ [ 0 : 5 ,:])) . sum ( axis = 1 ) pcr6 = ( logit_pcr6 . coef_ * np . transpose ( pca . components_ [ 0 : 6 ,:])) . sum ( axis = 1 ) pcr7 = ( logit_pcr7 . coef_ * np . transpose ( pca . components_ [ 0 : 7 ,:])) . sum ( axis = 1 ) results = np . vstack (( pcr1 , pcr2 , pcr3 , pcr4 , pcr5 , pcr6 , pcr7 , beta )) print ( results ) In [0]: plt . plot ([ 'PCR1' , 'PCR2' , 'PCR3' , 'PCR4' , 'PCR5' , 'PCR6' , 'PCR7' , 'Logistic' ], results ) plt . ylabel ( \"Back-calculated Beta Coefficients\" ); #plt.legend(X.columns); Q2.3 Interpret the plot above. Specifically, compare how each PCA vector \"contributes\" to the original logistic regression model using all 7 original predictors. How Does PCR-4 compare to the original logistic regression model (in estimated coefficients)? your answer here All of this PCA work should have been done using the standardized versions of the predictors. Below is the code that does exactly that: In [0]: scaler = sk . preprocessing . StandardScaler () scaler . fit ( X ) Z = scaler . transform ( X ) pca = PCA ( n_components = 7 ) . fit ( Z ) pcaZ = pca . transform ( Z ) pcaZ_df = pd . DataFrame ( pcaZ , columns = [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' , 'PCA5' , 'PCA6' , 'PCA7' ]]) print ( \"First PCA Component (w1):\" , pca . components_ [ 0 ,:]) print ( \"Second PCA Component (w2):\" , pca . components_ [ 1 ,:]) In [0]: #fit the 'full' model on the 4 predictors. and print out the coefficients logit_full = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( Z , y ) betaZ = logit_full . coef_ [ 0 ] print ( \"Logistic coef. on standardized predictors:\" , betaZ ) In [0]: # Fit the PCR logit_pcr1Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' ]], y ) logit_pcr2Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' , 'PCA2' ]], y ) logit_pcr3Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' , 'PCA2' , 'PCA3' ]], y ) logit_pcr4Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df [[ 'PCA1' , 'PCA2' , 'PCA3' , 'PCA4' ]], y ) logit_pcr7Z = LogisticRegression ( C = 1000000 , solver = \"lbfgs\" ) . fit ( pcaZ_df , y ) pcr1Z = ( logit_pcr1Z . coef_ * np . transpose ( pca . components_ [ 0 : 1 ,:])) . sum ( axis = 1 ) pcr2Z = ( logit_pcr2Z . coef_ * np . transpose ( pca . components_ [ 0 : 2 ,:])) . sum ( axis = 1 ) pcr3Z = ( logit_pcr3Z . coef_ * np . transpose ( pca . components_ [ 0 : 3 ,:])) . sum ( axis = 1 ) pcr4Z = ( logit_pcr4Z . coef_ * np . transpose ( pca . components_ [ 0 : 4 ,:])) . sum ( axis = 1 ) pcr7Z = ( logit_pcr7Z . coef_ * np . transpose ( pca . components_ )) . sum ( axis = 1 ) resultsZ = np . vstack (( pcr1Z , pcr2Z , pcr3Z , pcr4Z , pcr7Z , betaZ )) print ( resultsZ ) plt . plot ([ 'PCR1-Z' , 'PCR2-Z' , 'PCR3-Z' , 'PCR4-Z' , 'PCR7-Z' , 'Logistic' ], resultsZ ) plt . ylabel ( \"Back-calculated Beta Coefficients\" ); plt . legend ( X . columns ); Q2.4 Compare this plot to the previous one; why does this plot make sense?. What does this illustrate? your answer here In [0]:","tags":"labs","url":"labs/lecture-20/notebook/"},{"title":"Lecture 22: Classification Trees","text":"In [0]: import numpy as np import pandas as pd import sklearn as sk import matplotlib.pyplot as plt import seaborn as sns from sklearn.tree import DecisionTreeClassifier from sklearn import tree from sklearn.model_selection import cross_val_score pd . set_option ( 'display.width' , 100 ) pd . set_option ( 'display.max_columns' , 20 ) plt . rcParams [ \"figure.figsize\" ] = ( 12 , 8 ) Part 0: Reading and Exploring the data We will be using the county_election dataset (provided separately as train and test versions for you) to model the outcome of the 2016 presidential election (Did Trump or Clinton win each county?) from various predictors. We start by reading in the datasets for you and visualizing the main predictors for now: minority : Important note: use the training dataset for all exploratory analysis and model fitting. Only use the test dataset to evaluate and compare models. In [0]: elect_train = pd . read_csv ( \"data/county_election_train.csv\" ) elect_test = pd . read_csv ( \"data/county_election_test.csv\" ) elect_train . head () In [0]: # let's create the response variable and summarize it y_train = 1 * ( elect_train [ 'trump' ] > elect_train [ 'clinton' ]) y_test = 1 * ( elect_test [ 'trump' ] > elect_test [ 'clinton' ]) print ( \"The proportion of counties that favored Trump over Clinton in 2016 was:\" , ' %.4g ' % np . mean ( y_train ) ) Let's look at the main predictor's distribution via boxplots: and consider what the log-transformed version of it looks like: In [0]: fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = [ 15 , 6 ]) ax1 . boxplot ([ elect_train . loc [ y_train == 0 ][ 'minority' ], elect_train . loc [ y_train == 1 ][ 'minority' ]], labels = ( \"Clinton\" , \"Trump\" )) ax1 . set_ylabel ( \"Proportion of residents that are minorities\" ) ax2 . boxplot ([ np . log ( elect_train . loc [ y_train == 0 ][ 'minority' ]), np . log ( elect_train . loc [ y_train == 1 ][ 'minority' ])], labels = ( \"Clinton\" , \"Trump\" )) ax2 . set_ylabel ( \"Proportion of residents that are minorities\" ) plt . show () Q0.1 How would you describe the distribution of the variable minority ? What issues does this create in logistic regression, $k$-NN, and Decision Trees? How can these issues be fixed? Which of the two versions of 'minority' would be a better choice to use as a predictor for inference? For prediction? your answer here Part 1: Decision Trees We could use a simple Decision Tree regressor to predict votergap. That's not the aim of this lab, so we'll run a few of these models without any cross-validation or 'regularization' just to illustrate what is going on. This is what you ought to keep in mind about decision trees. from the docs: max_depth : int or None, optional (default=None) The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. min_samples_split : int, float, optional (default=2) The deeper the tree, the more prone you are to overfitting. The smaller min_samples_split , the more the overfitting. One may use min_samples_leaf instead. More samples per leaf, the higher the bias (aka, simpler, underfit model). Below we fit 2 decision treees that limit the max_depth : a single split, and one with depth of 3 (resulting in 8 leaves). In [0]: elect_train [ 'logminority' ] = np . log ( elect_train [ 'minority' ]) elect_test [ 'logminority' ] = np . log ( elect_test [ 'minority' ]) dummy_x = np . arange ( np . min ( elect_train [ 'minority' ]), np . max ( elect_train [ 'minority' ]), 0.01 ) plt . plot ( elect_train [ 'minority' ], y_train , '.' ) for i in [ 1 , 3 ]: dtree = DecisionTreeClassifier ( max_depth = i ) dtree . fit ( elect_train [[ 'minority' ]], y_train ) plt . plot ( dummy_x , dtree . predict ( dummy_x . reshape ( - 1 , 1 )), label = ( \"Classifications, max depth =\" + str ( i )), alpha = 0.5 , lw = 4 ) plt . plot ( dummy_x , dtree . predict_proba ( dummy_x . reshape ( - 1 , 1 ))[:, 1 ], label = ( \"Probabilities, max depth =\" + str ( i )), alpha = 0.5 , lw = 2 ) plt . legend (); And the actual decision tree can be printed out using sklearn.tree.plot_tree : In [0]: from sklearn import tree plt . figure ( figsize = ( 16 , 8 )) tree . plot_tree ( dtree , filled = True ) plt . show () Q1.1 Interpret the printed out tree above: how does it match the scatterplot visualization of the tree? your answer here Q1.2 Play around with the various arguments to define the complexity of the decision tree: max_depth , min_samples_split , and min_samples_leaf (do 1 at a time for now, you can use multiple of these arguments). Roughly, at what point do these start to overfit? In [0]: plt . plot ( elect_train [ 'minority' ], y_train , '.' ) for i in [ 1 , 30 , 100 ]: dtree = DecisionTreeClassifier ( min_samples_leaf = i ) dtree . fit ( elect_train [[ 'minority' ]], y_train ) plt . plot ( dummy_x , dtree . predict ( dummy_x . reshape ( - 1 , 1 )), label = ( \"min leaf size =\" + str ( i )), alpha = 0.8 , lw = 4 ) plt . legend (); In [0]: tree . plot_tree ( dtree , filled = True ) plt . show () *your answer here* Let's take this to the 2-dimensional feature/predictor set: also include bachelor \" the proportion of residents with at least a bachelor's degree. Let's start by visualizing the data: In [0]: plt . scatter ( elect_train [ 'minority' ][ y_train == 1 ], elect_train [ 'bachelor' ][ y_train == 1 ], marker = \".\" , color = \"green\" , label = \"Trump\" ) plt . scatter ( elect_train [ 'minority' ][ y_train == 0 ], elect_train [ 'bachelor' ][ y_train == 0 ], marker = \".\" , color = \"purple\" , label = \"Clinton\" ) plt . xlabel ( \"minority\" ) plt . ylabel ( \"bachelor\" ) plt . legend () plt . show () Q1.3 Based on the scatterplot above, does there appear to be good separability between the two classes? If you were to create a single box around the points to separate the 2 classes, where would you draw the box (a decision tree with max_depth=2 ? your answer here Q1.4 Create two decision tree classifiers below: one with max_depth=2 and one with max_depth=10 ? In [0]: ### edTest(test_dtrees) ### dtree2 = ___ dtree10 = ___ Let's plot the decision boundaries for these two trees (code provided for you below). In [0]: x1_min , x1_max = elect_train [ 'minority' ] . min () - 1 , elect_train [ 'minority' ] . max () + 1 x2_min , x2_max = elect_train [ 'bachelor' ] . min () - 1 , elect_train [ 'bachelor' ] . max () + 1 x1x , x2x = np . meshgrid ( np . arange ( x1_min , x1_max , 0.1 ), np . arange ( x2_min , x2_max , 0.1 )) yhat2 = dtree2 . predict ( np . c_ [ x1x . ravel (), x2x . ravel ()]) . reshape ( x1x . shape ) yhat10 = dtree10 . predict ( np . c_ [ x1x . ravel (), x2x . ravel ()]) . reshape ( x1x . shape ) fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = [ 15 , 6 ]) ax1 . contourf ( x1x , x2x , yhat2 , alpha = 0.2 , cmap = \"PiYG\" ); ax1 . scatter ( elect_train [ 'minority' ][ y_train == 1 ], elect_train [ 'bachelor' ][ y_train == 1 ], marker = \".\" , color = \"green\" , label = \"Trump\" ) ax1 . scatter ( elect_train [ 'minority' ][ y_train == 0 ], elect_train [ 'bachelor' ][ y_train == 0 ], marker = \".\" , color = \"purple\" , label = \"Clinton\" ) ax1 . set_xlabel ( \"minority\" ) ax1 . set_ylabel ( \"bachelor\" ) ax1 . set_title ( \"Decision Tree with max_depth=2\" ) ax1 . legend () ax2 . contourf ( x1x , x2x , yhat10 , alpha = 0.2 , cmap = \"PiYG\" ); ax2 . scatter ( elect_train [ 'minority' ][ y_train == 1 ], elect_train [ 'bachelor' ][ y_train == 1 ], marker = \".\" , color = \"green\" , label = \"Trump\" ) ax2 . scatter ( elect_train [ 'minority' ][ y_train == 0 ], elect_train [ 'bachelor' ][ y_train == 0 ], marker = \".\" , color = \"purple\" , label = \"Clinton\" ) ax2 . set_xlabel ( \"minority\" ) ax2 . set_ylabel ( \"bachelor\" ) ax2 . set_title ( \"Decision Tree with max_depth=10\" ) ax2 . legend () plt . show () Q1.4 How do these trees compare? Is there clear over or under fitting for either of these tree? In [0]: * your answer here * Q1.5 A larger X_train feature set is defined below with 8 predictors. Fit a decision tree with max_depth = 15 to this feature set and calculate the accuracy score on both the train and test sets. In [0]: ### edTest(test_dtree15) ### X_train = elect_train [[ 'minority' , 'density' , 'hispanic' , 'obesity' , 'female' , 'income' , 'bachelor' , 'inactivity' ]] X_test = elect_test [[ 'minority' , 'density' , 'hispanic' , 'obesity' , 'female' , 'income' , 'bachelor' , 'inactivity' ]] dtree15 = ___ dtree15_train_acc = ___ dtree15_test_acc = ___ print ( \"Train accuracy =\" , float ( ' %.4g ' % dtree15_train_acc ), \" \\n Test accuracy =\" , float ( ' %.4g ' % dtree15_test_acc )) Two plots are provided for you below to aid in interpreting this model (well, you have to fix the second one): The feature_importances_ the measures the total improvement (reduction) of the cost/loss/criterion every time that feature defines a split. Note: the default is criterion='gini . A \"predicted probability plot\" to get a very rough idea as to what the model is saying about how the chances of a county voting for Trump in 2016 were related to minority . In [0]: pd . Series ( dtree15 . feature_importances_ , index = list ( X_train )) . sort_values () . plot ( kind = \"barh\" ); Q1.6 Fix the spaghetti plot below so that it is at least a little interpretable. In [0]: ### edTest(test_spaghetti) ### ###Fix this spaghetti plot! Use `np.argsort` phat15 = dtree15 . predict_proba ( X_train )[:, 1 ] order = ___ minority_sorted = ___ phat15_sorted = ___ plt . scatter ( X_train [ 'minority' ], y_train ) plt . plot ( minority_sorted , phat15_sorted , alpha = 0.5 ) plt . show () Q1.7 Perform 5-fold cross-validation to determine what the best max_depth would be for a single regression tree using the entire X_train feature set defined below. Visualize the results with mean +/- 2 sd's across the validation sets. Interpret the result. In [0]: np . random . seed ( 109 ) depths = list ( range ( 1 , 21 )) train_scores = [] cvmeans = [] cvstds = [] cv_scores = [] for depth in depths : dtree = DecisionTreeClassifier ( max_depth = ___ ) # Perform 5-fold cross validation and store results train_scores . append ( dtree . fit ( ___ , ___ ) . score ( ___ , ___ )) scores = cross_val_score ( estimator = ___ , X = ___ , y = ___ , cv = ___ ) cvmeans . append ( scores . mean ()) cvstds . append ( scores . std ()) cvmeans = np . array ( cvmeans ) cvstds = np . array ( cvstds ) In [0]: # plot means and shade the 2*SD interval plt . plot ( depths , cvmeans , '*-' , label = \"Mean CV\" ) plt . fill_between ( depths , cvmeans - 2 * cvstds , cvmeans + 2 * cvstds , alpha = 0.3 ) ylim = plt . ylim () plt . plot ( depths , train_scores , '-+' , label = \"Train\" ) plt . legend () plt . ylabel ( \"Accuracy\" ) plt . xlabel ( \"Max Depth\" ) plt . xticks ( depths ); you answer here In [0]:","tags":"labs","url":"labs/lecture-22/notebook/"},{"title":"Lecture 20: PCA","text":"Slides PDF | Lecture 20: PCA Exercises Lecture 20: PCA [Notebook]","tags":"lectures","url":"lectures/lecture20/"},{"title":"Lecture 22: Classification Trees","text":"Slides PDF | Lecture 22: Classification Trees Exercises Lecture 22: Decision Tree Classification [Notebook]","tags":"lectures","url":"lectures/lecture22/"},{"title":"Lecture 19: Missing Data","text":"In [0]: % matplotlib inline from sklearn import datasets import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import scipy In [0]: from sklearn.linear_model import LinearRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.experimental import enable_iterative_imputer from sklearn.impute import SimpleImputer , KNNImputer , IterativeImputer , MissingIndicator Dealing with Missingness Missing Data Create data in which the true theoretical regression line is: $$ Y = 3X_1 + 2X_2 + \\varepsilon,\\hspace{0.1in} \\varepsilon \\sim N(0,1)$$ Note: $\\rho_{X1,X2} = 0.5$ We will be inserting missingness into x1 in various ways, and analyzing the results. In [0]: n = 500 np . random . seed ( 109 ) x1 = np . random . normal ( 0 , 1 , size = n ) x2 = 0.5 * x1 + np . random . normal ( 0 , np . sqrt ( 0.75 ), size = n ) X = pd . DataFrame ( data = np . transpose ([ x1 , x2 ]), columns = [ \"x1\" , \"x2\" ]) y = 3 * x1 - 2 * x2 + np . random . normal ( 0 , 1 , size = n ) y = pd . Series ( y ) df = pd . DataFrame ( data = np . transpose ([ x1 , x2 , y ]), columns = [ \"x1\" , \"x2\" , \"y\" ]) # Checking the correlation scipy . stats . pearsonr ( x1 , x2 ) In [0]: fig ,( ax1 , ax2 , ax3 ) = plt . subplots ( 1 , 3 , figsize = ( 18 , 5 )) ax1 . scatter ( x1 , y ) ax2 . scatter ( x2 , y ) ax3 . scatter ( x2 , x1 , color = \"orange\" ) ax1 . set_title ( \"y vs. x1\" ) ax2 . set_title ( \"y vs. x2\" ) ax3 . set_title ( \"x1 vs. x2\" ) plt . show () Poke holes in $X_1$ in 3 different ways (all roughly 20% of data are removed): MCAR: just take out a random sample of 20% of observations in $X_1$ MAR: missingness in $X_1$ depends on $X_2$, and thus can be recovered in some way MNAR: missingness in $X_1$ depends on $X_1$, and thus can be recovered in some way In [0]: x1_mcar = x1 . copy () x1_mar = x1 . copy () x1_mnar = x1 . copy () #missing completely at random miss_mcar = np . random . choice ( n , int ( 0.2 * n ), replace = False ) x1_mcar [ miss_mcar ] = np . nan #missing at random: one way to do it miss_mar = np . random . binomial ( 1 , 0.05 + 0.85 * ( x2 > ( x2 . mean () + x2 . std ())), n ) x1_mar [ miss_mar == 1 ] = np . nan #missing not at random: one way to do it miss_mnar = np . random . binomial ( 1 , 0.05 + 0.85 * ( y > ( y . mean () + y . std ())), n ) x1_mnar [ miss_mnar == 1 ] = np . nan In [0]: # Create the 3 datasets with missingness df_mcar = df . copy () df_mar = df . copy () df_mnar = df . copy () # plug in the appropriate x1 with missingness df_mcar [ 'x1' ] = x1_mcar df_mar [ 'x1' ] = x1_mar df_mnar [ 'x1' ] = x1_mnar In [0]: # no missingness: on the full dataset ols = LinearRegression () . fit ( df [[ 'x1' , 'x2' ]], df [ 'y' ]) print ( ols . intercept_ , ols . coef_ ) In [0]: # Fit the linear regression blindly on the dataset with MCAR missingness, see what happens LinearRegression () . fit ( df_mcar [[ 'x1' , 'x2' ]], df_mcar [ 'y' ]) Q1 Why aren't the estimates exactly $\\hat{\\beta}_1 = 3$ and $\\hat{\\beta}_2 = -2$ ? How does sklearn handle missingness? What would be a first naive approach to handling missingness? your answer here What happens when you just drop rows? In [0]: # no missingness for comparison sake ols = LinearRegression () . fit ( df [[ 'x1' , 'x2' ]], df [ 'y' ]) print ( ols . intercept_ , ols . coef_ ) In [0]: # MCAR: drop the rows that have any missingness ols_mcar = LinearRegression () . fit ( df_mcar . dropna ()[[ 'x1' , 'x2' ]], df_mcar . dropna ()[ 'y' ]) print ( ols_mcar . intercept_ , ols_mcar . coef_ ) In [0]: ### edTest(test_mar) ### # MAR: drop the rows that have any missingness ols_mar = LinearRegression () . fit ( ___ , ___ ) print ( ols_mar . intercept_ , ols_mar . coef_ ) In [0]: # MNAR: drop the rows that have any missingness ols_mnar = ___ print ( ___ , ___ ) Q2 How do the estimates compare when just dropping rows? Are they able to recover the values of $\\beta_1$ that they should? In which form of missingness is the result the worst? your answer here Let's Start Imputing In [0]: #Make back-=up copies for later since we'll have lots of imputation approaches. df_mcar_raw = df_mcar . copy () df_mar_raw = df_mar . copy () df_mnar_raw = df_mnar . copy () Mean Imputation: Perform mean imputation using the fillna , dropna , and mean functions. In [0]: df_mcar = df_mcar_raw . copy () df_mcar [ 'x1' ] = df_mcar [ 'x1' ] . fillna ( df_mcar [ 'x1' ] . dropna () . mean ()) ols_mcar_mean = LinearRegression () . fit ( df_mcar [[ 'x1' , 'x2' ]], df_mcar [ 'y' ]) print ( ols_mcar_mean . intercept_ , ols_mcar_mean . coef_ ) In [0]: ### edTest(test_mar_mean) ### df_mar = df_mar_raw . copy () df_mar [ 'x1' ] = df_mar [ 'x1' ] . fillna ( ___ ) ols_mar_mean = LinearRegression () . fit ( ___ , ___ ) print ( ols_mar_mean . intercept_ , ols_mar_mean . coef_ ) In [0]: df_mnar = df_mnar_raw . copy () df_mnar [ 'x1' ] = ___ ols_mnar_mean = ___ print ( ___ , ___ ) Q3 How do the estimates compare when performing mean imputation vs. just dropping rows? Have things gotten better or worse (for what types of missingness)? your answer here Linear Regression Imputation This is difficult to keep straight. There are two models here: an imputation model based on OLS concerning just the predictors (to predict $X_1$ from $X_2$) and the model we really care about to predict $Y$ from the 'improved' $X_1$ (now with imputed values) and $X_2$. In [0]: df_mcar = df_mcar_raw . copy () # fit the imputation model ols_imputer_mcar = LinearRegression () . fit ( df_mcar . dropna ()[[ 'x2' ]], df_mcar . dropna ()[ 'x1' ]) # perform some imputations x1hat_impute = pd . Series ( ols_imputer_mcar . predict ( df_mcar [[ 'x2' ]])) df_mcar [ 'x1' ] = df_mcar [ 'x1' ] . fillna ( x1hat_impute ) # fit the model we care about ols_mcar_ols = LinearRegression () . fit ( df_mcar [[ 'x1' , 'x2' ]], df_mcar [ 'y' ]) print ( ols_mcar_ols . intercept_ , ols_mcar_ols . coef_ ) In [0]: df_mar = df_mar_raw . copy () ols_imputer_mar = LinearRegression () . fit ( __ , __ ) x1hat_impute = pd . Series ( ols_imputer_mar . predict ( ___ )) df_mar [ 'x1' ] = df_mar [ 'x1' ] . fillna ( ___ ) ols_mar_ols = LinearRegression () . fit ( ___ , ___ ) print ( ols_mar_ols . intercept_ , ols_mar_ols . coef_ ) In [0]: ### edTest(test_mnar_ols) ### df_mnar = df_mnar_raw . copy () ols_imputer_mnar = ___ x1hat_impute = ___ df_mnar [ 'x1' ] = ___ ols_mnar_ols = ___ print ( ___ , ___ ) Q4 : How do the estimates compare when performing model-based imputation vs. mean imputation? Have things gotten better or worse (for what types of missingness)? your answer here $k$-NN Imputation ($k$=3) In [0]: df_mcar = df_mcar_raw . copy () X_mcar = KNNImputer ( n_neighbors = 3 ) . fit_transform ( df_mcar [[ 'x1' , 'x2' ]]) ols_mcar_knn = LinearRegression () . fit ( X_mcar , df_mcar [ 'y' ]) print ( ols_mcar_knn . intercept_ , ols_mcar_knn . coef_ ) In [0]: df_mar = df_mar_raw . copy () X_mar = KNNImputer ( n_neighbors = 3 ) . fit_transform ( ___ ) ols_mar_knn = LinearRegression () . fit ( ___ , ___ ) print ( ols_mar_knn . intercept_ , ols_mar_knn . coef_ ) In [0]: df_mnar = df_mnar_raw . copy () X_mnar = ___ ols_mnar_knn = ___ print ( ols_mnar_knn . intercept_ , ols_mnar_knn . coef_ ) Q5 : Which of the 4 methods for handling missingness worked best? Which worked the worst? Were the estimates improved or worsened in each of the 3 types of missingness? your answer here Q6 : This exercise focused on 'inference' (considering just the estimates of coefficients, not the uncertainty of these estimates, which would be even worse). What are the ramifications on prediction? Is the situation more or less concerning? your answer here In [0]:","tags":"labs","url":"labs/lecture-19/notebook/"},{"title":"Lecture 19: Missing Data","text":"Slides PDF | Lecture 19: Missing Data Exercises Lecture 19: 1 - Dealing with Missingness [Notebook]","tags":"lectures","url":"lectures/lecture19/"},{"title":"Lecture 18: Multiclass Logistic Regression","text":"In [1]: % matplotlib inline from sklearn import datasets import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np In [2]: from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier Irises Read in the data set and convert to a Pandas data frame: In [3]: raw = datasets . load_iris () iris = pd . DataFrame ( raw [ 'data' ], columns = raw [ 'feature_names' ]) iris [ 'type' ] = raw [ 'target' ] iris . head () Note: this violin plot is 'inverted': putting the response variable in the model on the x-axis. This is fine for exploration In [4]: sns . violinplot ( y = iris [ 'sepal length (cm)' ], x = iris [ 'type' ], split = True ); In [5]: # Create a violin plot to compare petal length # across the types of irises sns . violinplot ( ___ ); Here we fit our first model (the OvR logistic) and print out the coefficients: In [7]: logit_ovr = LogisticRegression ( penalty = 'none' , multi_class = 'ovr' , max_iter = 1000 ) . fit ( iris [[ 'sepal length (cm)' , 'sepal width (cm)' ]], iris [ 'type' ]) print ( logit_ovr . intercept_ ) print ( logit_ovr . coef_ ) In [10]: # we can predict classes or probabilities print ( logit_ovr . predict ( iris [[ 'sepal length (cm)' , 'sepal width (cm)' ]])[ 0 : 5 ]) print ( logit_ovr . predict_proba ( iris [[ 'sepal length (cm)' , 'sepal width (cm)' ]])[ 0 : 5 ]) In [21]: # and calculate accuracy print ( logit_ovr . score ( iris [[ 'sepal length (cm)' , 'sepal width (cm)' ]], iris [ 'type' ])) Now it's your turn: but this time with the multinomial logistic regression. In [22]: ### edTest(test_multinomial) ### # Fit the model and print out the coefficients logit_multi = LogisticRegression ( ___ ) . fit ( ___ ) intercept = logit_multi . intercept_ coefs = logit_multi . coef_ print ( intercept ) print ( coefs ) In [24]: ### edTest(test_multinomialaccuracy) ### multi_accuracy = ___ print ( multi_accuracy ) In [33]: # Plot the decision boundary. x1_range = iris [ 'sepal length (cm)' ] . max () - iris [ 'sepal length (cm)' ] . min () x2_range = iris [ 'sepal width (cm)' ] . max () - iris [ 'sepal width (cm)' ] . min () x1_min , x1_max = iris [ 'sepal length (cm)' ] . min () - 0.1 * x1_range , iris [ 'sepal length (cm)' ] . max () + 0.1 * x1_range x2_min , x2_max = iris [ 'sepal width (cm)' ] . min () - 0.1 * x2_range , iris [ 'sepal width (cm)' ] . max () + 0.1 * x2_range step = . 05 x1x , x2x = np . meshgrid ( np . arange ( x1_min , x1_max , step ), np . arange ( x2_min , x2_max , step )) y_hat_ovr = logit_ovr . predict ( np . c_ [ x1x . ravel (), x2x . ravel ()]) y_hat_multi = ___ fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 6 )) ax1 . pcolormesh ( x1x , x2x , y_hat_ovr . reshape ( x1x . shape ), cmap = plt . cm . Paired , alpha = 0.5 ) ax1 . scatter ( iris [ 'sepal length (cm)' ], iris [ 'sepal width (cm)' ], c = iris [ 'type' ], edgecolors = 'k' , cmap = plt . cm . Paired ) ### your job is to create the same plot, but for the multinomial ##### # your code here ##### plt . show () In [27]: #fit a knn model (k=5) for the same data knn5 = KNeighborsClassifier ( ___ ) . fit ( ___ ) In [0]: ### edTest(test_knnaccuracy) ### #Calculate the accuracy knn5_accuracy = ___ print ( knn5_accuracy ) In [32]: # and plot the classification boundary y_hat_knn5 = knn5 . predict ( np . c_ [ x1x . ravel (), x2x . ravel ()]) fig , ax1 = plt . subplots ( 1 , 1 , figsize = ( 8 , 6 )) ax1 . pcolormesh ( x1x , x2x , y_hat_knn5 . reshape ( x1x . shape ), cmap = plt . cm . Paired , alpha = 0.5 ) # Plot also the training points ax1 . scatter ( iris [ 'sepal length (cm)' ], iris [ 'sepal width (cm)' ], c = iris [ 'type' ], edgecolors = 'k' , cmap = plt . cm . Paired ) plt . show () In [0]:","tags":"labs","url":"labs/lecture-18/notebook/"},{"title":"Lecture 18: Multiclass Logistic Regression","text":"In [0]: % matplotlib inline from sklearn import datasets import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier #import sklearn.metrics as met from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split First Read in the data set and take a peak at it: In [0]: raw = datasets . load_wine () X_full = pd . DataFrame ( raw [ 'data' ], columns = raw [ 'feature_names' ]) y = raw [ 'target' ] print ( X_full . shape , y . shape ) In [0]: X_full . head () Q1 : Perform a 70-30 train_test_split using random_state=109 and shuffle=True . Why is it important to shuffle here? In [0]: ### your code here your answer here Q2 : Explore the data a little. Visualize the marginal association (aka, bivariate relationship) of wine type to amount of alcohol, level of malic acid, and total level of phenols. Which predictor seems to have the strongest association with the response? In [0]: ### your code here your answer here Q3 : Fit 3 different models with ['alcohol','malic_acid'] as the predictors: (1) a standard logistic regression to predict a binary indicator for class 0 (you'll have to crete it yourself), (2) a multinomial logistic regression to predict all 3 classes and (3) a OvR logistic reression to predict all 3 classes. Compare the results In [0]: ###your code here your answer here Q4 : For the Multinomial model, use the estimated coefficients to calculate the predicted probabilties by hand. Feel free to confirm with the predict_proba command. In [0]: ### your code here your answer here Q5 : For the OvR model, use the predict_proba() to estimate the predicted probabilities in the test set, and manually use this to calculate the predicted classes. Feel free to confirm with the predict command. In [0]: ### your code here Q6 : How could you use the predict_proba() command and 'change the threshold' in the multiclass setting to affect predictive accuracies within each class? Note: it is not as simple as changing a threshold because there is not threshold your answer here Q7 : Compare the accuracies in both train and test for both the multinomial and OvR logistic regressions. Which seems to be performing better? Is there any evidence of overfitting? How could this be corrected? In [0]: ### your code here your answer here Q8 : Create the classification boundaries for the two multiclass logistic regression models above. How do they compare? In [0]: ###your code here your answer here Q9 : Fit 3 different knn regression models: for $k = 3, 10, 30$. Visualize the classification boundaries for these 3 models and compare the results. Which seem to be overfit? In [0]: ### your answer here In [0]: ### your answer here your answer here Q10 How could you visualize the classification boundary for any of these models if there was a single predictor? What if there were more than 2 predictors? your answer here","tags":"labs","url":"labs/lecture-18/notebook-2/"},{"title":"Lecture 18: Multiclass Logistic Regression","text":"Slides Logistic Regression [PDF] Logistic Regression [PDF] Annotated Slides Logistic Regression [PDF] <9AM> Logistic Regression [PDF] <3PM> Exercises Lecture 18: 1 - Basic Multi-classification [Notebook] Lecture 18: 2 [Not Graded!] - A Walkthrough Example [Notebook]","tags":"lectures","url":"lectures/lecture18/"},{"title":"S-Section 05: Logistic Regression, Multiple Logistic Regression, and KNN-classification","text":"In [2]: %%html < style > . jp-MarkdownCell { background-color : cornsilk ;} . text_cell { background-color : cornsilk ;} style > CS109A Introduction to Data Science Standard Section 5: Logistic Regression, Multiple Logistic Regression, and kNN Classification Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Mike Sedelmeyer, N Ouyang In [28]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[28]: Contents In this section we will be covering Linear Regression, Logistic Regression, and kNN Classification using the Iris dataset. Specifically, we will: Import and explore data Fit a LINEAR regression model for classification, understand drawbacks, and interpret results Fit a simple LOGISTIC regression model for classification, compare performance, and interpret results Visualize predictions and decision boundaries Fit a higher order polynomial logistic regression model for classification, compare performance, plot decision boundaries, and interpret results Consider the impact of different regularization parameters on the above polynomial model Fit and investigate a kNN CLASSIFIER Introduce sklearn pipelines For our breakout exercises, we will: Standardize our data and fit a simple logistic regression model Plot and examine decision boundaries Add an interaction term and use LogisticRegressionCV to cross-validate a regularization parameter By the end, we hope you will be comfortable building your own classification models. For this section we will be using the following packages: In [29]: # Data and Stats packages import numpy as np import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsClassifier from sklearn.preprocessing import StandardScaler # NEW packages from sklearn import datasets from sklearn.linear_model import LogisticRegression from sklearn.linear_model import LogisticRegressionCV # Visualization packages % matplotlib inline import matplotlib.pyplot as plt import seaborn as sns import warnings warnings . filterwarnings ( \"ignore\" ) 1. Import and explore data ▲ Return to contents For Section 5 we will be using datasets available directly within sklearn . For section examples, we will be using Fisher's famous \"Iris\" dataset For our breakout sessions, we will be using the \"Wine\" dataset To learn more about sklearn datasets, please see the documentation: https://scikit-learn.org/stable/datasets/index.html In [30]: dataset = datasets . load_iris () print ( dataset . keys ()) dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']) What our the target and features? In [31]: print ( dataset . target_names ) print ( dataset . feature_names ) # Uncomment if you want to see much, much more info on this dataset # print(dataset['DESCR']) ['setosa' 'versicolor' 'virginica'] ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'] As we can see, the response variable (y) is the flower type, it has 3 classes: setosa versicolor virginica These are three different species of Iris flowers. The 4 predictor variables are flower characteristics (x): sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) Although this is a toy dataset, in real datasets it will be important to understand the underlying features. Below is a comparison of the three species and their sepals/petals, as well as the overall flower. Images not loading? See them online: Left: Labelled pictures of iris species from dataset Right: Yellow Iris flowers In [32]: X = pd . DataFrame ( data = dataset . data , columns = dataset . feature_names ) y = pd . DataFrame ( data = dataset . target , columns = [ 'species' ]) print ( X . shape ) print ( y . shape ) (150, 4) (150, 1) In [33]: display ( X . head ()) display ( X . describe ()) sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) 0 5.1 3.5 1.4 0.2 1 4.9 3.0 1.4 0.2 2 4.7 3.2 1.3 0.2 3 4.6 3.1 1.5 0.2 4 5.0 3.6 1.4 0.2 sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) count 150.000000 150.000000 150.000000 150.000000 mean 5.843333 3.057333 3.758000 1.199333 std 0.828066 0.435866 1.765298 0.762238 min 4.300000 2.000000 1.000000 0.100000 25% 5.100000 2.800000 1.600000 0.300000 50% 5.800000 3.000000 4.350000 1.300000 75% 6.400000 3.300000 5.100000 1.800000 max 7.900000 4.400000 6.900000 2.500000 In [34]: display ( y . head ()) display ( y . describe ()) species 0 0 1 0 2 0 3 0 4 0 species count 150.000000 mean 1.000000 std 0.819232 min 0.000000 25% 0.000000 50% 1.000000 75% 2.000000 max 2.000000 Explore Data Check which variables have high correlations and distinctive patterns with the response. Any patterns worth mentioning? In [35]: full_df = pd . concat ([ X , y ], axis = 1 ) full_df . head () Out[35]: sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) species 0 5.1 3.5 1.4 0.2 0 1 4.9 3.0 1.4 0.2 0 2 4.7 3.2 1.3 0.2 0 3 4.6 3.1 1.5 0.2 0 4 5.0 3.6 1.4 0.2 0 In [36]: sns . pairplot ( full_df ) plt . show () Are the features sepal/petal length and width uniformally distributed or do you observe some clusters of data points? What do you expect? Let's add color according to our response variable with hue='species' . In [37]: sns . pairplot ( full_df , hue = 'species' ) plt . show () Some features like 'petal length' and 'petal width' do have very high correlations and distinctive patterns with the response variable 'flower species'. When we would use these features for predicting the flower species, the classification wouldn't be very difficult. Certain ranges of 'petal length' and 'petal width' are very much correlated with a specific flower species and they are almost seperating our classes perfectly. Just for illustration purposes we will continue to use only 'sepal width (cm)' and 'sepal length (cm)' . We are making the problem harder for ourselves by only using 'weaker' or less-discriminative features. In [38]: X = X [[ 'sepal width (cm)' , 'sepal length (cm)' ]] Train-Test Split In [39]: X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.33 , random_state = 42 ) In [40]: print ( X_train . shape ) print ( y_train . shape ) print ( X_test . shape ) print ( y_test . shape ) (100, 2) (100, 1) (50, 2) (50, 1) 2. Fit a LINEAR regression model for classification, understand drawbacks, and interpret results ▲ Return to contents In [16]: model_linear_sklearn = LinearRegression () #Training model_linear_sklearn . fit ( X_train , y_train ) #Predict y_pred_train = model_linear_sklearn . predict ( X_train ) y_pred_test = model_linear_sklearn . predict ( X_test ) #Perfromance Evaluation train_score = accuracy_score ( y_train , y_pred_train ) * 100 test_score = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) in 9 10 #Perfromance Evaluation ---> 11 train_score = accuracy_score ( y_train , y_pred_train ) * 100 12 test_score = accuracy_score ( y_test , y_pred_test ) * 100 13 ~/v3/lib/python3.6/site-packages/sklearn/metrics/_classification.py in accuracy_score (y_true, y_pred, normalize, sample_weight) 183 184 # Compute accuracy for each possible representation --> 185 y_type , y_true , y_pred = _check_targets ( y_true , y_pred ) 186 check_consistent_length ( y_true , y_pred , sample_weight ) 187 if y_type . startswith ( 'multilabel' ) : ~/v3/lib/python3.6/site-packages/sklearn/metrics/_classification.py in _check_targets (y_true, y_pred) 88 if len ( y_type ) > 1 : 89 raise ValueError(\"Classification metrics can't handle a mix of {0} \" ---> 90 \"and {1} targets\".format(type_true, type_pred)) 91 92 # We can't have more than one value on y_type => The set is no more needed ValueError : Classification metrics can't handle a mix of multiclass and continuous targets **QUESTION: Can anyone explain why we are getting an error? What is wrong with `y_train` and `y_pred_train`?** In [41]: y_train [: 5 ] Out[41]: species 96 1 105 2 66 1 0 0 122 2 In [42]: y_pred_train [: 5 ] Out[42]: array([[1.01761878], [2.33100162], [0.88117842], [0.19897661], [2.53139119]]) The fact that our linear regression is outputting continuous predicstions is one of the major drawbacks of linear regression for classification. We can solve this in two manners: Simply rounding our prediction by using np.round() and converting it to an int data type with .astype(int) Or, use a modified algorithm that has bounded outputs (more about Logistic Regression later) In [43]: np . round ( y_pred_train [: 5 ]) Out[43]: array([[1.], [2.], [1.], [0.], [3.]]) In [44]: np . round ( y_pred_train [: 5 ]) . astype ( int ) Out[44]: array([[1], [2], [1], [0], [3]]) In [45]: model_linear_sklearn = LinearRegression () #Training model_linear_sklearn . fit ( X_train , y_train ) #Predict y_pred_train = np . round ( model_linear_sklearn . predict ( X_train )) . astype ( int ) y_pred_test = np . round ( model_linear_sklearn . predict ( X_test )) . astype ( int ) #Perfromance Evaluation train_score = accuracy_score ( y_train , y_pred_train ) * 100 test_score = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) Training Set Accuracy: 73.0% Testing Set Accuracy: 80.0% Get Performance by Class (Lookup Confusion Matrix) Each row of the matrix represents the instances in an actual class Each column represents the instances in a predicted class (or vice versa) The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). In [46]: confusion_matrix_linear = pd . crosstab ( y_test . values . flatten (), y_pred_test . flatten (), rownames = [ 'Actual Class' ], colnames = [ 'Predicted Class' ], ) display ( confusion_matrix_linear ) Predicted Class 0 1 2 3 Actual Class 0 19 0 0 0 1 0 10 5 0 2 0 4 11 1 **QUESTION: How many classes do we have in our y_test and in our y_pred_test? Why do we have 4 different predicted classes?** Can we use Linear Regression for classification? Four Assumptions of Linear Regression: Linearity: Our dependent variable Y is a linear combination of the explanatory variables X (and the error terms) Observations are independent of one another Constant variance of the error terms (i.e. homoscedasticity) The error terms are normally distributed ~ $N(0,\\sigma&#94;2)$ Suppose we have a binary outcome variable. Can we use Linear Regression? Then we will have the following problems: The error terms are heteroskedastic $\\epsilon$ is not normally distributed because Y takes on only two values The predicted probabilities can be greater than 1 or less than 0 Datasets where linear regression is problematic: Binary response data where there are only two outcomes (yes/no, 0/1, etc.) Categorical or ordinal data of any type, where the outcome is one of a number of discrete (possibly ordered) classes Count data in which the outcome is restricted to non-negative integers. Continuous data in which the noise is not normally distributed This is where Generalized Linear Models (GLMs), of which logistic regression is a specific type, come to the rescue! Logistic regression is most useful for predicting binary or multi-class responses. 3. Fit a simple LOGISTIC regression model for classification, compare performance, and interpret results ▲ Return to contents The logistic regression formula: $$\\hat{p}= \\dfrac{e&#94;{w&#94;T x}}{1+e&#94;{w&#94;T x}}$$ This is equivalent to: $$\\hat{p}= \\dfrac{1}{1+e&#94;{-w&#94;T x}}$$ Medium Article: Detailed overview of Logistic Regression In [47]: #Training model_logistic = LogisticRegression ( C = 100 ) . fit ( X_train , y_train ) #Predict y_pred_train = model_logistic . predict ( X_train ) y_pred_test = model_logistic . predict ( X_test ) #Perfromance Evaluation train_score = accuracy_score ( y_train , y_pred_train ) * 100 test_score = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) Training Set Accuracy: 82.0% Testing Set Accuracy: 80.0% Let's compare logistic regression against linear regression predictions To do this, we will hold one of our 2 predictors constant so we can easily visualize and compare our prediction curves. We fix X_train['sepal width (cm)'] to its mean value. x_1 = X_train['sepal width (cm)'] x_1_range = np.ones_like(x_2_range)*x_1.mean() We vary X_train['sepal length (cm)'] from its minimum to its maximum and look how the predicted class evolves. x_2 = X_train['sepal length (cm)'] x_2_min, x_2_max = x_2.min(), x_2.max()+0.3 x_2_range = np.arange(x_2_min, x_2_max, 0.003) In [48]: # Making our input features (x_2 varying, x_1 constat = mean of x_1) x_1 = X_train [ 'sepal width (cm)' ] x_2 = X_train [ 'sepal length (cm)' ] x_2_min , x_2_max = x_2 . min () - 0.1 , x_2 . max () + 0.3 x_2_range = np . arange ( x_2_min , x_2_max , 0.003 ) x_1_range = np . ones_like ( x_2_range ) * x_1 . mean () # Construct our input features X_with_varying_x_2 = np . stack ( [ x_1_range . ravel (), x_2_range . ravel ()], axis = 1 ) # Make linear Predictions prediction_linear = model_linear_sklearn . predict ( X_with_varying_x_2 ) # Make logistic Predictions prediction_proba = model_logistic . predict_proba ( X_with_varying_x_2 ) prediction_thresholded = model_logistic . predict ( X_with_varying_x_2 ) f , axes = plt . subplots ( 1 , 2 , figsize = ( 14 , 6 )) # Plot Linear Predictions axes [ 0 ] . plot ( x_2_range , prediction_linear , 'k--' , label = 'Linear Output (raw = continuous)' ) axes [ 0 ] . plot ( x_2_range , np . round ( prediction_linear ), 'k-' , alpha = 1 , label = 'Linear Predicted Class (rounded = integer)' , ) axes [ 0 ] . plot ( x_2_range , np . round ( prediction_thresholded ), 'r-' , alpha = 0.5 , label = 'Logistic Predicted Class (as shown on right)' , ) axes [ 0 ] . legend () axes [ 0 ] . set_title ( 'LINEAR Regression: \\n Raw output and rounded output' , fontsize = 14 , ) axes [ 0 ] . set_yticks ([ 0 , 1 , 2 , 3 ]) axes [ 0 ] . set_ylabel ( 'Predicted Class' , fontsize = 12 ) # Plot Logistic Predictions for i in sorted ( set ( prediction_thresholded )): axes [ 1 ] . plot ( x_2_range , prediction_proba [:, i ], linewidth = 2 , label = '$\\hat{{P}}\\:(Y= {} )$' . format ( i ) ) axes [ 1 ] . fill_between ( x_2_range [ prediction_thresholded == i ], 1 , 0 , alpha = 0.2 , edgecolor = 'gray' , ) axes [ 1 ] . legend () axes [ 1 ] . set_title ( \"LOGISTIC Regression: predicted probability \\n per class and the predicted class\" , fontsize = 14 , ) axes [ 1 ] . set_ylabel ( 'Predicted Probability' , fontsize = 12 ) for ax in axes . flat : ax . tick_params ( labelsize = 12 ) ax . set_xlabel ( 'sepal width (cm)' , fontsize = 12 ) ax . legend ( fontsize = 11 , framealpha = 1 , edgecolor = 'k' ) plt . tight_layout () plt . show () How does our Logistic Regression come up with mutiple class predictions? Each class $y_i$ has a sigmoid function that tries to predict the probability of the tested input belonging to that specific class $y_i$. In our case when we have 3 classes, thus we have 3 sigmoid functions (the blue, orange and green line in the right figure). LogisticRegression().predict_proba(...) : returns probability estimates $P(y_i|x)$ for each $y_i$. In our case .predict_proba(...) returns 3 values (one for each class). In the figure we observe that : we have a high probability of predicting Class 0 in regions with low 'sepal width' values (left) . we have a high probability of predicting Class 1 in regions with medium 'sepal width' regions (middle) . have a high probability of predicting Class 2 in regions with high 'sepal width' regions (right) . LogisticRegression().predict(...) : returns 1 value: the predicted class label. The class with the highest probability given by .predict_proba(...) is exactly the predicted class output of .predict(...) In the figure our final prediction is the red line . Get Performance by Class (Lookup Confusion Matrix) In [49]: confusion_matrix_logistic = pd . crosstab ( y_test . values . flatten (), y_pred_test . flatten (), rownames = [ 'Actual Class' ], colnames = [ 'Predicted Class' ], ) display ( confusion_matrix_logistic ) Predicted Class 0 1 2 Actual Class 0 19 0 0 1 0 8 7 2 0 3 13 Let's compare the confusion matrix of our linear model to the confusion matrix of our logistic regression model: In [50]: print ( '======================================' ) print ( 'Confusion Matrix Linear Regression:' ) display ( confusion_matrix_linear ) print ( ' \\n ======================================' ) print ( 'Confusion Matrix Logistic Regression:' ) display ( confusion_matrix_logistic ) print ( '======================================' ) ====================================== Confusion Matrix Linear Regression: Predicted Class 0 1 2 3 Actual Class 0 19 0 0 0 1 0 10 5 0 2 0 4 11 1 ====================================== Confusion Matrix Logistic Regression: Predicted Class 0 1 2 Actual Class 0 19 0 0 1 0 8 7 2 0 3 13 ====================================== Now we do observe that the logistic regression has the correct number of predicted classes. Breakout Session #1 ▲ Return to contents Fit and examine a multiclass logistic regression model Load and get to know a new dataset Select our predictors and response variable Generate a train-test-split Scale our data Perform logistic regression and investigate our results Image source: https://en.wikipedia.org/wiki/Wine In [51]: dataset_wine = datasets . load_wine () print ( dataset_wine . keys ()) print () print ( dataset_wine [ 'DESCR' ]) dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names']) .. _wine_dataset: Wine recognition dataset ------------------------ **Data Set Characteristics:** :Number of Instances: 178 (50 in each of three classes) :Number of Attributes: 13 numeric, predictive attributes and the class :Attribute Information: - Alcohol - Malic acid - Ash - Alcalinity of ash - Magnesium - Total phenols - Flavanoids - Nonflavanoid phenols - Proanthocyanins - Color intensity - Hue - OD280/OD315 of diluted wines - Proline - class: - class_0 - class_1 - class_2 :Summary Statistics: ============================= ==== ===== ======= ===== Min Max Mean SD ============================= ==== ===== ======= ===== Alcohol: 11.0 14.8 13.0 0.8 Malic Acid: 0.74 5.80 2.34 1.12 Ash: 1.36 3.23 2.36 0.27 Alcalinity of Ash: 10.6 30.0 19.5 3.3 Magnesium: 70.0 162.0 99.7 14.3 Total Phenols: 0.98 3.88 2.29 0.63 Flavanoids: 0.34 5.08 2.03 1.00 Nonflavanoid Phenols: 0.13 0.66 0.36 0.12 Proanthocyanins: 0.41 3.58 1.59 0.57 Colour Intensity: 1.3 13.0 5.1 2.3 Hue: 0.48 1.71 0.96 0.23 OD280/OD315 of diluted wines: 1.27 4.00 2.61 0.71 Proline: 278 1680 746 315 ============================= ==== ===== ======= ===== :Missing Attribute Values: None :Class Distribution: class_0 (59), class_1 (71), class_2 (48) :Creator: R.A. Fisher :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) :Date: July, 1988 This is a copy of UCI ML Wine recognition datasets. https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine. Original Owners: Forina, M. et al, PARVUS - An Extendible Package for Data Exploration, Classification and Correlation. Institute of Pharmaceutical and Food Analysis and Technologies, Via Brigata Salerno, 16147 Genoa, Italy. Citation: Lichman, M. (2013). UCI Machine Learning Repository [https://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. .. topic:: References (1) S. Aeberhard, D. Coomans and O. de Vel, Comparison of Classifiers in High Dimensional Settings, Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Technometrics). The data was used with many others for comparing various classifiers. The classes are separable, though only RDA has achieved 100% correct classification. (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) (All results using the leave-one-out technique) (2) S. Aeberhard, D. Coomans and O. de Vel, \"THE CLASSIFICATION PERFORMANCE OF RDA\" Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of Mathematics and Statistics, James Cook University of North Queensland. (Also submitted to Journal of Chemometrics). In [52]: X_wine = pd . DataFrame ( data = dataset_wine . data , columns = dataset_wine . feature_names ) y_wine = pd . DataFrame ( data = dataset_wine . target , columns = [ 'class' ]) print ( X_wine . shape ) print ( y_wine . shape ) (178, 13) (178, 1) In [53]: full_df_wine = pd . concat ([ X_wine , y_wine ], axis = 1 ) full_df_wine . head () Out[53]: alcohol malic_acid ash alcalinity_of_ash magnesium total_phenols flavanoids nonflavanoid_phenols proanthocyanins color_intensity hue od280/od315_of_diluted_wines proline class 0 14.23 1.71 2.43 15.6 127.0 2.80 3.06 0.28 2.29 5.64 1.04 3.92 1065.0 0 1 13.20 1.78 2.14 11.2 100.0 2.65 2.76 0.26 1.28 4.38 1.05 3.40 1050.0 0 2 13.16 2.36 2.67 18.6 101.0 2.80 3.24 0.30 2.81 5.68 1.03 3.17 1185.0 0 3 14.37 1.95 2.50 16.8 113.0 3.85 3.49 0.24 2.18 7.80 0.86 3.45 1480.0 0 4 13.24 2.59 2.87 21.0 118.0 2.80 2.69 0.39 1.82 4.32 1.04 2.93 735.0 0 Uncomment and run the pairplot in the cell below if you wish to see a scatter-matrix of all variables contained in the wine dataset. Please note: This plotting code will take a minute or so to run. In [54]: # sns.pairplot(full_df_wine, hue='class') # plt.suptitle('Wine dataset features, colored by cultivator class', fontsize=30, y=1) # plt.show() The wine dataset provides many variables to use in our model. Because we have limited time for this exercise, we will pretend we have already conducted EDA on our dataset and have chosen to focus solely on the predictors alcohol and flavanoids . Run the code below to subset our full wine dataframe and to perform our train-test-split. In [55]: predictors = [ 'alcohol' , 'flavanoids' ] full_df_wine_train , full_df_wine_test , = train_test_split ( full_df_wine [ predictors + [ 'class' ]], test_size = 0.2 , random_state = 109 , shuffle = True , stratify = full_df_wine [ 'class' ], ) Now let's take a quick look at our training data to get a sense for how alcohol and flavanoids relate to one another, as well as to our cultivator class response variable. In [56]: sns . scatterplot ( data = full_df_wine_train , x = 'alcohol' , y = 'flavanoids' , hue = 'class' , ) plt . title ( 'Wine training observations, \\n colored by cultivator class' , fontsize = 14 ) plt . show () What do you notice about the scale of our two predictor variables? It looks like the mean is shifted, but the total range of values are similar. Let's standardize our predictors to center our points and to ensure that both variables are commonly scaled. IMPORTANT: Remember to fit your scaler ONLY on your training data and to transform both train and test from that training fit. NEVER fit your scaler on the test set. In [57]: ################################### ## SUBSET OUR X AND y DATAFRAMES ## ################################### X_wine_train , y_wine_train = full_df_wine_train [ predictors ], full_df_wine_train [ 'class' ] X_wine_test , y_wine_test = full_df_wine_test [ predictors ], full_df_wine_test [ 'class' ] print ( \"Summary statistics for our training predictors BEFORE standardizing: \\n\\n \" \" {} \\n \" . format ( X_wine_train . describe ()) ) ########################## ## SCALE THE PREDICTORS ## ########################## # Be certain to ONLY EVER fit your scaler on X train (NEVER fit it on test) scaler = StandardScaler () . fit ( X_wine_train [ predictors ]) # Use your train-fitted scaler to transform both X train and X test X_wine_train [ predictors ] = scaler . transform ( X_wine_train [ predictors ]) X_wine_test [ predictors ] = scaler . transform ( X_wine_test [ predictors ]) print ( \"Summary statistics for our training predictors AFTER standardizing: \\n\\n \" \" {} \" . format ( X_wine_train . describe ()) ) Summary statistics for our training predictors BEFORE standardizing: alcohol flavanoids count 142.000000 142.000000 mean 12.948028 2.035634 std 0.806990 1.001051 min 11.030000 0.340000 25% 12.345000 1.205000 50% 12.945000 2.135000 75% 13.580000 2.897500 max 14.830000 5.080000 Summary statistics for our training predictors AFTER standardizing: alcohol flavanoids count 1.420000e+02 1.420000e+02 mean 1.238446e-15 2.001529e-16 std 1.003540e+00 1.003540e+00 min -2.385181e+00 -1.699850e+00 25% -7.499012e-01 -8.326991e-01 50% -3.765707e-03 9.961326e-02 75% 7.858943e-01 8.640091e-01 max 2.340343e+00 3.051936e+00 ### Student Exercise: 1. Fit a logistic regression model (name it `model1_wine`) without regularization (i.e. `penalty='none'`) to predict the `class` of each wine using our scaled predictors `alcohol` and `flavanoids` 2. Report on the training and test accuracy of our fitted model 3. Show that for each training prediction, the probability of each predicted class sums to one. (**HINT:** You can use the `predict_proba` method to generate your predicted probabilities.) In [58]: # Give it a shot! model1_wine = ... #### Solution After you've attempted the above exercise, uncomment and run the code below. In [59]: # %load '../solutions/breakout_1_sol.py' 4. Visualize Predictions and Decision boundaries ▲ Return to contents What are decision boundaries: In general, a pattern classifier carves up (or tesselates or partitions) the feature space into volumes called decision regions. All feature vectors in a decision region are assigned to the same category. In [35]: X_train . head () Out[35]: sepal width (cm) sepal length (cm) 96 2.9 5.7 105 3.0 7.6 66 3.0 5.6 0 3.5 5.1 122 2.8 7.7 In [36]: def plot_points ( ax ): for i , y_class in enumerate ( set ( y_train . values . flatten ())): index = ( y_train == y_class ) . values ax . scatter ( X_train [ index ][ 'sepal width (cm)' ], X_train [ index ][ 'sepal length (cm)' ], c = colors [ i ], marker = markers [ i ], s = 65 , edgecolor = 'w' , label = names [ i ], ) In [37]: f , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 6 )) colors = [ \"tab:blue\" , \"tab:orange\" , \"tab:green\" ] markers = [ \"s\" , \"o\" , \"D\" ] names = dataset . target_names plot_points ( ax ) ax . legend ( bbox_to_anchor = ( 1 , 1 ), loc = 'upper left' , ncol = 1 ) ax . set_title ( 'Classes of Flowers' ) ax . set_ylabel ( 'sepal length (cm)' ) ax . set_xlabel ( 'sepal width (cm)' ) plt . tight_layout () plt . show () Plotting the decision boundary by using the following functions: np.meshgrid() meshgrid returns coordinate matrices from coordinate vectors, effectively constructing a grid. The documentation: https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html plt.contourf() contourf draws filled contours. The documentation: https://matplotlib.org/api/_as_gen/matplotlib.pyplot.contourf.html#matplotlib.pyplot.contourf What is meshgrid? A quick numpy dive... How does meshgrid work? Creates two, 2D arrays of all the points in the grid: one for x coordinates, one for y coordinates We will go step-by-step to create a 3-by-4 grid below: In [38]: x_vector = np . linspace ( - 6 , 6 , 3 ) # start, stop, num pts y_vector = np . linspace ( - 2 , 1 , 4 ) print ( f \"x_vector: { x_vector } \\t shape: { x_vector . shape } \" ) print ( f \"y_vector: { y_vector } \\t shape: { y_vector . shape } \" ) x_vector: [-6. 0. 6.] shape: (3,) y_vector: [-2. -1. 0. 1.] shape: (4,) In [39]: # Return 2D x,y points of a grid x_grid , y_grid = np . meshgrid ( x_vector , y_vector ) print ( f \"x_grid, shape { x_grid . shape } : \\n\\n { x_grid } \\n \" ) print ( f \"y_grid, shape { y_grid . shape } : \\n\\n { y_grid } \" ) x_grid, shape (4, 3): [[-6. 0. 6.] [-6. 0. 6.] [-6. 0. 6.] [-6. 0. 6.]] y_grid, shape (4, 3): [[-2. -2. -2.] [-1. -1. -1.] [ 0. 0. 0.] [ 1. 1. 1.]] Additional numpy methods you will notice used in our decision boundary plots: np.ravel() , which returns a contiguous flattened array. np.stack() , which concatenates a sequence of arrays along a new axis. In [40]: x_grid_ravel = x_grid . ravel () y_grid_ravel = y_grid . ravel () print ( f \"x_grid.ravel(), shape { x_grid_ravel . shape } : \\n\\n { x_grid_ravel } \\n \" ) print ( f \"y_grid.ravel(), shape { y_grid_ravel . shape } : \\n\\n { y_grid_ravel } \" ) x_grid.ravel(), shape (12,): [-6. 0. 6. -6. 0. 6. -6. 0. 6. -6. 0. 6.] y_grid.ravel(), shape (12,): [-2. -2. -2. -1. -1. -1. 0. 0. 0. 1. 1. 1.] In [41]: xy_grid_stacked = np . stack ( ( x_grid . ravel (), y_grid . ravel ()), axis = 1 ) print ( f \"x and y stacked as columns in output: \\n\\n { xy_grid_stacked } \\n \" ) xy_grid_stacked = np . stack ( ( x_grid . ravel (), y_grid . ravel ()), axis = 0 ) print ( f \"x and y stacked as rows in output: \\n\\n { xy_grid_stacked } \" ) x and y stacked as columns in output: [[-6. -2.] [ 0. -2.] [ 6. -2.] [-6. -1.] [ 0. -1.] [ 6. -1.] [-6. 0.] [ 0. 0.] [ 6. 0.] [-6. 1.] [ 0. 1.] [ 6. 1.]] x and y stacked as rows in output: [[-6. 0. 6. -6. 0. 6. -6. 0. 6. -6. 0. 6.] [-2. -2. -2. -1. -1. -1. 0. 0. 0. 1. 1. 1.]] Now, with our numpy review over, let's get back to plotting our boundaries! First, we will create a very fine (spacing of 0.003!) np.meshgrid of points to evaluate and color. In [42]: x_1 = X_train [ 'sepal width (cm)' ] x_2 = X_train [ 'sepal length (cm)' ] # Just for illustration purposes we use a margin of 0.2 to the # left, right, top and bottum of our minimal and maximal points. # This way our minimal and maximal points won't lie exactly # on the axis. x_1_min , x_1_max = x_1 . min () - 0.2 , x_1 . max () + 0.2 x_2_min , x_2_max = x_2 . min () - 0.2 , x_2 . max () + 0.2 xx_1 , xx_2 = np . meshgrid ( np . arange ( x_1_min , x_1_max , 0.003 ), np . arange ( x_2_min , x_2_max , 0.003 ), ) Then, we will generate predictions for each point in our grid and use plt.contourf to plot those results! In [43]: # Plotting decision regions f , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 6 )) X_mesh = np . stack (( xx_1 . ravel (), xx_2 . ravel ()), axis = 1 ) Z = model_logistic . predict ( X_mesh ) Z = Z . reshape ( xx_1 . shape ) # contourf(): Takes in x,y, and z values -- all 2D arrays of same size ax . contourf ( xx_1 , xx_2 , Z , alpha = 0.3 , colors = colors , levels = 2 ) plot_points ( ax ) ax . legend ( bbox_to_anchor = ( 1 , 1 ), loc = 'upper left' , ncol = 1 ) ax . set_title ( 'Classes of Flowers' ) ax . set_ylabel ( 'sepal length (cm)' ) ax . set_xlabel ( 'sepal width (cm)' ) plt . tight_layout () plt . show () Why are the decision boundaries of this Logistic Regression linear? Imagine the simple case where we have only a 2 class classification problem: The logistic regression formula can be written as: $$ \\hat{p} = \\cfrac{e&#94;{w&#94;T x}}{1+e&#94;{w&#94;T x}} $$ Dividing through by the numerator, This is equivalent to $$ \\hat{p} = \\cfrac{1}{1+e&#94;{-w&#94;T x}} $$ Expanding $w&#94;T x$, we have $x_1$ (sepal width), $x_2$ (sepal length), and our intercept $x_0$ (constant = 1): $$ w&#94;T x = \\begin{bmatrix} w_0 & w_1 & w_2 \\\\ \\end{bmatrix} \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix} $$ Which makes our formula for $\\hat{p}$ equivalent to: $$ \\hat{p} = \\cfrac{1}{1 +e&#94;{\\displaystyle -(w_0 \\cdot 1 + w_1 \\cdot x_1 + w_2 \\cdot x_2)}} $$ Since we don't use multiple higher order polynomial features like $x_1&#94;2, x_2&#94;2$, our logistic model only depends on the first order simple features $x_1$ and $x_2$. What do we have to do to find the the decision boundary? The decision boundaries are exactly at the position where our algorithm \"hesitates\" when predicting which class to classify. The output probability of our sigmoid (or softmax) is exactly 0.5. Solving our sigmoid function for $p=0.5$: $$ \\begin{split} 0.5 &= \\cfrac{1}{1 +e&#94;{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 + w_2 x_2)}} \\\\ 0.5 &= \\cfrac{1}{1 + 1} \\\\ e&#94;{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 + w_2 x_2)} &= 1 \\\\ & \\\\ -(w_0 \\cdot 1 + w_1 x_1 + w_2 x_2 ) &= 0 \\end{split} $$ When we only use two predictor features this constraint of $p=0.5$ results in a linear system; thus we observe a linear decision boundary. In our case when we have three classes. 5. Fit a higher order polynomial logistic regression model for classification, compare performance, plot decision boundaries, and interpret results ▲ Return to contents In this section we will create degree-2 polynomial features for both of our predictors and re-fit our model and examine the results. In [44]: X_train . head () Out[44]: sepal width (cm) sepal length (cm) 96 2.9 5.7 105 3.0 7.6 66 3.0 5.6 0 3.5 5.1 122 2.8 7.7 In [45]: X_train_poly = X_train . copy () X_train_poly [ 'sepal width (cm)&#94;2' ] = X_train [ 'sepal width (cm)' ] ** 2 X_train_poly [ 'sepal length (cm)&#94;2' ] = X_train [ 'sepal length (cm)' ] ** 2 X_test_poly = X_test . copy () X_test_poly [ 'sepal width (cm)&#94;2' ] = X_test_poly [ 'sepal width (cm)' ] ** 2 X_test_poly [ 'sepal length (cm)&#94;2' ] = X_test_poly [ 'sepal length (cm)' ] ** 2 X_test_poly . head () Out[45]: sepal width (cm) sepal length (cm) sepal width (cm)&#94;2 sepal length (cm)&#94;2 73 2.8 6.1 7.84 37.21 18 3.8 5.7 14.44 32.49 118 2.6 7.7 6.76 59.29 78 2.9 6.0 8.41 36.00 76 2.8 6.8 7.84 46.24 In [46]: #Training model_logistic_poly = LogisticRegression ( penalty = \"none\" ) . fit ( X_train_poly , y_train ) #Predict y_pred_train = model_logistic_poly . predict ( X_train_poly ) y_pred_test = model_logistic_poly . predict ( X_test_poly ) #Perfromance Evaluation train_score = accuracy_score ( y_train , y_pred_train ) * 100 test_score = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) Training Set Accuracy: 81.0% Testing Set Accuracy: 72.0% **Our test performance is decreasing, what might be happening?** How would you test if this is happening? How would you inhibit this phenomenon from happening? In [47]: # Plotting decision regions f , ax = plt . subplots ( 1 , 1 , figsize = ( 6 , 6 )) X_mesh_poly = np . stack ( ( xx_1 . ravel (), xx_2 . ravel (), xx_1 . ravel () ** 2 , xx_2 . ravel () ** 2 ), axis = 1 , ) Z = model_logistic_poly . predict ( X_mesh_poly ) Z = Z . reshape ( xx_1 . shape ) ax . contourf ( xx_1 , xx_2 , Z , alpha = 0.3 , colors = colors , levels = 2 ) plot_points ( ax ) ax . legend ( bbox_to_anchor = ( 1 , 1 ), loc = 'upper left' , ncol = 1 ) ax . set_title ( 'Classes of Flowers' ) ax . set_ylabel ( 'sepal length (cm)' ) ax . set_xlabel ( 'sepal width (cm)' ) plt . show () **What do you observe regarding the form of the decision boundaries? Does this make sense?** Decision boundaries for higher order logistic regression Let's return to the mathematical representation of our logistic regression model: $$\\hat{p}= \\dfrac{e&#94;{w&#94;T x}}{1+e&#94;{w&#94;T x}}$$ Which is equivalent to: $$\\hat{p}= \\dfrac{1}{1+e&#94;{-w&#94;T x}}$$ Now we use $x_1$ (sepal width), $x_2$ (sepal length), an intercept $x_0$ (constant =1), PLUS two higher order terms while making predictions: $x_1&#94;2 = [\\text{sepal width}]&#94;2$ $x_2&#94;2 = [\\text{sepal length}]&#94;2$ $$\\hat{p}= \\cfrac{1}{1+e&#94;{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 + w_2 x_2 + w_3 x_1&#94;2 + w_4 x_2&#94;2)}}$$ Now solving for $p=0.5$ results in an equation also dependent on $x_1&#94;2$ and $x_2&#94;2$: thus we observe non-linear decision boundaries : $$ \\begin{split} 0.5 &= \\cfrac{1}{1 +e&#94;{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 + w_2 x_2 + w_3 x_1&#94;2 + w_4 x_2&#94;2)}} \\\\ 0.5 &= \\cfrac{1}{1 + 1} \\\\ e&#94;{\\displaystyle -(w_0 \\cdot 1 + w_1 x_1 + w_2 x_2 + w_3 x_1&#94;2 + w_4 x_2&#94;2)} &= 1 \\\\ & \\\\ -(w_0 \\cdot 1 + w_1 x_1 + w_2 x_2 + w_3 x_1&#94;2 + w_4 x_2&#94;2) &= 0 \\end{split} $$ Breakout Session #2 ▲ Return to contents Plot boundaries for our prior breakout model In this session, we will plot decision boundaries for the model1_wine model we fit during Breakout Session #1. ### Student Exercise: Using the `plot_wine_2d_boundaries` function provided below, you are going to plot the TRAINING data decision boundaries for `model1_wine` from breakout session 1 above. **To do this, you will need to:** 1. Define an appropriate `xx_1_wine` and `xx_2_wine` for input into this function using `np.meshgrid` 2. And call `plot_wine_2d_boundaries` specifying the appropriate input parameters In [48]: # We give you the plotting function here def plot_wine_2d_boundaries ( X_data , y_data , predictors , model , xx_1 , xx_2 ): \"\"\"Plots 2-dimensional decision boundaries for a fitted sklearn model :param X_data: pd.DataFrame object containing your predictor variables :param y_data: pd.Series object containing your response variable :param predictors: list of predictor names corresponding with X_data columns :param model: sklearn fitted model object :param xx_1: np.array object of 2-dimensions, generated using np.meshgrid :param xx_2: np.array object of 2-dimensions, generated using np.meshgrid \"\"\" def plot_points ( ax ): for i , y_class in enumerate ( set ( y_data . values . flatten ())): index = ( y_data == y_class ) . values ax . scatter ( X_data [ index ][ predictors [ 0 ]], X_data [ index ][ predictors [ 1 ]], c = colors [ i ], marker = markers [ i ], s = 65 , edgecolor = 'w' , label = \"class {} \" . format ( i ), ) # Plotting decision regions f , ax = plt . subplots ( 1 , 1 , figsize = ( 7 , 6 )) X_mesh = np . stack (( xx_1 . ravel (), xx_2 . ravel ()), axis = 1 ) Z = model . predict ( X_mesh ) Z = Z . reshape ( xx_1 . shape ) ax . contourf ( xx_1 , xx_2 , Z , alpha = 0.3 , colors = colors , levels = 2 ) plot_points ( ax ) ax . legend ( bbox_to_anchor = ( 1 , 1 ), loc = 'upper left' , ncol = 1 ) ax . set_title ( 'Wine cultivator class prediction \\n decision boundaries' , fontsize = 16 ) ax . set_xlabel ( predictors [ 0 ], fontsize = 12 ) ax . set_ylabel ( predictors [ 1 ], fontsize = 12 ) plt . tight_layout () plt . show () #### Enter your code below In [ ]: ## Your code here # Give breakout 2 a try! Just adapt the iris code to the wine dataset # Create your meshgrid arrays similar to xx_1 and xx_2 # Be certain to use reasonable min and max bounds for your data xx_1_wine , xx_2_wine = ... , ... # Then plot your boundaries plot_wine_2d_boundaries ( ... ) #### Solution After you've attempted the above exercise, uncomment and run the code below. In [56]: # %load '../solutions/breakout_2_sol.py' 6. Fit regularized polynomial logistic regression models and examine the results ▲ Return to contents **What do you expect to happen to our decision boundaries if we apply regularization to our polynomial regression model?** In [57]: %%time f , ax = plt . subplots ( 1 , 3 , sharey = True , figsize = ( 6 * 3 , 6 )) model_logistics = [] model_logistics_test_accs_scores = [] model_logistics_train_accs_scores = [] for test , C in enumerate ([ 10000 , 100 , 1 ]): model_logistics . append ( LogisticRegression ( C = C ) . fit ( X_train_poly , y_train )) y_pred_train = model_logistics [ test ] . predict ( X_train_poly ) y_pred_test = model_logistics [ test ] . predict ( X_test_poly ) model_logistics_train_accs_scores . append ( accuracy_score ( y_train , y_pred_train ) * 100 ) model_logistics_test_accs_scores . append ( accuracy_score ( y_test , y_pred_test ) * 100 ) Z = model_logistics [ test ] . predict ( X_mesh_poly ) Z = Z . reshape ( xx_1 . shape ) ax [ test ] . contourf ( xx_1 , xx_2 , Z , alpha = 0.3 , colors = colors , levels = 2 ) plot_points ( ax [ test ]) ax [ test ] . set_title ( 'Classes of Flowers, with C = ' + str ( C ), fontsize = 16 ) ax [ test ] . set_xlabel ( 'sepal width (cm)' , fontsize = 12 ) if test == 0 : ax [ test ] . legend ( loc = 'upper left' , ncol = 1 , fontsize = 12 ) ax [ test ] . set_ylabel ( 'sepal length (cm)' , fontsize = 12 ) plt . tight_layout () plt . show () CPU times: user 2.53 s, sys: 1.49 s, total: 4.02 s Wall time: 1.4 s What do you observe? How are the decision boundaries looking? What happens when the regularization term C changes? You may want to look at the documentation of sklearn.linear.LogisticRegression() to see how the C argument works: In [58]: # To get the documentation uncomment and run the following command: # LogisticRegression? What do expect regarding the evolution of the norm of the coefficients of our models when the regularization term C changes? Our list contains all 3 models with different values for C ( take a look at the first parameter within brackets ) In [59]: model_logistics Out[59]: [LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False), LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False)] In [60]: for test , model in enumerate ( model_logistics ): print ( ' \\n Regularization parameter : \\t C = {} ' . format ( model . C )) print ( \"Training Set Accuracy : \\t {} \" . format ( model_logistics_train_accs_scores [ test ]) + '%' ) print ( \"Testing Set Accuracy : \\t\\t {} \" . format ( model_logistics_test_accs_scores [ test ]) + '%' ) print ( 'Mean absolute coeficient : \\t {:0.2f} ' . format ( np . mean ( np . abs ( model . coef_ )))) Regularization parameter : C = 10000 Training Set Accuracy : 82.0% Testing Set Accuracy : 78.0% Mean absolute coeficient : 5.93 Regularization parameter : C = 100 Training Set Accuracy : 82.0% Testing Set Accuracy : 78.0% Mean absolute coeficient : 2.14 Regularization parameter : C = 1 Training Set Accuracy : 83.0% Testing Set Accuracy : 80.0% Mean absolute coeficient : 0.38 Interpretation of Results: What happens when our Regularization Parameter decreases? The amount of regularization increases, and this results in: The training set accuracy decreasing a little bit (not much of a problem) The TEST Accuracy INCREASING a little bit (better generalization!) The size of our coefficents DECREASES on average. Breakout Exercise #3 ▲ Return to contents Tune and fit a Lasso regularized model using cross-validation. Perform Lasso regularized logistic regression and choose an appropriate C by using cross-validation Confirm whether any predictors are identified as unimportant (i.e. $w_i=0$) There are a number of different tools built into sklearn that help you to perform cross-validation. A lot of examples in this class so far have used sklearn.model_selection.cross_validate as the primary means for peforming cross-validation. BUT WAIT!!! As it turns out, sklearn provides a very useful tool for performing logistic regression cross-validation across a range of regularization hyperparameters. For this problem, you should use the sklearn.linear_model.LogisticRegressionCV . ### Student Exercise: 1. Add a new predictor to your `X` data measuring the interaction between `alcohol` and `flavanoid` (make certain to do this for both your training and TEST `X` dataframes). 2. Please review the documentation to learn how to fit and use the `LogisticRegressionCV` model object: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html 3. Using `LogisticRegressionCV` to fit your model, perfrom cross-validation with `3` k-folds, Lasso-like regularization, and the following list of regularization parameters `[1e-4,1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3,1e4]`. 4. Print (1) the regularization parameter chosen by the model, (2) your train and test accuracies, and (3) your model coefficients (including the intercept). In [61]: ## Your code here. #### Solution After you've attempted the above exercise, uncomment and run the code below. In [62]: # %load '../solutions/breakout_3_sol.py' ## What did we learn today? - Linear regression does not work well with discrete features - Logistic regression is the way to go: outputs are clipped from 0 to 1 - Multiclass? Can still use logistic regression! Separate probabilities for each class - total P is 1 - If just have two features, put them on x and y axis and plot data, and can easily draw decision boundary - How to use meshgrid and contourf - What happens with polyomial features in logistic regression: nonlinear decision boundaries - Impact of regularization argument for logistic regession ------- END OF STANDARD SECTION 7. kNN Classification ▲ Return to contents In this section, we will fit a kNN-classification model, plot our decision boundaries, and interpret the results. You can read the KNeighborsClassifier documentation here: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html In [63]: #Training model_KNN_classifier = KNeighborsClassifier ( n_neighbors = 1 ) . fit ( X_train , y_train ) In [64]: #Predict y_pred_train = model_KNN_classifier . predict ( X_train ) y_pred_test = model_KNN_classifier . predict ( X_test ) #Perfromance Evaluation train_score = accuracy_score ( y_train , y_pred_train ) * 100 test_score = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) Training Set Accuracy: 96.0% Testing Set Accuracy: 72.0% The fact we have a big gap of performance between the test and training set means we are overfitting. This can be explained by our choice to use n_neighbors=1 . - **Based on your knowledge of kNN-regression can you guess how the decision boundary of the kNN-classification will look when using \"n_neighbors=1\"?** - **How about if we were to increase that number to \"n_neighbors=50\" or \"n_neighbors=100\" (our total number of training observations)?** Please note: The below code cell will take several minutes to run, primarily because of the large number of values in our meshgrid against which each kNN model must generate predictions. What does this tell us about the efficiency of the kNN algorithm vs. regularized logistic regression? Notice that the comparable set of 3 subplots generated using logistic regression only took about 1 second to run. In [65]: %%time # Similar to what we did above for varying values C # let's explore the effect that k in kNN classification has on our decision boundaries ks = [ 1 , 50 , 100 ] X_mesh = np . stack (( xx_1 . ravel (), xx_2 . ravel ()), axis = 1 ) Z = model_logistic . predict ( X_mesh ) Z = Z . reshape ( xx_1 . shape ) f , ax = plt . subplots ( 1 , 3 , sharey = True , figsize = ( 6 * 3 , 6 )) model_ks = [] model_ks_test_accs_scores = [] model_ks_train_accs_scores = [] for test , k in enumerate ( ks ): model_ks . append ( KNeighborsClassifier ( n_neighbors = k ) . fit ( X_train , y_train )) y_pred_train = model_ks [ test ] . predict ( X_train ) y_pred_test = model_ks [ test ] . predict ( X_test ) model_ks_train_accs_scores . append ( accuracy_score ( y_train , y_pred_train ) * 100 ) model_ks_test_accs_scores . append ( accuracy_score ( y_test , y_pred_test ) * 100 ) Z = model_ks [ test ] . predict ( X_mesh ) Z = Z . reshape ( xx_1 . shape ) ax [ test ] . contourf ( xx_1 , xx_2 , Z , alpha = 0.3 , colors = colors , levels = 2 ) plot_points ( ax [ test ]) ax [ test ] . set_title ( 'Classes of Flowers, with k = ' + str ( k ), fontsize = 16 ) ax [ test ] . set_xlabel ( 'sepal width (cm)' , fontsize = 12 ) if test == 0 : ax [ test ] . legend ( loc = 'upper left' , ncol = 1 , fontsize = 12 ) ax [ test ] . set_ylabel ( 'sepal length (cm)' , fontsize = 12 ) plt . tight_layout () plt . show () CPU times: user 2min 48s, sys: 9.78 s, total: 2min 58s Wall time: 2min 51s - **What do you notice in the plots above in regards to our decision boundaries as $k$ increases?** - **What implications might this have for our Train vs. Test accuracies at each value $k$?** - **If we wanted to tune our kNN classification model to find the best value $k$ given our data, what approach should we take?** (You better say \"cross-validation\" with many values $k$!) 8. Introducing sklearn pipelines (optional) ▲ Return to contents Pipelines can be used to to sequentially apply a list of transforms (e.g. scaling, polynomial feature creation) and a final estimator. Instead of manually building our polynomial features which might take a lot of lines of code we can use a pipeline to sequentially create polynomials before fitting our logistic regression. Scaling can also be done inside the make_pipeline . The documentation: https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html Previously we did: X_train_poly_cst = X_train_cst . copy () X_train_poly_cst [ 'sepal width (cm)&#94;2' ] = X_train_cst [ 'sepal width (cm)' ] ** 2 X_train_poly_cst [ 'sepal length (cm)&#94;2' ] = X_train_cst [ 'sepal length (cm)' ] ** 2 X_test_poly_cst = X_test_cst . copy () X_test_poly_cst [ 'sepal width (cm)&#94;2' ] = X_test_poly_cst [ 'sepal width (cm)' ] ** 2 X_test_poly_cst [ 'sepal length (cm)&#94;2' ] = X_test_poly_cst [ 'sepal length (cm)' ] ** 2 Now it is a one-liner: make_pipeline ( PolynomialFeatures ( degree = 2 , include_bias = False ), LogisticRegression ()) In [66]: # your code here from sklearn.pipeline import make_pipeline from sklearn.preprocessing import PolynomialFeatures polynomial_logreg_estimator = make_pipeline ( PolynomialFeatures ( degree = 2 , include_bias = False ), LogisticRegression (), ) #Training polynomial_logreg_estimator . fit ( X_train_poly , y_train ) #Predict y_pred_train = polynomial_logreg_estimator . predict ( X_train_poly ) y_pred_test = polynomial_logreg_estimator . predict ( X_test_poly ) #Perfromance Evaluation train_score = accuracy_score ( y_train , y_pred_train ) * 100 test_score = accuracy_score ( y_test , y_pred_test ) * 100 print ( \"Training Set Accuracy:\" , str ( train_score ) + '%' ) print ( \"Testing Set Accuracy:\" , str ( test_score ) + '%' ) Training Set Accuracy: 80.0% Testing Set Accuracy: 74.0%","tags":"sections","url":"sections/sec_5/"},{"title":"S-Section 05:  Logistic Regression, Multiple Logistic Regression, and KNN-classification","text":"Jupyter Notebooks S-Section 5: Logistic Regression, Multiple Logistic Regression, and KNN-classification","tags":"sections","url":"sections/section5/"},{"title":"Advanced Section 3: Generalized Linear Models","text":"Slides A-Section 3: Generalized Linear Models (Slides) [PPTX] A-Section 3: Generalized Linear Models (Slides) [PDF] Notes A-Section 3: Generalized Linear Models (Notes) [PDF]","tags":"a-sections","url":"a-sections/a-section3/"},{"title":"Lecture 17:  kNN classification and Logistic Regression II","text":"In [1]: import pandas as pd import numpy as np import sklearn as sk import matplotlib.pyplot as plt % matplotlib inline from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import PolynomialFeatures In [2]: heart = pd . read_csv ( 'Heart.csv' ) # Force the response into a binary indicator: heart [ 'AHD' ] = 1 * ( heart [ 'AHD' ] == \"Yes\" ) print ( heart . shape ) #heart.head() heart . describe () In [3]: heart_train , heart_test = train_test_split ( heart , test_size = 0.3 , random_state = 109 ) Q1.1 Below we fit an unregularized logistic regression model ( logit1 ) to predict AHD from Age and MaxHR in the training set (with penalty='none' ). Print out the coefficient estimates, and interpret general trends. In [4]: degree = 1 predictors = [ 'Age' , 'MaxHR' ] X_train1 = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( heart_train [ predictors ]) y_train = heart_train [ 'AHD' ] logit1 = LogisticRegression ( penalty = 'none' , max_iter = 5000 ) . fit ( X_train1 , y_train ) print ( \"Logistic Regression Estimated Betas:\" , logit1 . ___ , logit1 . ___ ) your interpretation here Q1.1 Fit an unregularized 4th order polynomial (with interactions) logistic regression model ( logit4 ) to predict AHD from Age and MaxHR in the training set (with penalty='none' ). Print out the coefficient estimates. In [5]: ### edTest(test_logit4) ### degree = ___ predictors = [ 'Age' , 'MaxHR' ] X_train4 = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( ___ ) logit4 = LogisticRegression ( penalty = 'none' , max_iter = 5000 ) . fit ( ___ ) print ( \"Logistic Regression Estimated Betas:\" , logit4 . ___ , logit4 . ___ ) Q1.2 Evaluate the models based on misclassification rate in both the test set. In [6]: ### edTest(test_misclass) ### ###### # your code here ###### predictors = [ 'Age' , 'MaxHR' ] X_test1 = PolynomialFeatures ( degree = 1 , include_bias = False ) . fit_transform ( heart_test [ predictors ]) X_test4 = PolynomialFeatures ( degree = 4 , include_bias = False ) . fit_transform ( heart_test [ predictors ]) y_test = heart_test [ 'AHD' ] # use logit.score() misclass_logit1 = ___ misclass_logit4 = ___ print ( \"Overall misclassification rate in test for logit1:\" , misclass_logit1 ) print ( \"Overall misclassification rate in test for logit4:\" , misclass_logit4 ) The code below performs the classification predictions for the model at all values in the range of the two predictors for logit1 . Then the predictions and the train dataset are added to a scatterplot in the second code chunk: In [7]: n = 100 x1 = np . linspace ( np . min ( heart [[ 'Age' ]]), np . max ( heart [[ 'Age' ]]), n ) x2 = np . linspace ( np . min ( heart [[ 'MaxHR' ]]), np . max ( heart [[ 'MaxHR' ]]), n ) x1v , x2v = np . meshgrid ( x1 , x2 ) # This is how we would typically do the prediction (have a vector of yhats) #yhat10 = knn10.predict(np.array([x1v.flatten(),x2v.flatten()]).reshape(-1,2)) # To do the predictions and keep the yhats on 2-D (to match the dummy predictor shapes), use this X = np . c_ [ x1v . ravel (), x2v . ravel ()] X_dummy = PolynomialFeatures ( degree = 1 , include_bias = False ) . fit_transform ( X ) yhat1 = logit1 . predict ( X_dummy ) In [8]: plt . pcolormesh ( x1v , x2v , yhat1 . reshape ( x1v . shape ), alpha = 0.05 ) plt . scatter ( heart_train [ 'Age' ], heart_train [ 'MaxHR' ], c = heart_train [ 'AHD' ]) plt . ylabel ( \"MaxHR\" ) plt . xlabel ( \"Age\" ) plt . title ( \"Yellow = Predicted to have AHD, Purple = Predicted to not have AHD\" ) plt . colorbar () plt . show () In [9]: #Perform the same calculation above, but for the 4th order polynomial X_dummy = PolynomialFeatures ( degree = 4 , include_bias = False ) . fit_transform ( X ) yhat4 = logit4 . predict ( ___ ) plt . pcolormesh ( x1v , x2v , yhat4 . reshape ( x1v . shape ), alpha = 0.05 ) plt . scatter ( heart_train [ 'Age' ], heart_train [ 'MaxHR' ], c = heart_train [ 'AHD' ]) plt . ylabel ( \"MaxHR\" ) plt . xlabel ( \"Age\" ) plt . title ( \"Yellow = Predicted to have AHD, Purple = Predicted to not have AHD\" ) plt . colorbar () plt . show () Q1.3 Compare the two models above on how they create the classification boundary. Which is more likely to be overfit? How would regularization affect these boundaries? your answer here Q1.4 Fit a ridge-like Logistic Regression model with C=0.0001 on the 4th order polynomial as before. Compare this regularized model with the unregularized one by using the classification boundary. In [10]: ### edTest(test_ridge) ### logit_ridge = LogisticRegression ( ___ , max_iter = 5000 ) . fit ( ___ , ___ ) In [11]: #yhat_ridge = logit_ridge.predict_proba(X_dummy)[:,1] yhat_ridge = ___ plt . pcolormesh ( x1v , x2v , yhat_ridge . reshape ( x1v . shape ), alpha = 0.05 ) plt . scatter ( heart_train [ 'Age' ], heart_train [ 'MaxHR' ], c = heart_train [ 'AHD' ]) plt . ylabel ( \"MaxHR\" ) plt . xlabel ( \"Age\" ) plt . title ( \"Yellow = Predicted to have AHD, Purple = Predicted to not have AHD\" ) plt . colorbar () plt . show () your answer here","tags":"labs","url":"labs/lecture-17/notebook/"},{"title":"Lecture 17:  kNN classification and Logistic Regression II","text":"In [1]: import pandas as pd import numpy as np import sklearn as sk import matplotlib.pyplot as plt % matplotlib inline from sklearn.linear_model import LogisticRegression from sklearn import metrics from sklearn.preprocessing import PolynomialFeatures from sklearn.model_selection import train_test_split In [2]: heart = pd . read_csv ( 'Heart.csv' ) # Force the response into a binary indicator: heart [ 'AHD' ] = 1 * ( heart [ 'AHD' ] == \"Yes\" ) heart_train , heart_test = train_test_split ( heart , test_size = 0.3 , random_state = 109 ) In [3]: degree = 3 predictors = [ 'Age' , 'Sex' , 'MaxHR' , 'RestBP' , 'Chol' ] X_train = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( heart_train [ predictors ]) y_train = heart_train [ 'AHD' ] X_test = PolynomialFeatures ( degree = degree , include_bias = False ) . fit_transform ( heart_test [ predictors ]) y_test = heart_test [ 'AHD' ] logit = LogisticRegression ( penalty = 'none' , max_iter = 10000 ) . fit ( X_train , y_train ) logit_ridge = LogisticRegression ( C = 0.001 , penalty = 'l2' , solver = 'lbfgs' , max_iter = 10000 ) . fit ( X_train , y_train ) In [4]: yhat_logit = logit . predict_proba ( X_test )[:, 1 ] yhat_logit_ridge = logit_ridge . predict_proba ( X_test )[:, 1 ] threshold = 0.5 print ( 'The confusion matrix in test for logit when cut-off is' , threshold , ': \\n ' , sk . metrics . confusion_matrix ( y_test , yhat_logit > threshold )) print ( 'The confusion matrix in test for logit_ridge when cut-off is' , threshold , ': \\n ' , sk . metrics . confusion_matrix ( y_test , yhat_logit_ridge > threshold )) In [5]: ###### # your code here ###### yhat_logit = logit . predict_proba ( X_test )[:, 1 ] yhat_logit_ridge = logit_ridge . predict_proba ( X_test )[:, 1 ] fpr , tpr , thresholds = metrics . roc_curve ( y_test , yhat_logit ) fpr_ridge , tpr_ridge , thresholds_ridge = metrics . roc_curve ( y_test , yhat_logit_ridge ) x = np . arange ( 0 , 100 ) / 100 plt . plot ( x , x , '--' , color = \"gray\" , alpha = 0.3 ) plt . plot ( fpr , tpr , label = \"logit\" ) plt . plot ( fpr_ridge , tpr_ridge , label = \"logit_ridge\" ) plt . ylabel ( \"True Positive Rate\" ) plt . xlabel ( \"False Positive Rate\" ) plt . title ( \"ROC Curve for Predicting AHD in a Logistic Regression Model\" ) plt . legend () plt . show () In [6]: print ( metrics . auc ( fpr , tpr )) print ( metrics . auc ( fpr_ridge , tpr_ridge )) In [0]: In [0]:","tags":"labs","url":"labs/lecture-17/notebook-2/"},{"title":"Lecture 17:  kNN classification and Logistic Regression II","text":"Slides Logistic Regression [PDF] Logistic Regression [PPTX] Exercises Lecture 17: 1 - Regularization and Decision Boundaries in Logistic Regression [Notebook] Lecture 17: 2 [Not Graded!] - Confusion Matrices & ROC Curves [Notebook]","tags":"lectures","url":"lectures/lecture17/"},{"title":"Lecture 16:  Case Study","text":"CS-109A Introduction to Data Science Lecture 16: Review and a Preview Harvard University Fall 2020 In [ ]: import pandas as pd import sys import numpy as np import sklearn as sk import scipy as sp import matplotlib.pyplot as plt import seaborn as sns import datetime % matplotlib inline from sklearn.linear_model import LinearRegression from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.neighbors import KNeighborsRegressor import sklearn.metrics as met from sklearn.preprocessing import PolynomialFeatures In [ ]: movies = pd . read_csv ( 'tmdb_5000_movies.csv' ) credits = pd . read_csv ( 'tmdb_5000_credits.csv' ) movies . head () In [ ]: credits . head () In [ ]: print ( movies . dtypes ) quants = movies . columns [( movies . dtypes == \"int64\" ) | ( movies . dtypes == \"float64\" ) ] . values quants = quants [ quants != 'id' ] In [ ]: pd . Series ( np . append ( quants , 'year' )) In [ ]: movies [ 'release_date' ] = pd . to_datetime ( movies [ 'release_date' ]) movies [ 'year' ] = pd . DatetimeIndex ( movies [ 'release_date' ]) . year movies [ 'month' ] = pd . DatetimeIndex ( movies [ 'release_date' ]) . month movies [ 'decade' ] = (( movies [ 'year' ]) // 10 ) * 10 In [ ]: oldest = np . argmin ( movies [ 'release_date' ]) newest = np . argmax ( movies [ 'release_date' ]) print ( \"Oldest Movie:\" , movies [ 'title' ][ oldest ], \" in\" , movies [ 'release_date' ][ oldest ]) print ( \"Newest Movie:\" , movies [ 'title' ][ newest ], \" in\" , movies [ 'release_date' ][ newest ]) In [ ]: sns . pairplot ( movies [ np . append ( quants , 'year' )]); In [ ]: movies_raw = movies . copy () Breakout 1 Tasks (15-20min): Someone share (the person who resides closest to the Bahamas…thanks Columbus). Someone different will share in the next breakout. Explore the data (some of that is done for you above). Please do a little more exploration and wrangling. Come up with an interesting question or two you can answer with this data set. Come up with a question or two that can be answered with supplemental data: start with ideal, and then get more practical based on what is likely available. In [ ]:","tags":"labs","url":"labs/lecture-16/notebook/"},{"title":"Lecture 16:  Case Study","text":"Breakout 2 Tasks (20+min): Someone else share and take notes (who resides furthest from the Bahamas) Solidify your question(s) of interest. Determine the next tasks: What other data do you need? How will this data be collected and combined? What data cleaning and wrangling tasks are needed? What other EDA is necessary? What visuals should be included? What is a goal for a first baseline model (Key: should be interpretable)? (Be sure to include the class of model and the variables involved. What is a reasonable goal for a final model and product? Determine how long each task should take. Assign next tasks to group members. Do not actual perform these tasks! In [95]: import pandas as pd import sys import numpy as np import sklearn as sk import scipy as sp import matplotlib.pyplot as plt import seaborn as sns import datetime % matplotlib inline from sklearn.linear_model import LinearRegression from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier from sklearn.neighbors import KNeighborsRegressor import sklearn.metrics as met from sklearn.preprocessing import PolynomialFeatures In [137]: movies = pd . read_csv ( 'data/tmdb_5000_movies.csv' ) credits = pd . read_csv ( 'data/tmdb_5000_credits.csv' ) movies . head () Out[137]: budget genres homepage id keywords original_language original_title overview popularity production_companies production_countries release_date revenue runtime spoken_languages status tagline title vote_average vote_count 0 237000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam... http://www.avatarmovie.com/ 19995 [{\"id\": 1463, \"name\": \"culture clash\"}, {\"id\":... en Avatar In the 22nd century, a paraplegic Marine is di... 150.437577 [{\"name\": \"Ingenious Film Partners\", \"id\": 289... [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2009-12-10 2787965087 162.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}, {\"iso... Released Enter the World of Pandora. Avatar 7.2 11800 1 300000000 [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"... http://disney.go.com/disneypictures/pirates/ 285 [{\"id\": 270, \"name\": \"ocean\"}, {\"id\": 726, \"na... en Pirates of the Caribbean: At World's End Captain Barbossa, long believed to be dead, ha... 139.082615 [{\"name\": \"Walt Disney Pictures\", \"id\": 2}, {\"... [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2007-05-19 961000000 169.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}] Released At the end of the world, the adventure begins. Pirates of the Caribbean: At World's End 6.9 4500 2 245000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam... http://www.sonypictures.com/movies/spectre/ 206647 [{\"id\": 470, \"name\": \"spy\"}, {\"id\": 818, \"name... en Spectre A cryptic message from Bond's past sends him o... 107.376788 [{\"name\": \"Columbia Pictures\", \"id\": 5}, {\"nam... [{\"iso_3166_1\": \"GB\", \"name\": \"United Kingdom\"... 2015-10-26 880674609 148.0 [{\"iso_639_1\": \"fr\", \"name\": \"Fran\\u00e7ais\"},... Released A Plan No One Escapes Spectre 6.3 4466 3 250000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 80, \"nam... http://www.thedarkknightrises.com/ 49026 [{\"id\": 849, \"name\": \"dc comics\"}, {\"id\": 853,... en The Dark Knight Rises Following the death of District Attorney Harve... 112.312950 [{\"name\": \"Legendary Pictures\", \"id\": 923}, {\"... [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2012-07-16 1084939099 165.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}] Released The Legend Ends The Dark Knight Rises 7.6 9106 4 260000000 [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"nam... http://movies.disney.com/john-carter 49529 [{\"id\": 818, \"name\": \"based on novel\"}, {\"id\":... en John Carter John Carter is a war-weary, former military ca... 43.926995 [{\"name\": \"Walt Disney Pictures\", \"id\": 2}] [{\"iso_3166_1\": \"US\", \"name\": \"United States o... 2012-03-07 284139100 132.0 [{\"iso_639_1\": \"en\", \"name\": \"English\"}] Released Lost in our world, found in another. John Carter 6.1 2124 In [138]: credits . head () Out[138]: movie_id title cast crew 0 19995 Avatar [{\"cast_id\": 242, \"character\": \"Jake Sully\", \"... [{\"credit_id\": \"52fe48009251416c750aca23\", \"de... 1 285 Pirates of the Caribbean: At World's End [{\"cast_id\": 4, \"character\": \"Captain Jack Spa... [{\"credit_id\": \"52fe4232c3a36847f800b579\", \"de... 2 206647 Spectre [{\"cast_id\": 1, \"character\": \"James Bond\", \"cr... [{\"credit_id\": \"54805967c3a36829b5002c41\", \"de... 3 49026 The Dark Knight Rises [{\"cast_id\": 2, \"character\": \"Bruce Wayne / Ba... [{\"credit_id\": \"52fe4781c3a36847f81398c3\", \"de... 4 49529 John Carter [{\"cast_id\": 5, \"character\": \"John Carter\", \"c... [{\"credit_id\": \"52fe479ac3a36847f813eaa3\", \"de... In [139]: print ( movies . dtypes ) quants = movies . columns [( movies . dtypes == \"int64\" ) | ( movies . dtypes == \"float64\" ) ] . values quants = quants [ quants != 'id' ] budget int64 genres object homepage object id int64 keywords object original_language object original_title object overview object popularity float64 production_companies object production_countries object release_date object revenue int64 runtime float64 spoken_languages object status object tagline object title object vote_average float64 vote_count int64 dtype: object In [140]: pd . Series ( np . append ( quants , 'year' )) Out[140]: 0 budget 1 popularity 2 revenue 3 runtime 4 vote_average 5 vote_count 6 year dtype: object In [141]: movies [ 'release_date' ] = pd . to_datetime ( movies [ 'release_date' ]) movies [ 'year' ] = pd . DatetimeIndex ( movies [ 'release_date' ]) . year movies [ 'month' ] = pd . DatetimeIndex ( movies [ 'release_date' ]) . month movies [ 'decade' ] = (( movies [ 'year' ]) // 10 ) * 10 In [142]: oldest = np . argmin ( movies [ 'release_date' ]) newest = np . argmax ( movies [ 'release_date' ]) print ( \"Oldest Movie:\" , movies [ 'title' ][ oldest ], \" in\" , movies [ 'release_date' ][ oldest ]) print ( \"Newest Movie:\" , movies [ 'title' ][ newest ], \" in\" , movies [ 'release_date' ][ newest ]) Oldest Movie: Intolerance in 1916-09-04 00:00:00 Newest Movie: Growing Up Smith in 2017-02-03 00:00:00 In [143]: sns . pairplot ( movies [ np . append ( quants , 'year' )]); Out[143]: In [145]: f , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax1 . scatter ( movies [ 'budget' ], movies [ 'revenue' ]) ax1 . set_title ( \"Revenue vs. Budget\" ) ax2 . scatter ( np . log10 ( movies [ 'budget' ] + 0.1 ), np . log10 ( movies [ 'revenue' ] + 0.1 )) ax2 . set_title ( \"Revenue vs. Budget (both on log10 scale)\" ) plt . show () In [146]: print ( np . sum ( movies [ 'runtime' ] == 0 )) movies [( movies [ 'budget' ] < 1000 ) | ( movies [ 'revenue' ] < 1000 )][[ 'revenue' , 'budget' ]] 35 Out[146]: revenue budget 83 0 27000000 135 0 150000000 265 0 0 309 0 84000000 321 104907746 0 ... ... ... 4797 0 0 4799 0 9000 4800 0 0 4801 0 0 4802 0 0 1592 rows × 2 columns In [147]: movies_raw = movies . copy () In [148]: movies = movies [( movies [ 'budget' ] >= 1000 ) & ( movies [ 'revenue' ] >= 1000 )] movies [ 'logbudget' ] = np . log10 ( movies [ 'budget' ]) movies [ 'logrevenue' ] = np . log10 ( movies [ 'revenue' ]) f , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) ax1 . scatter ( movies [ 'logbudget' ], movies [ 'logrevenue' ]) ax1 . set_title ( \"Revenue vs. Budget (both on log10 scale) After Trimming\" ) ax2 . scatter ( movies [ 'budget' ], movies [ 'revenue' ]) ax2 . set_title ( \"Revenue vs. Budget After Trimming\" ) plt . show () In [149]: ols1 = LinearRegression () ols1 . fit ( movies [[ 'logbudget' ]], movies [ 'logrevenue' ]) print ( f \"Estimated Linear Regression Coefficients: Intercept = { ols1 . intercept_ : .4f } , Slope(s) = { ols1 . coef_ [ 0 ] : .4f } \" ) Estimated Linear Regression Coefficients: Intercept = 1.6785, Slope(s) = 0.8125 In [150]: ols2 = LinearRegression () ols2 . fit ( movies [[ 'logbudget' , 'year' ]], movies [ 'logrevenue' ]) print ( f \"Estimated Linear Regression Coefficients: Intercept = { ols2 . intercept_ : .3f } , Slope(s) =\" , np . round ( ols2 . coef_ , 5 )) Estimated Linear Regression Coefficients: Intercept = 14.954, Slope(s) = [ 0.86571 -0.00683] In [157]: poly = PolynomialFeatures ( interaction_only = True , include_bias = False ) X_interact = poly . fit_transform ( movies [[ 'logbudget' , 'year' ]]) In [159]: ols3 = LinearRegression () ols3 . fit ( X_interact , movies [ 'logrevenue' ]) print ( f \"Estimated Linear Regression Coefficients: Intercept = { ols3 . intercept_ : .3f } , Slope(s) =\" , np . round ( ols3 . coef_ , 4 )) Estimated Linear Regression Coefficients: Intercept = 161.257, Slope(s) = [-2.0854e+01 -7.9900e-02 1.0900e-02]","tags":"labs","url":"labs/lecture-16/notebook-2/"},{"title":"Lecture 16:  Case Study","text":"Slides Case Study [PDF] Case Study [PPTX] Exercises Lecture 16: 1 - Exploration, Wrangling, and Defining a Question [Notebook] Lecture 16: 2 - Redefining and Scoping [Notebook]","tags":"lectures","url":"lectures/lecture16/"},{"title":"Lecture 15:  kNN classification and Logistic Regression I","text":"In [1]: # import important libraries % matplotlib inline import numpy as np import pandas as pd from math import exp import matplotlib.pyplot as plt from sklearn.preprocessing import normalize from sklearn.metrics import accuracy_score In [2]: # Make a dataframe of the file \"insurance_claim.csv\" data_filename = 'insurance_claim.csv' df = pd . read_csv ( data_filename ) In [0]: # Take a quick look of the data, notice that the response variable is binary df . head () In [3]: # Assign age as the predictor variable using double brackets x = df [[ ___ ]] # Assign insuranceclaim as the response variable y = df [ ___ ] In [0]: # Make a plot of the response (insuranceclaim) vs the predictor (age) plt . plot ( ___ , ___ , 'o' , markersize = 7 , color = \"#011DAD\" , label = \"Data\" ) # Also add the labels for 'x' & 'y' values plt . xlabel ( ___ ) plt . ylabel ( ___ ) plt . xticks ( np . arange ( 18 , 80 , 4.0 )) # Label the value 1 as 'Yes' & 0 as 'No' plt . yticks (( 0 , 1 ), labels = ( 'No' , 'Yes' )) plt . legend ( loc = 'best' ) plt . show () In [5]: ### edTest(test_beta_guesstimate) ### # Guesstimate the values of beta0 & beta1 beta0 = ___ beta1 = ___ In [6]: ### edTest(test_beta_computation) ### # Use the logistic function below to predict the response based on the input logit = [] for i in x : # Append the P(y=1) values to the logit list logit . append ( ___ ) In [6]: # If the predictions are above a threshold of 0.5, predict as 1, else 0 y_pred = [] for py in ___ : if py >= 0.5 : ____ else : ____ In [0]: # Use accuracy_score function to find the accuracy accuracy = accuracy_score ( ___ , ___ ) # Print the accuracy print ( ___ ) In [0]: # Make a plot similar to the one above along with the fit curve plt . plot ( ___ , ___ , 'o' , markersize = 7 , color = \"#011DAD\" , label = \"Data\" ) plt . plot ( ___ , ___ , linewidth = 2 , color = 'black' , label = \"Prediction\" ) plt . xticks ( np . arange ( 18 , 80 , 4.0 )) plt . xlabel ( \"Age\" ) plt . ylabel ( \"Insurance claim\" ) plt . yticks (( 0 , 1 ), labels = ( 'No' , 'Yes' )) plt . legend () plt . show () Post exercise question: In this exercise, you may have had to stumble around to find the right values of $\\beta_0$ and $\\beta_1$ to get accurate results. Although you may have used visual inspection to find a good fit, in most problems you would need a quantative method to measure the performance of your model. ( Loss function ) Which of the following below are NOT possible ways of quantifying the performance of the model. A. Compute the mean squared error loss of the predicted labels. B. Evaluate the log-likelihood for this Bernoulli response variable. C. Go the the temple of Apollo at Delphi, and ask the high priestess Pythia D. Compute the total number of misclassified labels. In [0]: ### edTest(test_quiz) ### # Put down your answers in a string format below (using quotes) # for. eg, if you think the options are 'A' & 'B', input below as \"A,B\" answer = ___","tags":"labs","url":"labs/lecture-15/notebook/"},{"title":"Lecture 15:  kNN classification and Logistic Regression I","text":"In [0]: # import libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.neighbors import KNeighborsClassifier Read-in the dataset In [0]: # Read the \"Heart.csv\" dataset and take a quick look heart = pd . read_csv ( 'Heart.csv' ) # Force the response into a binary indicator: heart [ 'AHD' ] = 1 * ( heart [ 'AHD' ] == \"Yes\" ) print ( heart . shape ) In [0]: # split into train and validation heart_train , heart_val = train_test_split ( heart , train_size = 0.75 , random_state = 109 ) print ( heart_train . shape , heart_val . shape ) $k$-NN model fitting Define and fit a $k$-NN classification model with $k=20$ to predict AHD from Age . In [0]: # select variables for model estimation: be careful of format # (aka, single or double square brackets) x_train = heart_train [ ____ ] y_train = heart_train [ ____ ] # define the model knn20 = KNeighborsClassifier ( ___ ) # fit to the data knn20 . fit ( ___ , ___ ) $k$-NN prediction Perform some simple predictions: both the pure classifications and the probability estimates. In [0]: ### edTest(test_knn) ### # there are two types of predictions in classification models in sklearn # model.predict for pure classifications, and model.predict_proba for probabilities # create the predictions based on the train data yhat20_class = knn20 . predict ( ___ ) yhat20_prob = knn20 . predict_proba ( ___ ) # print out the first 10 predictions for the actual data print ( yhat20_class [ 1 : 10 ]) print ( yhat20_prob [ 1 : 10 ]) What do you notice about the probability estimates? Which 'column' is which? How could you manually create yhat20_class from yhat20_prob ? Your answer here Simple logisitc regression model fitting Define and fit a $k$-NN classification model with $k=20$ to predict AHD from Age . In [0]: ### edTest(test_logit) ### # Create a logistic regression model, with 'none' as the penalty logit1 = LogisticRegression ( penalty = ____ , max_iter = 1000 ) #Fit the model using the training set logit1 . fit ( ____ , ____ ) # Get the coefficient estimates print ( \"Logistic Regression Estimated Betas (B0,B1):\" , logit1 . ____ , logit1 . ____ ) Interpret the Coefficient Estimates Write down the logistic regression model below (edit the provided latex formula). Use this formula to answer: What is the estimated odds that a 60 year old will have AHD in the ICU? What is the probability? $$ \\ln\\left( \\frac{P(Y=1)}{1-P(Y=1)} \\right) = \\hat{\\beta}_0 + \\hat{\\beta}_1 X$$ your answer here In [0]: # Make the predictions on the training & validation set # Be careful as to how you define the new observation. Hint: double brackets is one way to do it logit1 . predict ( ____ ) Accuracy computation $k$-NN and logistic accuracy In [0]: ### edTest(test_accuracy) ### # Define the equivalent validation variables from `heart_val` x_val = heart_val [ ____ ] y_val = heart_val [ ____ ] # Compute the training & validation accuracy using the estimator.score() function knn20_train_accuracy = knn20 . score ( x_train , y_train ) knn20_val_accuracy = knn20 . score ( x_val , y_val ) logit_train_accuracy = ____ logit_val_accuracy = ____ # Print the accuracies below print ( \"k-NN Train & Validation Accuracy:\" , knn20_train_accuracy , knn20_val_accuracy ) print ( \"Logisitic Train & Validation Accuracy:\" , logit_train_accuracy , logit_val_accuracy ) Plot the predictions Use a 'dummy' x variable for plotting for the two different models. Here we plot the train and validation data separately, and the 4 different types of predictions (2 for each model: pure classification and probability estimation) In [0]: # set-up the dummy x for plotting: we extend it a little bit beyond the range of observed values x = np . linspace ( np . min ( heart [[ 'Age' ]]) - 10 , ____ + 10 , 200 ) # be careful in pulling off only the correct column of the probability calculations: use `[:,1]` yhat_class_knn20 = knn20 . predict ( x ) yhat_prob_knn20 = _____ yhat_class_logit = logit1 . predict ( x ) yhat_prob_logit = _____ # plot the observed data. Note: we offset the validation points to make them more clearly differentiated from train plt . plot ( x_train , y_train , 'o' , alpha = 0.1 , label = 'Train Data' ) plt . plot ( x_val , 0.94 * y_val + 0.03 , 'o' , alpha = 0.1 , label = 'Validation Data' ) # plot the predictions plt . plot ( x , yhat_class_knn20 , label = 'knn20 Classifications' ) plt . plot ( x , yhat_prob_knn20 , label = 'knn20 Probabilities' ) plt . plot ( ____ , ____ , label = 'logit1 Classifications' ) plt . plot ( ____ , ____ , ____ ) # put the lower-left part of the legend 5% to the right along the x-axis, and 45% up along the y-axis plt . legend ( loc = ( 0.05 , 0.45 )) # Don't forget your axis labels! plt . xlabel ( \"Age\" ) plt . ylabel ( \"Heart disease (AHD)\" ) plt . show () Post exercise questions Describe the estimated associations between AHD and Age based on these two models. Your answer here Which model apears to be more overfit to the training data? How do you know? How can this be resolved? Your answer here How could you engineer features for the logistic regression model so that the association between AHD and Age resembles the general trend in the knn20 model more similarly? Your answer here In [0]: # Your answer here Refit the models above to predict AHD from 3 predictors (please also play around with $k$): {python} ['MaxHR','Age','Sex'] Investigate the associations in each of the two models and evaluate the predictive accuracy of each of these models In [0]: # Your answer here","tags":"labs","url":"labs/lecture-15/notebook-2/"},{"title":"Lecture 15:  kNN classification and Logistic Regression I","text":"Slides Logistic Regression [PDF] Logistic Regression [PPTX] Exercises Lecture 15: 1 - Guesstimating Beta values for Logistic Regression [Notebook] Lecture 15: 2 - Simple k-NN Classification and Logistic Regression [Notebook]","tags":"lectures","url":"lectures/lecture15/"},{"title":"Lecture 14: Visualization","text":"CS109A Introduction to Data Science Lecture 14, Exercise: Visualization Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner In [6]: import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.datasets import load_boston In [7]: # load the boston housing dataset boston = load_boston () boston_pd = pd . DataFrame ( boston . data ) boston_pd . columns = boston . feature_names boston_pd . describe () Out[7]: CRIM ZN INDUS CHAS NOX RM AGE DIS RAD TAX PTRATIO B LSTAT count 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 506.000000 mean 3.613524 11.363636 11.136779 0.069170 0.554695 6.284634 68.574901 3.795043 9.549407 408.237154 18.455534 356.674032 12.653063 std 8.601545 23.322453 6.860353 0.253994 0.115878 0.702617 28.148861 2.105710 8.707259 168.537116 2.164946 91.294864 7.141062 min 0.006320 0.000000 0.460000 0.000000 0.385000 3.561000 2.900000 1.129600 1.000000 187.000000 12.600000 0.320000 1.730000 25% 0.082045 0.000000 5.190000 0.000000 0.449000 5.885500 45.025000 2.100175 4.000000 279.000000 17.400000 375.377500 6.950000 50% 0.256510 0.000000 9.690000 0.000000 0.538000 6.208500 77.500000 3.207450 5.000000 330.000000 19.050000 391.440000 11.360000 75% 3.677083 12.500000 18.100000 0.000000 0.624000 6.623500 94.075000 5.188425 24.000000 666.000000 20.200000 396.225000 16.955000 max 88.976200 100.000000 27.740000 1.000000 0.871000 8.780000 100.000000 12.126500 24.000000 711.000000 22.000000 396.900000 37.970000 In [8]: # our canonical example plt . figure ( figsize = ( 5 , 4 )) plt . hist ( boston . target ) plt . title ( 'Boston Housing Prices' ) plt . xlabel ( 'Price ($1000s)' ) plt . ylabel ( '# of Houses' ) plt . show () In [4]: # YOUR FIRST PLOT In [5]: # YOUR SECOND PLOT","tags":"labs","url":"labs/lecture-14/notebook/"},{"title":"Lecture 14: Visualization","text":"Slides Visualization 1 [PDF] Visualization 2 [PDF] Visualization 3 [PDF] Visualization 4 [PDF] Visualization 5 [PDF] Visualization 6 [PDF] Visualization 7 [PDF] Visualization 8 [PDF] Visualization 9 [PDF] Visualization 10 [PDF] Visualization 11 [PDF] Visualization 12 [PDF] Visualization [PPTX] Exercises Lecture 14: Visualization [Notebook]","tags":"lectures","url":"lectures/lecture14/"},{"title":"Lecture 13: Thinking critically about models, data, and debugging","text":"CS109A Introduction to Data Science Lecture 13, Exercise 1: Debugging Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner In [0]: ########################################### ## DO NOT EDIT ANYTHING IN THIS CELL ## ########################################### import matplotlib.pyplot as plt import math import numpy as np from sklearn.datasets import load_boston from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures # ensures consistency between everyone's runs np . random . seed ( 100 ) ########################################### ## DO NOT EDIT ANYTHING IN THIS CELL ## ########################################### REMINDER : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. In [0]: # function to plot data with model predictions def plot_model ( x , y_true , y_pred ): plt . figure () ax = plt . gca () ax . scatter ( x , y_true ) # sort both x and y_pred for plot y_pred_sorted = [ y for _ , y in sorted ( zip ( x , y_pred ))] x_sorted = sorted ( x ) ax . plot ( x_sorted , y_pred_sorted , \"C1\" , linewidth = 4 ) # label the x-axis \"Crime\" and the y-axis \"Median Value\" ax . xlabel ( \"Crime\" ) ax . xlabel ( \"Median Value\" ) plt . show () return ax ### DO NOT EDIT THIS LINE In [0]: ### edTest(test_a) ### x , y = load_boston ( return_X_y = True ) # let's use just the 1st column (crime statistics) x = x [:, 0 ] # create a random train/test split with the test size being 0.2 and a random_state of 2 x_train , x_test = train_test_split ( x , test_size = 0.2 , random_state = 2 ) y_train , y_test = train_test_split ( y , test_size = 0.2 , random_state = 2 ) In [0]: ### edTest(test_b) ### # create polynomial (cubic) features poly = PolynomialFeatures ( 3 ) x_poly_train = poly . fit_transform ( x_train ) x_poly_test = poly . transform ( x_test ) # fit the model lr = LinearRegression () lr . fit ( x_poly_train , y_train ) In [0]: ### edTest(test_c) ### # make predictions y_pred = lr . predict ( x_poly_train ) # plot the model # NOTE: WHATEVER CODE YOU CHOOSE TO WRITE, MAKE SURE YOU ASSIGN # `ax` to equal the return value of plot_model() # otherwise, our unit test will be unable to auto-grade your work ax = plot_model ( x_poly_train , y_train , y_pred ) NOTE: The above cell checks to see if your predictions are good, but we cannot auto-check to see if your plot values are correct. Your plot's values should look as follows: In [0]: # inputs: a polynomial linear regression model # purpose: prints the intercept and coefficients of model # returns: the MSE of the polynomial model def get_metrics ( linreg ): # view coefficients print ( \"Intersects:\" , lr . intercept ) print ( \"Coefficients:\" , lr . coef ) # calculate test MSE mse = mean_squared_error ( y_test , y_pred ) return mse ## DO NOT EDIT THIS LINE In [0]: ### edTest(test_d) ### mse = get_metrics ( lr ) ## DO NOT EDIT THIS LINE","tags":"labs","url":"labs/lecture-13/notebook/"},{"title":"Lecture 13: Thinking critically about models, data, and debugging","text":"Slides Models, Data, and Debugging [PDF] Models, Data, and Debugging [PPTX] Exercises Lecture 13: 1 - Debugging [Notebook]","tags":"lectures","url":"lectures/lecture13/"},{"title":"S-Section 04: Regularization and Model Selection","text":"Jupyter Notebooks S-Section 4: Regularization and Model Selection","tags":"sections","url":"sections/section4/"},{"title":"S-Section 04: Regularization and Model Selection","text":"CS109A Introduction to Data Science Standard Section 4: Regularization and Model Selection Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Hayden Joy In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) from IPython.display import Image For this section, our goal is to get you familiarized with Regularization in Multiple Linear Regression and to start thinking about Model and Hyper-Parameter Selection. Specifically, we will: Load in the King County House Price Dataset Perform some basic EDA Split the data up into a training, validation , and test set (we'll see why we need a validation set) Scale the variables (by standardizing or normalizing them) and seeing why we need to do this Make our multiple & polynomial regression models (like we did in the previous section) Learn what regularization is and how it can help Understand ridge and lasso regression Get an introduction to cross-validation using RidgeCV and LassoCV In [2]: # Data and Stats packages import numpy as np import pandas as pd pd . set_option ( 'max_columns' , 200 ) # Visualization packages import matplotlib.pyplot as plt import seaborn as sns sns . set () import warnings warnings . filterwarnings ( \"ignore\" ) from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn import preprocessing from sklearn.metrics import mean_squared_error from sklearn.metrics import r2_score #from prettytable import PrettyTable EDA: House Prices Data From Kaggle For our dataset, we'll be using the house price dataset from King County, WA . The dataset is from Kaggle . The task is to build a regression model to predict the price , based on different attributes. First, let's do some EDA. In [3]: # Load the dataset house_df = pd . read_csv ( '../data/kc_house_data.csv' ) house_df = house_df . sample ( frac = 1 , random_state = 42 )[ 0 : 4000 ] print ( house_df . shape ) print ( house_df . dtypes ) house_df . head () (4000, 21) id int64 date object price float64 bedrooms int64 bathrooms float64 sqft_living int64 sqft_lot int64 floors float64 waterfront int64 view int64 condition int64 grade int64 sqft_above int64 sqft_basement int64 yr_built int64 yr_renovated int64 zipcode int64 lat float64 long float64 sqft_living15 int64 sqft_lot15 int64 dtype: object Out[3]: id date price bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 735 2591820310 20141006T000000 365000.0 4 2.25 2070 8893 2.0 0 0 4 8 2070 0 1986 0 98058 47.4388 -122.162 2390 7700 2830 7974200820 20140821T000000 865000.0 5 3.00 2900 6730 1.0 0 0 5 8 1830 1070 1977 0 98115 47.6784 -122.285 2370 6283 4106 7701450110 20140815T000000 1038000.0 4 2.50 3770 10893 2.0 0 2 3 11 3770 0 1997 0 98006 47.5646 -122.129 3710 9685 16218 9522300010 20150331T000000 1490000.0 3 3.50 4560 14608 2.0 0 2 3 12 4560 0 1990 0 98034 47.6995 -122.228 4050 14226 19964 9510861140 20140714T000000 711000.0 3 2.50 2550 5376 2.0 0 0 3 9 2550 0 2004 0 98052 47.6647 -122.083 2250 4050 Now let's check for null values and look at the datatypes within the dataset. In [4]: house_df . info () Int64Index: 4000 entries, 735 to 3455 Data columns (total 21 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 4000 non-null int64 1 date 4000 non-null object 2 price 4000 non-null float64 3 bedrooms 4000 non-null int64 4 bathrooms 4000 non-null float64 5 sqft_living 4000 non-null int64 6 sqft_lot 4000 non-null int64 7 floors 4000 non-null float64 8 waterfront 4000 non-null int64 9 view 4000 non-null int64 10 condition 4000 non-null int64 11 grade 4000 non-null int64 12 sqft_above 4000 non-null int64 13 sqft_basement 4000 non-null int64 14 yr_built 4000 non-null int64 15 yr_renovated 4000 non-null int64 16 zipcode 4000 non-null int64 17 lat 4000 non-null float64 18 long 4000 non-null float64 19 sqft_living15 4000 non-null int64 20 sqft_lot15 4000 non-null int64 dtypes: float64(5), int64(15), object(1) memory usage: 687.5+ KB In [5]: house_df . describe () Out[5]: id price bedrooms bathrooms sqft_living sqft_lot floors waterfront view condition grade sqft_above sqft_basement yr_built yr_renovated zipcode lat long sqft_living15 sqft_lot15 count 4.000000e+03 4.000000e+03 4000.000000 4000.000000 4000.000000 4.000000e+03 4000.000000 4000.000000 4000.000000 4000.000000 4000.000000 4000.000000 4000.00000 4000.000000 4000.000000 4000.000000 4000.000000 4000.000000 4000.00000 4000.00000 mean 4.586542e+09 5.497522e+05 3.379250 2.116563 2096.645250 1.616511e+04 1.475000 0.007750 0.232500 3.420750 7.668250 1792.465000 304.18025 1970.564250 89.801500 98078.035500 47.560091 -122.214060 1997.75900 12790.67800 std 2.876700e+09 3.890505e+05 0.922568 0.783175 957.785141 5.120888e+04 0.530279 0.087703 0.768174 0.646393 1.194173 849.986192 455.26354 29.141872 413.760082 54.073374 0.139070 0.141879 701.60987 26085.20301 min 1.000102e+06 8.250000e+04 0.000000 0.000000 384.000000 5.720000e+02 1.000000 0.000000 0.000000 1.000000 4.000000 384.000000 0.00000 1900.000000 0.000000 98001.000000 47.155900 -122.515000 620.00000 659.00000 25% 2.126074e+09 3.249500e+05 3.000000 1.750000 1420.000000 5.200000e+03 1.000000 0.000000 0.000000 3.000000 7.000000 1180.000000 0.00000 1951.000000 0.000000 98033.000000 47.468175 -122.328000 1490.00000 5200.00000 50% 3.889350e+09 4.550000e+05 3.000000 2.250000 1920.000000 7.675000e+03 1.000000 0.000000 0.000000 3.000000 7.000000 1550.000000 0.00000 1974.500000 0.000000 98065.000000 47.573800 -122.231000 1840.00000 7628.00000 75% 7.334526e+09 6.541250e+05 4.000000 2.500000 2570.000000 1.087125e+04 2.000000 0.000000 0.000000 4.000000 8.000000 2250.000000 590.00000 1995.000000 0.000000 98118.000000 47.679100 -122.127000 2370.00000 10240.00000 max 9.842300e+09 5.570000e+06 11.000000 8.000000 13540.000000 1.651359e+06 3.500000 1.000000 4.000000 5.000000 13.000000 9410.000000 4130.00000 2015.000000 2015.000000 98199.000000 47.777500 -121.315000 5790.00000 560617.00000 Let's choose a subset of columns here. NOTE : The way I'm selecting columns here is not principled and is just for convenience. In your homework assignments (and in real life), we expect you to choose columns more rigorously. bedrooms bathrooms sqft_living sqft_lot floors sqft_above sqft_basement lat long price : Our response variable In [6]: cols_of_interest = [ 'bedrooms' , 'bathrooms' , 'sqft_living' , 'sqft_lot' , 'floors' , 'sqft_above' , 'sqft_basement' , 'lat' , 'long' , 'price' ] house_df = house_df [ cols_of_interest ] # Convert house price to 1000s of dollars house_df [ 'price' ] = house_df [ 'price' ] / 1000 Let's see how the response variable ( price ) is distributed In [7]: fig , ax = plt . subplots ( figsize = ( 12 , 5 )) ax . hist ( house_df [ 'price' ], bins = 100 ) ax . set_title ( 'Histogram of house price (in 1000s of dollars)' ); In [8]: # This takes a bit of time but is worth it!! sns . pairplot ( house_df ); Train-Validation-Test Split Up until this point, we have only had a train-test split. Why are we introducing a validation set? What's the point? This is the general idea: Training Set : Data you have seen. You train different types of models with various different hyper-parameters and regularization parameters on this data. Validation Set : Used to compare different models. We use this step to tune our hyper-parameters i.e. find the optimal set of hyper-parameters (such as $k$ for k-NN or our $\\beta_i$ values or number of degrees of our polynomial for linear regression). Pick your best model here. Test Set : Using the best model from the previous step, simply report the score e.g. $R&#94;2$ score, MSE or any metric that you care about, of that model on your test set. DON'T TUNE YOUR PARAMETERS HERE! . Why, I hear you ask? Because we want to know how our model might do on data it hasn't seen before. We don't have access to this data (because it may not exist yet) but the test set, which we haven't seen or touched so far, is a good way to mimic this new data. Let's do 60% train, 20% validation, 20% test for this dataset. In [9]: from sklearn.model_selection import train_test_split # first split the data into a train-test split and don't touch the test set yet train_df , test_df = train_test_split ( house_df , test_size = 0.2 , random_state = 42 ) # next, split the training set into a train-validation split # the test-size is 0.25 since we are splitting 80% of the data into 20% and 60% overall train_df , val_df = train_test_split ( train_df , test_size = 0.25 , random_state = 42 ) print ( 'Train Set: {0:0.2f} %' . format ( 100 * train_df . size / house_df . size )) print ( 'Validation Set: {0:0.2f} %' . format ( 100 * val_df . size / house_df . size )) print ( 'Test Set: {0:0.2f} %' . format ( 100 * test_df . size / house_df . size )) Train Set: 60.00% Validation Set: 20.00% Test Set: 20.00% Modeling In the last section , we went over the mechanics of Multiple Linear Regression and created models that had interaction terms and polynomial terms. Specifically, we dealt with the following sorts of models. $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_M x_M + \\epsilon $$ Let's adopt a similar process here and get a few different models. Creating a Design Matrix From our model setup in the equation in the previous section, we obtain the following: $$ Y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}, \\quad X = \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\\\ \\end{bmatrix} = \\begin{bmatrix} x_{1,1} & x_{1,2} & \\dots & x_{1,M} \\\\ x_{2,1} & x_{2,2} & \\dots & x_{2,M} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ x_{n,1} & x_{n,2} & \\dots & x_{n,M} \\\\ \\end{bmatrix}, \\quad \\beta = \\begin{bmatrix} \\beta_1 \\\\ \\beta_2 \\\\ \\vdots \\\\ \\beta_M \\end{bmatrix}, \\quad \\epsilon = \\begin{bmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\vdots \\\\ \\epsilon_n \\end{bmatrix}, $$ $X$ is an n$\\times$M matrix: this is our design matrix (and ${x_1}$ is a vector consisting of the values of the predictors for the first data point), $\\beta$ is an M-dimensional vector (an M$\\times$1 matrix), and $Y$ is an n-dimensional vector (an n$\\times$1 matrix). In addition, we know that $\\epsilon$ is an n-dimensional vector (an n$\\times$1 matrix). We have $M$ predictors and $n$ data points. In [10]: X = train_df [ cols_of_interest ] y = train_df [ 'price' ] print ( X . shape ) print ( y . shape ) (2400, 10) (2400,) Scaling our Design Matrix Warm-Up Exercise $$\\text{Euclidean Distance} : \\sqrt{ (X_{1A}-X_{1B})&#94;2 + (X_{2A} - X_{2B})&#94;2 }$$ Warm-Up Exercise: for which of the following do the units of the predictors matter (e.g., trip length in minutes vs seconds; temperature in F or C)? A similar question would be: for which of these models do the magnitudes of values taken by different predictors matter? (We will go over Ridge and Lasso Regression in greater detail later) k-NN (Nearest Neighbors regression) Linear regression Lasso regression Ridge regression Solutions kNN: yes . Scaling affects distance metric, which determines what \"neighbor\" means Linear regression: no . Multiply predictor by $c$ -> divide coef by $c$. Lasso: yes : If we divided coef by $c$, then corresponding penalty term is also divided by $c$. Ridge: yes : Same as Lasso, except penalty divided by $c&#94;2$. Standard Scaler (Standardization) Here's the scikit-learn implementation of the standard scaler. What is it doing though? Hint: you may have seen this in STAT 110 or another statistics course multiple times. $$ z = \\frac{x-\\mu}{\\sigma} $$ In the above setup: $z$ is the standardized variable $x$ is the variable before standardization $\\mu$ is the mean of the variable before standardization $\\sigma$ is the standard deviation of the variable before standardization Let's see an example of how this works: In [11]: from sklearn.preprocessing import StandardScaler x = house_df [ 'sqft_living' ] mu = x . mean () sigma = x . std () z = ( x - mu ) / sigma # reshaping x to be a n by 1 matrix since that's how scikit learn likes data for standardization x_reshaped = np . array ( x ) . reshape ( - 1 , 1 ) z_sklearn = StandardScaler () . fit_transform ( x_reshaped ) # Plotting the histogram of the variable before standardization fig , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 24 , 5 )) ax = ax . ravel () ax [ 0 ] . hist ( x , bins = 100 ) ax [ 0 ] . set_title ( 'Histogram of sqft_living before standardization' ) ax [ 1 ] . hist ( z , bins = 100 ) ax [ 1 ] . set_title ( 'Manually standardizing sqft_living' ) ax [ 2 ] . hist ( z_sklearn , bins = 100 ) ax [ 2 ] . set_title ( 'Standardizing sqft_living using scikit learn' ); # making things a dataframe to check if they work pd . DataFrame ({ 'x' : x , 'z_manual' : z , 'z_sklearn' : z_sklearn . flatten ()}) . describe () Out[11]: x z_manual z_sklearn count 4000.000000 4.000000e+03 4.000000e+03 mean 2096.645250 -2.775558e-17 -4.096723e-17 std 957.785141 1.000000e+00 1.000125e+00 min 384.000000 -1.788131e+00 -1.788355e+00 25% 1420.000000 -7.064687e-01 -7.065571e-01 50% 1920.000000 -1.844310e-01 -1.844540e-01 75% 2570.000000 4.942181e-01 4.942799e-01 max 13540.000000 1.194773e+01 1.194922e+01 Min-Max Scaler (Normalization) Here's the scikit-learn implementation of the standard scaler. What is it doing though? $$ x_{new} = \\frac{x-x_{min}}{x_{max}-x_{min}} $$ In the above setup: $x_{new}$ is the normalized variable $x$ is the variable before normalized $x_{max}$ is the max value of the variable before normalization $x_{min}$ is the min value of the variable before normalization Let's see an example of how this works: In [12]: from sklearn.preprocessing import MinMaxScaler x = house_df [ 'sqft_living' ] x_new = ( x - x . min ()) / ( x . max () - x . min ()) # reshaping x to be a n by 1 matrix since that's how scikit learn likes data for normalization x_reshaped = np . array ( x ) . reshape ( - 1 , 1 ) x_new_sklearn = MinMaxScaler () . fit_transform ( x_reshaped ) # Plotting the histogram of the variable before normalization fig , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 24 , 5 )) ax = ax . ravel () ax [ 0 ] . hist ( x , bins = 100 ) ax [ 0 ] . set_title ( 'Histogram of sqft_living before normalization' ) ax [ 1 ] . hist ( x_new , bins = 100 ) ax [ 1 ] . set_title ( 'Manually normalizing sqft_living' ) ax [ 2 ] . hist ( x_new_sklearn , bins = 100 ) ax [ 2 ] . set_title ( 'Normalizing sqft_living using scikit learn' ); # making things a dataframe to check if they work pd . DataFrame ({ 'x' : x , 'x_new_manual' : x_new , 'x_new_sklearn' : x_new_sklearn . flatten ()}) . describe () Out[12]: x x_new_manual x_new_sklearn count 4000.000000 4000.000000 4000.000000 mean 2096.645250 0.130180 0.130180 std 957.785141 0.072802 0.072802 min 384.000000 0.000000 0.000000 25% 1420.000000 0.078747 0.078747 50% 1920.000000 0.116753 0.116753 75% 2570.000000 0.166160 0.166160 max 13540.000000 1.000000 1.000000 In [27]: fig , ax = plt . subplots ( nrows = 1 , ncols = 1 , figsize = ( 16 , 5 )) ax . hist (( x - np . mean ( x )) / np . std ( x ), bins = 100 , alpha = 0.25 , label = \"sq feet standardized\" , color = \"blue\" ) ax . hist ( x_new , bins = 100 , label = \"sq feet normalized\" , color = \"red\" , alpha = . 5 ) ax . set_title ( 'Histogram of sqft_living Comparing normalization and standardization' ) plt . legend () Out[27]: The million dollar question Should I standardize or normalize my data? This , this and this are useful resources that I highly recommend. But in a nutshell, what they say is the following: Pros of Normalization Normalization (which makes your data go from 0-1) is widely used in image processing and computer vision, where pixel intensities are non-negative and are typically scaled from a 0-255 scale to a 0-1 range for a lot of different algorithms. Normalization is also very useful in neural networks (which we will see later in the course) as it leads to the algorithms converging faster. #next semester also we will see Normalization is useful when your data does not have a discernible distribution and you are not making assumptions about your data's distribution. Pros of Standardization Standardization maintains outliers (do you see why?) whereas normalization makes outliers less obvious. In applications where outliers are useful, standardization should be done. Standardization is useful when you assume your data comes from a Gaussian distribution (or something that is approximately Gaussian). Some General Advice We use the data to calculate the parameters for standardization ($\\mu$ and $\\sigma$) and for normalization ($x_{min}$ and $x_{max}$). Make sure these parameters are learned on the training set i.e use the training set parameters even when normalizing/standardizing the test set. In sklearn terms, fit your scaler on the training set and use the scaler to transform your test set and validation set ( don't re-fit your scaler on test set data! ). The point of standardization and normalization is to make your variables take on a more manageable scale. You should ideally standardize or normalize all your variables at the same time. Standardization and normalization is not always needed and is not an automatic thing you have to do on any data science homework!! Do so sparingly and try to justify why this is needed. Interpreting Coefficients A great quote from here [Standardization] makes it so the intercept term is interpreted as the expected value of 𝑌𝑖 when the predictor values are set to their means. Otherwise, the intercept is interpreted as the expected value of 𝑌𝑖 when the predictors are set to 0, which may not be a realistic or interpretable situation (e.g. what if the predictors were height and weight?) Standardizing our Design Matrix In [28]: features = [ 'bedrooms' , 'bathrooms' , 'sqft_living' , 'sqft_lot' , 'floors' , 'sqft_above' , 'sqft_basement' , 'lat' , 'long' ] X_train = train_df [ features ] y_train = np . array ( train_df [ 'price' ]) . reshape ( - 1 , 1 ) X_val = val_df [ features ] y_val = np . array ( val_df [ 'price' ]) . reshape ( - 1 , 1 ) X_test = test_df [ features ] y_test = np . array ( test_df [ 'price' ]) . reshape ( - 1 , 1 ) scaler = StandardScaler () . fit ( X_train ) # This converts our matrices into numpy matrices X_train_t = scaler . transform ( X_train ) X_val_t = scaler . transform ( X_val ) X_test_t = scaler . transform ( X_test ) # Making the numpy matrices pandas dataframes X_train_df = pd . DataFrame ( X_train_t , columns = features ) X_val_df = pd . DataFrame ( X_val_t , columns = features ) X_test_df = pd . DataFrame ( X_test_t , columns = features ) display ( X_train_df . describe ()) display ( X_val_df . describe ()) display ( X_test_df . describe ()) bedrooms bathrooms sqft_living sqft_lot floors sqft_above sqft_basement lat long count 2.400000e+03 2.400000e+03 2.400000e+03 2.400000e+03 2.400000e+03 2.400000e+03 2.400000e+03 2.400000e+03 2.400000e+03 mean 2.250977e-16 4.503342e-17 1.471971e-16 -2.406640e-17 2.680263e-16 -8.234154e-18 -1.709281e-16 4.928733e-14 4.897231e-14 std 1.000208e+00 1.000208e+00 1.000208e+00 1.000208e+00 1.000208e+00 1.000208e+00 1.000208e+00 1.000208e+00 1.000208e+00 min -3.618993e+00 -2.677207e+00 -1.766429e+00 -3.364203e-01 -8.897850e-01 -1.636285e+00 -6.704685e-01 -2.937091e+00 -2.084576e+00 25% -4.009185e-01 -4.598398e-01 -7.087691e-01 -2.324570e-01 -8.897850e-01 -7.089826e-01 -6.704685e-01 -6.732889e-01 -8.086270e-01 50% -4.009185e-01 1.736938e-01 -1.933403e-01 -1.774091e-01 -8.897850e-01 -2.895998e-01 -6.704685e-01 8.468878e-02 -1.278830e-01 75% 6.717731e-01 4.904606e-01 4.973342e-01 -1.033061e-01 9.975186e-01 5.375162e-01 6.315842e-01 8.607566e-01 6.455277e-01 max 7.107923e+00 7.459330e+00 1.179553e+01 1.945618e+01 3.828474e+00 8.878574e+00 8.291994e+00 1.560846e+00 6.062967e+00 bedrooms bathrooms sqft_living sqft_lot floors sqft_above sqft_basement lat long count 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 mean 0.018772 0.041444 0.024401 0.016506 0.026737 0.044415 -0.031370 -0.056059 0.016900 std 0.982683 0.997594 0.989079 1.074079 0.991645 0.993807 0.999638 1.008010 1.028649 min -2.546302 -1.726907 -1.626232 -0.328715 -0.889785 -1.477851 -0.670469 -2.693960 -2.141602 25% -0.400918 -0.459840 -0.677843 -0.234254 -0.889785 -0.685684 -0.670469 -0.737509 -0.815755 50% -0.400918 0.173694 -0.172723 -0.177521 0.053867 -0.266301 -0.670469 0.031504 -0.088678 75% 0.671773 0.490461 0.487026 -0.113533 0.997519 0.595764 0.550206 0.817340 0.597412 max 8.180614 4.925195 7.321611 21.716593 2.884822 5.139078 5.839795 1.554333 6.369480 bedrooms bathrooms sqft_living sqft_lot floors sqft_above sqft_basement lat long count 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 800.000000 mean 0.010727 -0.018346 -0.029080 0.052808 0.006684 -0.021866 -0.020484 -0.005631 0.000897 std 0.965422 0.963162 0.946367 1.569619 1.012587 0.955805 0.938793 1.022830 1.028155 min -3.618993 -2.677207 -1.657158 -0.335005 -0.889785 -1.524449 -0.670469 -2.656332 -1.778063 25% -0.400918 -0.459840 -0.701038 -0.229160 -0.889785 -0.708983 -0.670469 -0.624084 -0.789024 50% -0.400918 0.173694 -0.183032 -0.174051 -0.889785 -0.295425 -0.670469 0.174054 -0.174216 75% 0.671773 0.490461 0.417443 -0.100683 0.997519 0.479269 0.588182 0.842124 0.604540 max 4.962540 3.658128 5.785633 36.746809 3.828474 5.010933 4.016921 1.544926 6.412249 In [29]: scaler = StandardScaler () . fit ( y_train ) y_train = scaler . transform ( y_train ) y_val = scaler . transform ( y_val ) y_test = scaler . transform ( y_test ) One-Degree Polynomial Model In [30]: import statsmodels.api as sm from statsmodels.regression.linear_model import OLS model_1 = OLS ( np . array ( y_train ) . reshape ( - 1 , 1 ), sm . add_constant ( X_train_df )) . fit () model_1 . summary () Out[30]: OLS Regression Results Dep. Variable: y R-squared: 0.586 Model: OLS Adj. R-squared: 0.584 Method: Least Squares F-statistic: 422.3 Date: Fri, 02 Oct 2020 Prob (F-statistic): 0.00 Time: 11:01:43 Log-Likelihood: -2348.5 No. Observations: 2400 AIC: 4715. Df Residuals: 2391 BIC: 4767. Df Model: 8 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const -5.118e-15 0.013 -3.89e-13 1.000 -0.026 0.026 bedrooms -0.1592 0.017 -9.505 0.000 -0.192 -0.126 bathrooms 0.0422 0.022 1.914 0.056 -0.001 0.085 sqft_living 0.4011 0.011 36.238 0.000 0.379 0.423 sqft_lot -0.0058 0.014 -0.420 0.675 -0.033 0.021 floors -0.0470 0.017 -2.690 0.007 -0.081 -0.013 sqft_above 0.3866 0.013 30.254 0.000 0.362 0.412 sqft_basement 0.1242 0.014 8.651 0.000 0.096 0.152 lat 0.2414 0.013 17.983 0.000 0.215 0.268 long -0.1388 0.014 -9.605 0.000 -0.167 -0.110 Omnibus: 1646.401 Durbin-Watson: 2.009 Prob(Omnibus): 0.000 Jarque-Bera (JB): 52596.394 Skew: 2.797 Prob(JB): 0.00 Kurtosis: 25.241 Cond. No. 8.22e+15 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 1.24e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. Two-Degree Polynomial Model In [31]: def add_square_terms ( df ): df = df . copy () cols = df . columns . copy () for col in cols : df [ ' {} &#94;2' . format ( col )] = df [ col ] ** 2 return df X_train_df_2 = add_square_terms ( X_train ) X_val_df_2 = add_square_terms ( X_val ) # Standardizing our added coefficients cols = X_train_df_2 . columns scaler = StandardScaler () . fit ( X_train_df_2 ) X_train_df_2 = pd . DataFrame ( scaler . transform ( X_train_df_2 ), columns = cols ) X_val_df_2 = pd . DataFrame ( scaler . transform ( X_val_df_2 ), columns = cols ) print ( X_train_df . shape , X_train_df_2 . shape ) # Also check using the describe() function that the mean and standard deviations are the way we want them X_train_df_2 . head () (2400, 9) (2400, 18) Out[31]: bedrooms bathrooms sqft_living sqft_lot floors sqft_above sqft_basement lat long bedrooms&#94;2 bathrooms&#94;2 sqft_living&#94;2 sqft_lot&#94;2 floors&#94;2 sqft_above&#94;2 sqft_basement&#94;2 lat&#94;2 long&#94;2 0 -0.400918 -0.459840 -0.533523 -0.184294 -0.889785 -0.243002 -0.670469 -0.261919 -1.179294 -0.462425 -0.498149 -0.435619 -0.081332 -0.820725 -0.317640 -0.429442 -0.263451 1.180094 1 -0.400918 1.123994 0.919986 -0.129729 0.997519 1.399581 -0.670469 0.525365 0.289117 -0.462425 0.962623 0.551247 -0.079773 0.882097 1.104202 -0.429442 0.524670 -0.289785 2 0.671773 0.490461 -0.049020 -0.167446 -0.889785 -0.860426 1.499619 0.720739 0.545733 0.533184 0.286055 -0.174327 -0.080898 -0.820725 -0.625213 0.965746 0.720531 -0.546402 3 -0.400918 0.490461 -0.121180 -0.035583 -0.889785 0.222979 -0.670469 0.066599 -0.088678 -0.462425 0.286055 -0.217531 -0.076044 -0.820725 -0.003426 -0.429442 0.065197 0.088151 4 -0.400918 0.490461 -0.327352 -0.187215 -0.889785 -0.452693 0.154165 -1.411729 0.232092 -0.462425 0.286055 -0.332701 -0.081403 -0.820725 -0.436000 -0.227977 -1.411246 -0.232748 In [32]: model_2 = OLS ( np . array ( y_train ) . reshape ( - 1 , 1 ), sm . add_constant ( X_train_df_2 )) . fit () model_2 . summary () Out[32]: OLS Regression Results Dep. Variable: y R-squared: 0.612 Model: OLS Adj. R-squared: 0.609 Method: Least Squares F-statistic: 220.8 Date: Fri, 02 Oct 2020 Prob (F-statistic): 0.00 Time: 11:01:45 Log-Likelihood: -2269.9 No. Observations: 2400 AIC: 4576. Df Residuals: 2382 BIC: 4680. Df Model: 17 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const -6.174e-12 0.013 -4.84e-10 1.000 -0.025 0.025 bedrooms -0.1271 0.058 -2.186 0.029 -0.241 -0.013 bathrooms 0.1537 0.060 2.569 0.010 0.036 0.271 sqft_living 0.3406 0.026 12.895 0.000 0.289 0.392 sqft_lot -0.0278 0.030 -0.920 0.358 -0.087 0.032 floors -0.1006 0.087 -1.151 0.250 -0.272 0.071 sqft_above 0.2460 0.036 6.809 0.000 0.175 0.317 sqft_basement 0.2587 0.033 7.758 0.000 0.193 0.324 lat 83.5852 8.613 9.705 0.000 66.696 100.474 long -7.0103 16.124 -0.435 0.664 -38.628 24.608 bedrooms&#94;2 -0.0117 0.057 -0.207 0.836 -0.123 0.099 bathrooms&#94;2 -0.1395 0.061 -2.293 0.022 -0.259 -0.020 sqft_living&#94;2 0.2606 0.104 2.498 0.013 0.056 0.465 sqft_lot&#94;2 0.0395 0.029 1.366 0.172 -0.017 0.096 floors&#94;2 0.0449 0.083 0.539 0.590 -0.118 0.208 sqft_above&#94;2 0.0384 0.105 0.366 0.714 -0.167 0.244 sqft_basement&#94;2 -0.2640 0.049 -5.424 0.000 -0.359 -0.169 lat&#94;2 -83.3483 8.612 -9.678 0.000 -100.237 -66.460 long&#94;2 -6.8786 16.124 -0.427 0.670 -38.498 24.741 Omnibus: 1594.128 Durbin-Watson: 2.011 Prob(Omnibus): 0.000 Jarque-Bera (JB): 41401.592 Skew: 2.739 Prob(JB): 0.00 Kurtosis: 22.596 Cond. No. 7.38e+15 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 2.96e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. Three-Degree Polynomial Model In [33]: # generalizing our function from above def add_square_and_cube_terms ( df ): df = df . copy () cols = df . columns . copy () for col in cols : df [ ' {} &#94;2' . format ( col )] = df [ col ] ** 2 df [ ' {} &#94;3' . format ( col )] = df [ col ] ** 3 return df X_train_df_3 = add_square_and_cube_terms ( X_train_df ) X_val_df_3 = add_square_and_cube_terms ( X_val_df ) # Standardizing our added coefficients cols = X_train_df_3 . columns scaler = StandardScaler () . fit ( X_train_df_3 ) X_train_df_3 = pd . DataFrame ( scaler . transform ( X_train_df_3 ), columns = cols ) X_val_df_3 = pd . DataFrame ( scaler . transform ( X_val_df_3 ), columns = cols ) print ( X_train_df . shape , X_train_df_3 . shape ) # Also check using the describe() function that the mean and standard deviations are the way we want them X_train_df_3 . head () (2400, 9) (2400, 27) Out[33]: bedrooms bathrooms sqft_living sqft_lot floors sqft_above sqft_basement lat long bedrooms&#94;2 bedrooms&#94;3 bathrooms&#94;2 bathrooms&#94;3 sqft_living&#94;2 sqft_living&#94;3 sqft_lot&#94;2 sqft_lot&#94;3 floors&#94;2 floors&#94;3 sqft_above&#94;2 sqft_above&#94;3 sqft_basement&#94;2 sqft_basement&#94;3 lat&#94;2 lat&#94;3 long&#94;2 long&#94;3 0 -0.400918 -0.459840 -0.533523 -0.184294 -0.889785 -0.243002 -0.670469 -0.261919 -1.179294 -0.395932 -0.062775 -0.343010 -0.069010 -0.200089 -0.059468 -0.075186 -0.051101 -0.171243 -0.342467 -0.358041 -0.097806 -0.239718 -0.142904 -0.807917 0.160517 0.235473 -0.384346 1 -0.400918 1.123994 0.919986 -0.129729 0.997519 1.399581 -0.670469 0.525365 0.289117 -0.395932 -0.062775 0.114560 0.046943 -0.042970 -0.032841 -0.076520 -0.051082 -0.004075 0.091496 0.364844 0.066486 -0.239718 -0.142904 -0.628007 0.218528 -0.552269 -0.127566 2 0.671773 0.490461 -0.049020 -0.167446 -0.889785 -0.860426 1.499619 0.720739 0.545733 -0.258865 -0.027621 -0.330352 -0.052563 -0.279035 -0.055126 -0.075648 -0.051094 -0.171243 -0.342467 -0.098806 -0.134926 0.543850 0.131547 -0.416828 0.300182 -0.423162 -0.106217 3 -0.400918 0.490461 -0.121180 -0.035583 -0.889785 0.222979 -0.670469 0.066599 -0.088678 -0.395932 -0.062775 -0.330352 -0.052563 -0.275599 -0.055174 -0.077731 -0.051073 -0.171243 -0.342467 -0.361592 -0.096290 -0.239718 -0.142904 -0.863576 0.167018 -0.597904 -0.131402 4 -0.400918 0.490461 -0.327352 -0.187215 -0.889785 -0.452693 0.154165 -1.411729 0.232092 -0.395932 -0.062775 -0.330352 -0.052563 -0.249734 -0.056126 -0.075102 -0.051102 -0.171243 -0.342467 -0.302532 -0.102481 -0.425128 -0.120115 0.861334 -0.834585 -0.570181 -0.129365 In [34]: model_3 = OLS ( np . array ( y_train ) . reshape ( - 1 , 1 ), sm . add_constant ( X_train_df_3 )) . fit () model_3 . summary () Out[34]: OLS Regression Results Dep. Variable: y R-squared: 0.698 Model: OLS Adj. R-squared: 0.695 Method: Least Squares F-statistic: 211.2 Date: Fri, 02 Oct 2020 Prob (F-statistic): 0.00 Time: 11:01:46 Log-Likelihood: -1967.6 No. Observations: 2400 AIC: 3989. Df Residuals: 2373 BIC: 4145. Df Model: 26 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const -9.563e-17 0.011 -8.48e-15 1.000 -0.022 0.022 bedrooms -0.0970 0.018 -5.494 0.000 -0.132 -0.062 bathrooms 0.1240 0.022 5.567 0.000 0.080 0.168 sqft_living 0.3296 0.015 21.655 0.000 0.300 0.359 sqft_lot 0.0446 0.042 1.066 0.286 -0.037 0.127 floors -0.1099 0.024 -4.620 0.000 -0.157 -0.063 sqft_above 0.3201 0.015 20.960 0.000 0.290 0.350 sqft_basement 0.0975 0.021 4.668 0.000 0.057 0.138 lat 0.5444 0.023 24.003 0.000 0.500 0.589 long -0.1476 0.016 -9.086 0.000 -0.179 -0.116 bedrooms&#94;2 -0.0467 0.018 -2.620 0.009 -0.082 -0.012 bedrooms&#94;3 0.0502 0.020 2.535 0.011 0.011 0.089 bathrooms&#94;2 0.0129 0.031 0.418 0.676 -0.048 0.074 bathrooms&#94;3 -0.1951 0.035 -5.531 0.000 -0.264 -0.126 sqft_living&#94;2 0.7470 0.075 9.907 0.000 0.599 0.895 sqft_living&#94;3 -0.6541 0.118 -5.535 0.000 -0.886 -0.422 sqft_lot&#94;2 -0.1336 0.112 -1.193 0.233 -0.353 0.086 sqft_lot&#94;3 0.1242 0.083 1.491 0.136 -0.039 0.288 floors&#94;2 -0.1112 0.057 -1.945 0.052 -0.223 0.001 floors&#94;3 0.1317 0.064 2.048 0.041 0.006 0.258 sqft_above&#94;2 -0.3946 0.067 -5.891 0.000 -0.526 -0.263 sqft_above&#94;3 0.5469 0.093 5.876 0.000 0.364 0.729 sqft_basement&#94;2 -0.0241 0.072 -0.336 0.737 -0.165 0.117 sqft_basement&#94;3 -0.1414 0.091 -1.546 0.122 -0.321 0.038 lat&#94;2 -0.3803 0.018 -21.416 0.000 -0.415 -0.345 lat&#94;3 -0.5534 0.028 -19.441 0.000 -0.609 -0.498 long&#94;2 -0.0943 0.024 -3.993 0.000 -0.141 -0.048 long&#94;3 0.0896 0.024 3.664 0.000 0.042 0.138 Omnibus: 1337.688 Durbin-Watson: 1.993 Prob(Omnibus): 0.000 Jarque-Bera (JB): 29677.038 Skew: 2.170 Prob(JB): 0.00 Kurtosis: 19.672 Cond. No. 2.27e+15 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 3.58e-27. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. N-Degree Polynomial Model In [35]: # generalizing our function from above def add_higher_order_polynomial_terms ( df , N = 7 ): df = df . copy () cols = df . columns . copy () for col in cols : for i in range ( 2 , N + 1 ): df [ ' {} &#94; {} ' . format ( col , i )] = df [ col ] ** i return df N = 6 X_train_df_N = add_higher_order_polynomial_terms ( X_train_df , N ) X_val_df_N = add_higher_order_polynomial_terms ( X_val_df , N ) # Standardizing our added coefficients cols = X_train_df_N . columns scaler = StandardScaler () . fit ( X_train_df_N ) X_train_df_N = pd . DataFrame ( scaler . transform ( X_train_df_N ), columns = cols ) X_val_df_N = pd . DataFrame ( scaler . transform ( X_val_df_N ), columns = cols ) print ( X_train_df . shape , X_train_df_N . shape ) # Also check using the describe() function that the mean and standard deviations are the way we want them X_train_df_N . head () (2400, 9) (2400, 54) Out[35]: bedrooms bathrooms sqft_living sqft_lot floors sqft_above sqft_basement lat long bedrooms&#94;2 bedrooms&#94;3 bedrooms&#94;4 bedrooms&#94;5 bedrooms&#94;6 bathrooms&#94;2 bathrooms&#94;3 bathrooms&#94;4 bathrooms&#94;5 bathrooms&#94;6 sqft_living&#94;2 sqft_living&#94;3 sqft_living&#94;4 sqft_living&#94;5 sqft_living&#94;6 sqft_lot&#94;2 sqft_lot&#94;3 sqft_lot&#94;4 sqft_lot&#94;5 sqft_lot&#94;6 floors&#94;2 floors&#94;3 floors&#94;4 floors&#94;5 floors&#94;6 sqft_above&#94;2 sqft_above&#94;3 sqft_above&#94;4 sqft_above&#94;5 sqft_above&#94;6 sqft_basement&#94;2 sqft_basement&#94;3 sqft_basement&#94;4 sqft_basement&#94;5 sqft_basement&#94;6 lat&#94;2 lat&#94;3 lat&#94;4 lat&#94;5 lat&#94;6 long&#94;2 long&#94;3 long&#94;4 long&#94;5 long&#94;6 0 -0.400918 -0.459840 -0.533523 -0.184294 -0.889785 -0.243002 -0.670469 -0.261919 -1.179294 -0.395932 -0.062775 -0.086938 -0.039031 -0.038513 -0.343010 -0.069010 -0.072891 -0.045728 -0.040692 -0.200089 -0.059468 -0.034406 -0.025039 -0.022277 -0.075186 -0.051101 -0.041706 -0.036934 -0.034107 -0.171243 -0.342467 -0.164549 -0.168232 -0.140792 -0.358041 -0.097806 -0.059279 -0.036309 -0.028245 -0.239718 -0.142904 -0.060674 -0.036484 -0.027295 -0.807917 0.160517 -0.409582 0.187027 -0.227089 0.235473 -0.384346 -0.058733 -0.071619 -0.038993 1 -0.400918 1.123994 0.919986 -0.129729 0.997519 1.399581 -0.670469 0.525365 0.289117 -0.395932 -0.062775 -0.086938 -0.039031 -0.038513 0.114560 0.046943 -0.054770 -0.042675 -0.040217 -0.042970 -0.032841 -0.032811 -0.024888 -0.022266 -0.076520 -0.051082 -0.041707 -0.036934 -0.034107 -0.004075 0.091496 -0.132280 -0.123447 -0.136263 0.364844 0.066486 -0.030525 -0.031619 -0.027497 -0.239718 -0.142904 -0.060674 -0.036484 -0.027295 -0.628007 0.218528 -0.396987 0.189912 -0.226509 -0.552269 -0.127566 -0.120948 -0.058487 -0.041612 2 0.671773 0.490461 -0.049020 -0.167446 -0.889785 -0.860426 1.499619 0.720739 0.545733 -0.258865 -0.027621 -0.084111 -0.038678 -0.038482 -0.330352 -0.052563 -0.072737 -0.045645 -0.040691 -0.279035 -0.055126 -0.034610 -0.025029 -0.022277 -0.075648 -0.051094 -0.041706 -0.036934 -0.034107 -0.171243 -0.342467 -0.164549 -0.168232 -0.140792 -0.098806 -0.134926 -0.055194 -0.036721 -0.028204 0.543850 0.131547 -0.012150 -0.026938 -0.025600 -0.416828 0.300182 -0.362861 0.200711 -0.223174 -0.423162 -0.106217 -0.118310 -0.058221 -0.041587 3 -0.400918 0.490461 -0.121180 -0.035583 -0.889785 0.222979 -0.670469 0.066599 -0.088678 -0.395932 -0.062775 -0.086938 -0.039031 -0.038513 -0.330352 -0.052563 -0.072737 -0.045645 -0.040691 -0.275599 -0.055174 -0.034609 -0.025030 -0.022277 -0.077731 -0.051073 -0.041707 -0.036934 -0.034107 -0.171243 -0.342467 -0.164549 -0.168232 -0.140792 -0.361592 -0.096290 -0.059287 -0.036308 -0.028245 -0.239718 -0.142904 -0.060674 -0.036484 -0.027295 -0.863576 0.167018 -0.410408 0.187114 -0.227098 -0.597904 -0.131402 -0.121172 -0.058499 -0.041613 4 -0.400918 0.490461 -0.327352 -0.187215 -0.889785 -0.452693 0.154165 -1.411729 0.232092 -0.395932 -0.062775 -0.086938 -0.039031 -0.038513 -0.330352 -0.052563 -0.072737 -0.045645 -0.040691 -0.249734 -0.056126 -0.034581 -0.025030 -0.022277 -0.075102 -0.051102 -0.041706 -0.036934 -0.034107 -0.171243 -0.342467 -0.164549 -0.168232 -0.140792 -0.302532 -0.102481 -0.058990 -0.036325 -0.028244 -0.425128 -0.120115 -0.062688 -0.036316 -0.027309 0.861334 -0.834585 0.289508 -0.204930 -0.005493 -0.570181 -0.129365 -0.121080 -0.058495 -0.041613 In [36]: model_N = OLS ( np . array ( y_train ) . reshape ( - 1 , 1 ), sm . add_constant ( X_train_df_N )) . fit () model_N . summary () Out[36]: OLS Regression Results Dep. Variable: y R-squared: 0.740 Model: OLS Adj. R-squared: 0.734 Method: Least Squares F-statistic: 128.5 Date: Fri, 02 Oct 2020 Prob (F-statistic): 0.00 Time: 11:01:47 Log-Likelihood: -1788.4 No. Observations: 2400 AIC: 3683. Df Residuals: 2347 BIC: 3989. Df Model: 52 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 7.511e-16 0.011 7.14e-14 1.000 -0.021 0.021 bedrooms -0.0634 0.025 -2.510 0.012 -0.113 -0.014 bathrooms -0.0059 0.034 -0.177 0.860 -0.072 0.060 sqft_living 0.3077 0.027 11.211 0.000 0.254 0.362 sqft_lot 0.3300 0.074 4.448 0.000 0.185 0.476 floors -0.0806 0.112 -0.718 0.473 -0.301 0.140 sqft_above 0.3124 0.028 11.091 0.000 0.257 0.368 sqft_basement 0.0659 0.073 0.899 0.369 -0.078 0.209 lat 1.1461 0.050 22.964 0.000 1.048 1.244 long -0.1710 0.031 -5.509 0.000 -0.232 -0.110 bedrooms&#94;2 -0.0460 0.033 -1.410 0.159 -0.110 0.018 bedrooms&#94;3 -0.0852 0.102 -0.831 0.406 -0.286 0.116 bedrooms&#94;4 -0.0524 0.127 -0.413 0.679 -0.301 0.196 bedrooms&#94;5 0.5475 0.338 1.619 0.106 -0.116 1.211 bedrooms&#94;6 -0.3952 0.267 -1.477 0.140 -0.920 0.129 bathrooms&#94;2 -0.1075 0.062 -1.744 0.081 -0.228 0.013 bathrooms&#94;3 0.8911 0.179 4.989 0.000 0.541 1.241 bathrooms&#94;4 1.4365 0.665 2.159 0.031 0.132 2.741 bathrooms&#94;5 -7.9201 1.377 -5.752 0.000 -10.620 -5.220 bathrooms&#94;6 5.8168 0.856 6.797 0.000 4.139 7.495 sqft_living&#94;2 0.5941 0.131 4.547 0.000 0.338 0.850 sqft_living&#94;3 -0.5976 0.832 -0.719 0.472 -2.228 1.033 sqft_living&#94;4 0.2231 5.224 0.043 0.966 -10.021 10.468 sqft_living&#94;5 3.0067 12.201 0.246 0.805 -20.919 26.933 sqft_living&#94;6 -3.9928 9.392 -0.425 0.671 -22.410 14.424 sqft_lot&#94;2 -1.9787 0.942 -2.101 0.036 -3.825 -0.132 sqft_lot&#94;3 2.2924 5.153 0.445 0.656 -7.813 12.397 sqft_lot&#94;4 7.5331 12.135 0.621 0.535 -16.264 31.330 sqft_lot&#94;5 -17.1041 12.680 -1.349 0.177 -41.969 7.760 sqft_lot&#94;6 8.9749 4.832 1.858 0.063 -0.500 18.450 floors&#94;2 -0.0739 0.060 -1.223 0.221 -0.192 0.045 floors&#94;3 0.1038 0.540 0.192 0.848 -0.956 1.163 floors&#94;4 0.0237 0.429 0.055 0.956 -0.818 0.865 floors&#94;5 -0.0140 0.392 -0.036 0.972 -0.782 0.754 floors&#94;6 -0.0340 0.360 -0.095 0.925 -0.739 0.671 sqft_above&#94;2 -0.4273 0.121 -3.527 0.000 -0.665 -0.190 sqft_above&#94;3 0.4173 0.468 0.891 0.373 -0.501 1.335 sqft_above&#94;4 4.3082 2.935 1.468 0.142 -1.447 10.063 sqft_above&#94;5 -16.0047 7.978 -2.006 0.045 -31.649 -0.361 sqft_above&#94;6 15.6074 6.787 2.300 0.022 2.299 28.916 sqft_basement&#94;2 0.1087 0.147 0.741 0.458 -0.179 0.396 sqft_basement&#94;3 0.4147 1.869 0.222 0.824 -3.250 4.080 sqft_basement&#94;4 -7.2271 9.329 -0.775 0.439 -25.521 11.067 sqft_basement&#94;5 20.6378 18.977 1.088 0.277 -16.575 57.850 sqft_basement&#94;6 -17.2149 13.324 -1.292 0.196 -43.342 8.912 lat&#94;2 -0.3905 0.053 -7.426 0.000 -0.494 -0.287 lat&#94;3 -2.8185 0.187 -15.089 0.000 -3.185 -2.452 lat&#94;4 -0.6466 0.136 -4.742 0.000 -0.914 -0.379 lat&#94;5 3.3679 0.305 11.037 0.000 2.770 3.966 lat&#94;6 2.3154 0.234 9.897 0.000 1.857 2.774 long&#94;2 -0.4234 0.055 -7.664 0.000 -0.532 -0.315 long&#94;3 0.3683 0.141 2.610 0.009 0.092 0.645 long&#94;4 2.1536 0.355 6.064 0.000 1.457 2.850 long&#94;5 -4.5241 0.925 -4.889 0.000 -6.339 -2.710 long&#94;6 2.4022 0.597 4.027 0.000 1.232 3.572 Omnibus: 1376.286 Durbin-Watson: 2.022 Prob(Omnibus): 0.000 Jarque-Bera (JB): 32029.307 Skew: 2.243 Prob(JB): 0.00 Kurtosis: 20.325 Cond. No. 9.61e+15 Warnings: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. [2] The smallest eigenvalue is 4.49e-28. This might indicate that there are strong multicollinearity problems or that the design matrix is singular. You can also create a model with interaction terms or any other higher order polynomial term of your choice. Note: Can you see how creating a function that takes in a dataframe and a degree and creates polynomial terms up until that degree can be useful? This is what we have you do in your homework! First Breakout Room: Practice with Standardization, conceptualizing overfitting and the validation set In [37]: # Don't change this cell df1 = pd . read_csv ( \"../data/bo_room2.csv\" ) #importing the custom make_plots function: from make_plots import make_plots make_plots_ = make_plots . make_plots #split the data train_ , test = train_test_split ( df1 , train_size = 0.8 , random_state = 42 ) train , val = train_test_split ( train_ , train_size = 0.8 , random_state = 42 ) #sort the data train = train . sort_values ( by = [ \"x1\" ]) val = val . sort_values ( by = [ \"x1\" ]) test = test . sort_values ( by = [ \"x1\" ]) display ( train_ . head ()); x1 x2 x3 x4 y 55 0.222222 -0.044462 -0.708154 0.314861 -0.707167 88 1.555556 0.006496 0.698881 0.045399 2.652650 26 -0.949495 0.016209 0.142260 0.023058 2.107119 42 -0.303030 -0.040598 0.190789 -0.077456 0.099612 69 0.787879 -0.043232 0.537321 -0.232292 0.136506 Here we examine a synthetic dataset to motivate both standardization and regularization. 1) First simply run the make_plots function line below and answer the questions in discussion with your classmates. 2) Create standardized training, validation and test datasets and then pass the modified dataframes to the make_plots function. 3) Comment on what you observe by answering the questions below. 1) Run the cell below and talk with your breakout room members and discuss the patterns we see in the MSE and $R&#94;2$ plots. Also comment on the size of the beta coefficients. Do they seem large or small to you? In [38]: #run make_plots on the data, which will make the plots of the various polynomial regressions and save the r2 of the best model. best_r2 = make_plots_ ( train , val , test , max_degree = 12 ) print ( \"best r2: {} \" . format ( best_r2 )) #please note that the best R&#94;2 reported is based on the moodel chosen from the validation set but # reported on the test set. #The faint red lines are the different models being fit for each degree polynomial regression. best r2: 0.7692497813625571 2) Now standardize the data on the training set and pass the standardized training, validation, and test sets to the make_plots function. In [25]: #hint use the fit and transform calls with the StandardScaler class from sklearn. scaler = #TODO #assign the standardized datasets here. train_standard = #TODO val_standard = #TODO test_standard = #TODO File \" \" , line 2 scaler = #TODO &#94; SyntaxError : invalid syntax In [ ]: # %load '../solutions/solution1.py' 3) Run the make_plots function again by running the cell below then answer the following questions: 1) What degree would you select for a polynomial model based on the $R&#94;2$ plot? 2) Did the $R&#94;2$ and MSE plots change after standardization? 3) Did the values of the beta coefficients change? In [ ]: train_standard = pd . DataFrame ( train_standard ) val_standard = pd . DataFrame ( val_standard ) test_standard = pd . DataFrame ( test_standard ) train_standard . columns = [ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"y\" ] val_standard . columns = [ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"y\" ] test_standard . columns = [ \"x1\" , \"x2\" , \"x3\" , \"x4\" , \"y\" ] best_OLS_r2 = make_plots_ ( train_standard , val_standard , test_standard , max_degree = 12 ) best_OLS_r2 Bonus question: when we split into X_train and y_train as Pavlos does in the class exercises, why is the X capitalized? Regularization What is Regularization and why should I care? When we have a lot of predictors, we need to worry about overfitting. Let's check this out: In [39]: x = [ 1 , 2 , 3 , N ] models = [ model_1 , model_2 , model_3 , model_N ] X_trains = [ X_train_df , X_train_df_2 , X_train_df_3 , X_train_df_N ] X_vals = [ X_val_df , X_val_df_2 , X_val_df_3 , X_val_df_N ] r2_train = [] r2_val = [] for i , model in enumerate ( models ): y_pred_tra = model . predict ( sm . add_constant ( X_trains [ i ])) y_pred_val = model . predict ( sm . add_constant ( X_vals [ i ])) r2_train . append ( r2_score ( y_train , y_pred_tra )) r2_val . append ( r2_score ( y_val , y_pred_val )) fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax . plot ( x , r2_train , 'o-' , label = r 'Training $R&#94;2$' ) ax . plot ( x , r2_val , 'o-' , label = r 'Validation $R&#94;2$' ) ax . set_xlabel ( 'Number of degree of polynomial' ) ax . set_ylabel ( r '$R&#94;2$ score' ) ax . set_title ( r '$R&#94;2$ score vs polynomial degree' ) ax . legend (); We notice a big difference between training and validation R&#94;2 scores: seems like we are overfitting. Introducing: regularization. What about Multicollinearity? There's seemingly a lot of multicollinearity in the data. Take a look at this warning that we got when showing our summary for our polynomial models: What is multicollinearity ? Why do we have it in our dataset? Why is this a problem? Does regularization help solve the issue of multicollinearity? What does Regularization help with? We have some pretty large and extreme coefficient values in our most recent models. These coefficient values also have very high variance. We can also clearly see some overfitting to the training set. In order to reduce the coefficients of our parameters, we can introduce a penalty term that penalizes some of these extreme coefficient values. Specifically, regularization helps us: Avoid overfitting. Reduce features that have weak predictive power. Discourage the use of a model that is too complex Big Idea: Reduce Variance by Increasing Bias Image Source: here Ridge Regression Ridge Regression is one such form of regularization. In practice, the ridge estimator reduces the complexity of the model by shrinking the coefficients, but it doesn't nullify them. We control the amount of regularization using a parameter $\\lambda$. NOTE : sklearn's ridge regression package represents this $\\lambda$ using a parameter alpha. In Ridge Regression, the penalty term is proportional to the L2-norm of the coefficients. Lasso Regression Lasso Regression is another form of regularization. Again, we control the amount of regularization using a parameter $\\lambda$. NOTE : sklearn's lasso regression package represents this $\\lambda$ using a parameter alpha. In Lasso Regression, the penalty term is proportional to the L1-norm of the coefficients. Some Differences between Ridge and Lasso Regression Since Lasso regression tend to produce zero estimates for a number of model parameters - we say that Lasso solutions are sparse - we consider to be a method for variable selection. In Ridge Regression, the penalty term is proportional to the L2-norm of the coefficients whereas in Lasso Regression, the penalty term is proportional to the L1-norm of the coefficients. Ridge Regression has a closed form solution! Lasso Regression does not. We often have to solve this iteratively. In the sklearn package for Lasso regression, there is a parameter called max_iter that determines how many iterations we perform. Why Standardizing Variables was not a waste of time Lasso regression puts constraints on the size of the coefficients associated to each variable. However, this value will depend on the magnitude of each variable. It is therefore necessary to standardize the variables. Let's use Ridge and Lasso to regularize our degree N polynomial Exercise : Play around with different values of alpha. Notice the new $R&#94;2$ value and also the range of values that the predictors take in the plot. In [40]: from sklearn.linear_model import Ridge # some values you can try out: 0.01, 0.1, 0.5, 1, 5, 10, 20, 40, 100, 200, 500, 1000, 10000 alpha = 100 ridge_model = Ridge ( alpha = alpha ) . fit ( X_train_df_N , y_train ) print ( 'R squared score for our original OLS model: {} ' . format ( r2_val [ - 1 ])) print ( 'R squared score for Ridge with alpha= {} : {} ' . format ( alpha , ridge_model . score ( X_val_df_N , y_val ))) fig , ax = plt . subplots ( figsize = ( 18 , 8 ), ncols = 2 ) ax = ax . ravel () ax [ 0 ] . hist ( model_N . params , bins = 10 , alpha = 0.5 ) ax [ 0 ] . set_title ( 'Histogram of predictor values for Original model with N: {} ' . format ( N )) ax [ 0 ] . set_xlabel ( 'Predictor values' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) ax [ 1 ] . hist ( ridge_model . coef_ . flatten (), bins = 20 , alpha = 0.5 ) ax [ 1 ] . set_title ( 'Histogram of predictor values for Ridge Model with alpha: {} ' . format ( alpha )) ax [ 1 ] . set_xlabel ( 'Predictor values' ) ax [ 1 ] . set_ylabel ( 'Frequency' ); R squared score for our original OLS model: 0.11935480685118427 R squared score for Ridge with alpha=100: 0.6535651209719516 In [41]: from sklearn.linear_model import Lasso # some values you can try out: 0.00001, 0.0001, 0.001, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10, 20 alpha = 0.01 lasso_model = Lasso ( alpha = alpha , max_iter = 1000 ) . fit ( X_train_df_N , y_train ) print ( 'R squared score for our original OLS model: {} ' . format ( r2_val [ - 1 ])) print ( 'R squared score for Lasso with alpha= {} : {} ' . format ( alpha , lasso_model . score ( X_val_df_N , y_val ))) fig , ax = plt . subplots ( figsize = ( 18 , 8 ), ncols = 2 ) ax = ax . ravel () ax [ 0 ] . hist ( model_N . params , bins = 10 , alpha = 0.5 ) ax [ 0 ] . set_title ( 'Histogram of predictor values for Original model with N: {} ' . format ( N )) ax [ 0 ] . set_xlabel ( 'Predictor values' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) ax [ 1 ] . hist ( lasso_model . coef_ . flatten (), bins = 20 , alpha = 0.5 ) ax [ 1 ] . set_title ( 'Histogram of predictor values for Lasso Model with alpha: {} ' . format ( alpha )) ax [ 1 ] . set_xlabel ( 'Predictor values' ) ax [ 1 ] . set_ylabel ( 'Frequency' ); R squared score for our original OLS model: 0.11935480685118427 R squared score for Lasso with alpha=0.01: 0.6651205006878169 Model Selection and Cross-Validation Here's our current setup so far: So we try out 10,000 different models on our validation set and pick the one that's the best? No! Since we could also be overfitting the validation set! One solution to the problems raised by using a single validation set is to evaluate each model on multiple validation sets and average the validation performance. This is the essence of cross-validation! Image source: here Let's give this a try using RidgeCV and LassoCV : In [42]: from sklearn.linear_model import RidgeCV from sklearn.linear_model import LassoCV alphas = ( 0.001 , 0.01 , 0.1 , 10 , 100 , 1000 , 10000 ) # Let us do k-fold cross validation k = 4 fitted_ridge = RidgeCV ( alphas = alphas ) . fit ( X_train_df_N , y_train ) fitted_lasso = LassoCV ( alphas = alphas ) . fit ( X_train_df_N , y_train ) print ( 'R&#94;2 score for our original OLS model: {} \\n ' . format ( r2_val [ - 1 ])) ridge_a = fitted_ridge . alpha_ print ( 'Best alpha for ridge: {} ' . format ( ridge_a )) print ( 'R&#94;2 score for Ridge with alpha= {} : {} \\n ' . format ( ridge_a , fitted_ridge . score ( X_val_df_N , y_val ))) lasso_a = fitted_lasso . alpha_ print ( 'Best alpha for lasso: {} ' . format ( lasso_a )) print ( 'R squared score for Lasso with alpha= {} : {} ' . format ( lasso_a , fitted_lasso . score ( X_val_df_N , y_val ))) R&#94;2 score for our original OLS model: 0.11935480685118427 Best alpha for ridge: 1000.0 R&#94;2 score for Ridge with alpha=1000.0: 0.6148040375619066 Best alpha for lasso: 0.01 R squared score for Lasso with alpha=0.01: 0.6651205006878169 We can also look at the coefficients of our CV models. Final Step: report the score on the test set for the model you have chosen to be the best. Sklearn's cross_validate function: You may find this useful on the homework. Say you have fit a model on the training set. You will be asked to perform cross validation and score models on the homework based on this function. Below is an example of how to do this. You will also want to choose an appropriate scoring method . Please read the documentation for this function. I've included some psuedo-code below: loss_score = cross_validate(model, X, y, cv=5, scoring = 'r2') In [47]: from sklearn.model_selection import cross_validate #scale the design matrix X_df = X_train_df_N X_df = StandardScaler () . fit_transform ( X_df ) y_train = StandardScaler () . fit_transform ( y_train ) #alpha values to optimize alphas = [ np . exp ( i ) for i in np . linspace ( - 5 , 0 , 25 )] sklearn_models = [] #fit our various models for i , alpha in enumerate ( alphas ): model = Lasso ( alpha = alpha , max_iter = 1000 ) . fit ( X_df , y_train ) sklearn_models . append ( model ) #get the cross validation scores for i , model in enumerate ( sklearn_models ): #if i == 0 initialize our lists if not i : r2_train = [] r2_val = [] y_pred_tra = model . predict ( X_df ) r2_cv_scores = cross_validate ( model , X_df , y_train , cv = 5 , scoring = 'r2' ) r2_avg_score = np . mean ( r2_cv_scores [ \"test_score\" ]) r2_train . append ( r2_avg_score ) #plot fig , ax = plt . subplots ( figsize = ( 8 , 6 )) ax . plot ( np . log10 ( alphas ), r2_train , 'o-' , label = r 'CV $R&#94;2$ score' ) ax . set_xlabel ( 'log10(alpha)' ) ax . set_ylabel ( r 'Cross Validated $R&#94;2$ score' ) ax . set_title ( r 'Ridge Regression $R&#94;2$ score vs log10(alpha)' ) ax . legend () print ( \"Best R&#94;2 Score: {} : \" . format ( np . max ( r2_train ))) Best R&#94;2 Score: 0.5694057130667505: Breakout Room 2 1) Fix the fit_ridge_and_lasso_cv function 2) Play with the arguments of the preprocess and fit_ridge_and_lasso_cv helper functions to try and improve the performance on the test set. 3) Talk with your break out room members about the questions posed In [45]: df1 = pd . read_csv ( \"../data/bo_room2.csv\" ) #Here are your helper functions. def preprocess ( df , standardize = False ): \"\"\"Splits the data into training and validation sets. arguments: df: the dataframe of training and test data you want to split. standardize: if True returns standardized data. \"\"\" #split the data train , test = train_test_split ( df , train_size = 0.8 , random_state = 42 ) #sort the data train = train . sort_values ( by = [ \"x1\" ]) test = test . sort_values ( by = [ \"x1\" ]) train . describe () X_train , y_train = train [[ \"x1\" ]], train [ \"y\" ] X_test , y_test = test [[ \"x1\" ]], test [ \"y\" ] X_train_N = add_higher_order_polynomial_terms ( X_train , N = 15 ) X_test_N = add_higher_order_polynomial_terms ( X_test , N = 15 ) if standardize : scaler = StandardScaler () . fit ( X_train_N ) X_train_N = scaler . transform ( X_train_N ) X_test_N = scaler . transform ( X_test_N ) #\"X_val\" : X_val_N, \"y_val\" : y_val, datasets = { \"X_train\" : X_train_N , \"y_train\" : y_train , \"X_test\" : X_test_N , \"y_test\" : y_test } return ( datasets ) 1) Fill out the missing lines in this helper function in order to perform cross validated ridge and lasso regression on the synthetic dataset. In [46]: def fit_ridge_and_lasso_cv ( X_train , y_train , X_test , y_test , k = None , alphas = [ 10 ** 7 ], best_OLS_r2 = best_OLS_r2 ): #X_val, y_val, \"\"\" takes in train and validation test sets and reports the best selected model using ridge and lasso regression. Arguments: X_train: the train design matrix y_train: the reponse variable for the training set X_val: the validation design matrix y_train: the reponse variable for the validation set k: the number of k-fold cross validation sections to be fed to Ridge and Lasso Regularization. \"\"\" # Let us do k-fold cross validation. make sure to provide k and alphas as arguments to both ridge and lasso. fitted_ridge = #TODO fitted_lasso = #TODO print ( 'R&#94;2 score for our original OLS model: {} \\n ' . format ( best_OLS_r2 )) ridge_a = fitted_ridge . alpha_ ridge_score = fitted_ridge . score ( X_test , y_test ) print ( 'Best alpha for ridge: {} ' . format ( ridge_a )) print ( 'R&#94;2 score for Ridge with alpha= {} : {} \\n ' . format ( ridge_a , ridge_score )) lasso_a = fitted_lasso . alpha_ lasso_score = fitted_lasso . score ( X_test , y_test ) print ( 'Best alpha for lasso: {} ' . format ( lasso_a )) print ( 'R&#94;2 score for Lasso with alpha= {} : {} ' . format ( lasso_a , lasso_score )) r2_df = pd . DataFrame ({ \"OLS\" : best_OLS_r2 , \"Lasso\" : lasso_score , \"Ridge\" : ridge_score }, index = [ 0 ]) r2_df = r2_df . melt () r2_df . columns = [ \"model\" , \"r2_Score\" ] plt . title ( \"Validation set\" ) sns . barplot ( x = \"model\" , y = \"r2_Score\" , data = r2_df ) plt . show () File \" \" , line 13 fitted_ridge = #TODO &#94; SyntaxError : invalid syntax In [ ]: # %load \"../solutions/solution2.py\" 1) Run the cell below to see the performance of the OLS, Lasso, and Ridge models on the test set In [ ]: datasets = preprocess ( df1 ) #Here we input arguments to a function from a dictionary using the ** syntax option. # X_train, y_train, X_val, y_val, fit_ridge_and_lasso_cv ( ** datasets ) 2) How can we improve our results using ridge and lasso (ie boost the $R&#94;2$ score?). Try to play with the arguments of the helper functions to increase the $R&#94;2$ scores and discuss with your group why this works. Don't split the data or try to write your own functions here. In [ ]: #TODO (hint copy and paste the previous cell) In [ ]: # %load \"../solutions/solution3.py\" 3) Would we in effect be cheating if we optimize our model over k at this stage? Why? End of Standard Section","tags":"sections","url":"sections/section4-a/"},{"title":"Advanced Section 2: Methods of regularization and their justifications","text":"Slides A-Section 1: Linear Algebra and Hypothesis Testing (slides) [PDF] A-Section 1: Linear Algebra and Hypothesis Testing (slides) [PPTX] Notes A-Section 1: Linear Algebra and Hypothesis Testing (notes) [PDF]","tags":"a-sections","url":"a-sections/a-section2/"},{"title":"Lecture 12: Estimation of the Regularization Coefficients using CV and comparison","text":"In [0]: # Import required libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from prettytable import PrettyTable from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import mean_squared_error from sklearn.linear_model import Ridge from sklearn.model_selection import cross_validate % matplotlib inline In [0]: # Initialising required parameters # The list of random states ran_state = [ 0 , 10 , 21 , 42 , 66 , 109 , 310 , 1969 ] # The list of alpha for regularization alphas = [ 1e-7 , 1e-5 , 1e-3 , 0.01 , 0.1 , 1 ] # The degree of the polynomial degree = 30 In [0]: # Read the file 'polynomial50.csv' as a dataframe df = pd . read_csv ( 'polynomial50.csv' ) # Assign the values of the 'x' column as the predictor x = df [[ 'x' ]] . values # Assign the values of the 'y' column as the response y = df [ 'y' ] . values # Also assign the true value of the function (column 'f') to the variable f f = df [ 'f' ] . values In [0]: # Use the helper code below to visualise the distribution of the x, y values & also the value of the true function f fig , ax = plt . subplots () # Plot x vs y values ax . plot ( x , y , 'o' , label = 'Observed values' , markersize = 10 , color = 'Darkblue' ) # Plot x vs true function value ax . plot ( x , f , 'k-' , label = 'True function' , linewidth = 4 , color = '#9FC131FF' ) ax . legend ( loc = 'best' ); ax . set_xlabel ( 'Predictor - $X$' , fontsize = 16 ) ax . set_ylabel ( 'Response - $Y$' , fontsize = 16 ) ax . set_title ( 'Predictor vs Response plot' , fontsize = 16 ); In [0]: # Function to perform regularization with simple validation def reg_with_validation ( rs ): # Split the data into train and validation sets with train size as 80% and random_state as x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.8 , random_state = rs ) # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features train and validation sets x_poly_train = ___ x_poly_val = ___ # Run a loop for all alpha values for alpha in alphas : # Initialise a Ridge regression model by specifying the alpha and with fit_intercept=False ridge_reg = ___ # Fit on the modified training data ___ # Predict on the training set y_train_pred = ___ # Predict on the validation set y_val_pred = ___ # Compute the training and validation mean squared errors mse_train = ___ mse_val = ___ # Append the MSEs to their respective lists training_error . append ( mse_train ) validation_error . append ( mse_val ) # Return the train and validation MSE return training_error , validation_error In [0]: ### edTest(test_validation) ### # Initialise a list to store the best alpha using simple validation for varying random states best_alpha = [] # Run a loop for different random_states for i in range ( len ( ran_state )): # Get the train and validation error by calling the function reg_with_validation training_error , validation_error = ___ # Get the best mse from the validation_error list best_mse = ___ # Get the best alpha value based on the best mse best_parameter = ___ # Append the best alpha to the list best_alpha . append ( best_parameter ) # Use the helper code given below to plot the graphs fig , ax = plt . subplots ( figsize = ( 6 , 4 )) # Plot the training errors for each alpha value ax . plot ( alphas , training_error , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( alphas , validation_error , 's-' , label = 'Validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( best_parameter , 0 , 0.75 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . set_ylim ([ 0 , 0.010 ]) ax . legend ( loc = 'best' , fontsize = 12 ) bm = round ( best_mse , 5 ) ax . set_title ( f 'Best alpha is { best_parameter } with MSE { bm } ' , fontsize = 16 ) ax . set_xscale ( 'log' ) plt . tight_layout () plt . show () In [0]: # Function to perform regularization with cross validation def reg_with_cross_validation ( rs ): # Sample your data to get different splits using the random state df_new = ___ # Assign the values of the 'x' column as the predictor from your sampled dataframe x = df_new [[ 'x' ]] . values # Assign the values of the 'y' column as the response from your sampled dataframe y = df_new [ 'y' ] . values # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features on the entire data x_poly = ___ # Run a loop for all alpha values for alpha in alphas : # Initialise a Ridge regression model by specifying the alpha value and with fit_intercept=False ridge_reg = ___ # Perform cross validation on the modified data with neg_mean_squared_error as the scoring parameter and cv=5 # Remember to get the train_score ridge_cv = ___ # Compute the training and validation errors got after cross validation mse_train = ___ mse_val = ___ # Append the MSEs to their respective lists training_error . append ( mse_train ) validation_error . append ( mse_val ) # Return the train and validation MSE return training_error , validation_error In [0]: ### edTest(test_cross_validation) ### # Initialise a list to store the best alpha using cross validation for varying random states best_cv_alpha = [] # Run a loop for different random_states for i in range ( len ( ran_state )): # Get the train and validation error by calling the function reg_with_cross_validation training_error , validation_error = ___ # Get the best mse from the validation_error list best_mse = ___ # Get the best alpha value based on the best mse best_parameter = ___ # Append the best alpha to the list best_cv_alpha . append ( best_parameter ) # Use the helper code given below to plot the graphs fig , ax = plt . subplots ( figsize = ( 6 , 4 )) # Plot the training errors for each alpha value ax . plot ( alphas , training_error , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( alphas , validation_error , 's-' , label = 'Validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( best_parameter , 0 , 0.75 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . legend ( loc = 'best' , fontsize = 12 ) bm = round ( best_mse , 5 ) ax . set_title ( f 'Best alpha is { best_parameter } with MSE { bm } ' , fontsize = 16 ) ax . set_xscale ( 'log' ) plt . tight_layout () In [0]: # Use the helper code below to print your findings pt = PrettyTable () pt . field_names = [ \"Random State\" , \"Best Alpha with Validation\" , \"Best Alpha with Cross-Validation\" ] for i in range ( 6 ): pt . add_row ([ ran_state [ i ], best_alpha [ i ], best_cv_alpha [ i ]]) print ( pt ) What can you infer about cross-validation based on the previous analysis? After marking, change the random states and alpha values. Run the code again. Comment on the results of regularization with simple validation and cross-validation.","tags":"labs","url":"labs/lecture-12/notebook/"},{"title":"Lecture 12: Estimation of the Regularization Coefficients using CV and comparison","text":"In [0]: # Import libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures In [0]: # Run this cell for more readable visuals large = 22 ; med = 16 ; small = 10 params = { 'axes.titlesize' : large , 'legend.fontsize' : med , 'figure.figsize' : ( 16 , 10 ), 'axes.labelsize' : med , 'axes.titlesize' : med , 'axes.linewidth' : 2 , 'xtick.labelsize' : med , 'ytick.labelsize' : med , 'figure.titlesize' : large } plt . style . use ( 'seaborn-white' ) plt . rcParams . update ( params ) #sns.set_style(\"white\") % matplotlib inline In [0]: # Read the file \"bacteria_train.csv\" as a dataframe # The file is the same as your homework 2 df = pd . read_csv ( \"bacteria_train.csv\" ) In [0]: # Take a quick look of your dataset df . head () In [0]: # Store the predictor ('Spreading_factor') and the response ('Perc_population') values as the variables 'x' and 'y' x , y = df [[ ___ ]], df [ ___ ] In [0]: # Select the number of polynomial features as per the maximum degree maxdeg = 4 x_poly = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) In [0]: # Select a list of alpha values ranging from 10 to 120 with 1000 points between them alpha_list = np . linspace ( ___ , ___ , ___ ) len ( alpha_list ) In [0]: ### edTest(test_ridge_fit) ### # Make an empty list called coeff_list and for each alpha value, compute the coefficients and add it to coeff_list coeff_list = [] #Now, you will implement the ridge regularisation for each alpha value, make sure you set Normalize=True for i in alpha_list : ridge_reg = Ridge ( alpha = ___ , normalize = ___ ) #Fit on the entire data because we just want to see the trend of the coefficients ridge_reg . fit ( ___ , ___ ) # Append the coeff_list with the coefficients of the model coeff_list . append ( ___ ) In [0]: # We take the transpose of the list to get the variation in the coefficient values per degree trend = np . array ( coeff_list ) . T In [0]: # Use the code below to plot the variation of the coefficients as per the alpha value # Just adding some nice colors. make sure to comment this cell out if you plan to use degree more than 7 colors = [ '#5059E8' , '#9FC131FF' , '#D91C1C' , '#9400D3' , '#FF2F92' , '#336600' , 'black' ] fig , ax = plt . subplots ( figsize = ( 10 , 6 )) for i in range ( maxdeg ): ax . plot ( alpha_list , np . abs ( trend [ i + 1 ]), color = colors [ i ], alpha = 0.9 , label = f 'Degree { i + 1 } ' , lw = 2.2 ) ax . legend ( loc = 'best' , fontsize = 10 ) ax . set_xlabel ( r '$\\alpha$ values' , fontsize = 20 ) ax . set_ylabel ( r '$\\beta$ values' , fontsize = 20 ) fig . suptitle ( r 'Ridge ($L_2$) Regression' ); Compare the results of Ridge regression with the Lasso variant In [0]: # Select a list of alpha values ranging from 1e-4 to 1e-1 with 1000 points between them alpha_list = np . linspace ( ___ , ___ , ___ ) len ( alpha_list ) In [0]: ### edTest(test_lasso_fit) ### # Make an empty list called coeff_list and for each alpha value, compute the coefficients and add it to coeff_list coeff_list = [] #Now, you will implement the ridge regularisation for each alpha value, again normalize for i in alpha_list : lasso_reg = Lasso ( alpha = i , max_iter = 250000 , normalize = ___ ) #Fit on the entire data because we just want to see the trend of the coefficients lasso_reg . fit ( ___ , ___ ) # Again append the coeff_list with the coefficients of the model coeff_list . append ( ___ ) In [0]: # We take the transpose of the list to get the variation in the coefficient values per degree trend = np . array ( coeff_list ) . T In [0]: # Use helper code below to plot the variation of the coefficients as per the alpha value colors = [ '#5059E8' , '#9FC131FF' , '#D91C1C' , '#9400D3' , '#FF2F92' , '#336600' , 'black' ] fig , ax = plt . subplots ( figsize = ( 10 , 6 )) for i in range ( maxdeg ): ax . plot ( alpha_list , np . abs ( trend [ i + 1 ]), color = colors [ i ], alpha = 0.9 , label = f 'Degree { i + 1 } ' , lw = 2 ) ax . legend ( loc = 'best' , fontsize = 10 ) ax . set_xlabel ( r '$\\alpha$ values' , fontsize = 20 ) ax . set_ylabel ( r '$\\beta$ values' , fontsize = 20 ) fig . suptitle ( r 'Lasso ($L_1$) Regression' ); Mindchow 🍲 After marking the exercise, go back and change your maximum degree, and see how your coefficients vary for higher degrees Remember to hide your colors variable to avoid index error while plotting coefficients Your answer here Now set the normalize=False. Things look different. Try to think why - Hint think of colinearity. In [0]:","tags":"labs","url":"labs/lecture-12/notebook-2/"},{"title":"Lecture 12: Estimation of the Regularization Coefficients using CV and comparison","text":"Slides Ridge & Lasso - Hyperparameters [PDF] Ridge & Lasso - Hyperparameters [PPTX] Ridge & Lasso - Comparison [PDF] Ridge & Lasso - Comparison [PPTX] Exercises Lecture 12: 1 - Regularization with Cross-validation [Notebook] Lecture 12: 2 - Variation of coefficients [Notebook]","tags":"lectures","url":"lectures/lecture12/"},{"title":"Lecture 11: Regularization","text":"In [99]: # Import libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures Reading the dataset In [100]: # Read the file \"Boston_housing.csv\" as a dataframe df = pd . read_csv ( \"Boston_housing.csv\" ) df . head () Predictors & Response variables Select the 'medv' column as response variable and the rest of the columns as predictors. As such, all the following columns are predictors: crim indus nox rm age dis rad tax ptratio black lstat In [101]: # Select a subdataframe of predictors mentioned above X = df [ ___ ] # Normalize the values of the dataframe X_norm = preprocessing . normalize ( ___ ) # Select medv as the response variable y = df [ ___ ] Split the dataset into train and validation sets Keep the test size as 30% of the dataset, and use random_state =31 In [102]: ### edTest(test_random) ### # Split the data into train and validation sets X_train , X_val , y_train , y_val = train_test_split ( ___ ) Multi-linear Regression Analysis In [103]: #Fit a linear regression model on the training data lreg = LinearRegression () lreg . fit ( ___ ) # Predict on the validation set y_val_pred = lreg . predict ( ___ ) Computing the MSE for Multi-Linear Regression In [0]: # Use the mean_squared_error function to compute the validation mse mse = mean_squared_error ( ___ , ___ ) # print the MSE value print ( \"Multi-linear regression validation MSE is\" , mse ) Obtaining the coefficients of the predictors In [105]: #make a dictionary of the coefficients along with the predictors as keys lreg_coef = dict ( zip ( X . columns , np . transpose ( lreg . coef_ ))) #Linear regression coefficient values to plot lreg_x = list ( lreg_coef . keys ()) lreg_y = list ( lreg_coef . values ()) Implementing Lasso regularization In [106]: # Now, you will implement the lasso regularisation # Use alpha = 0.001 lasso_reg = Lasso ( ___ ) #Fit on training data lasso_reg . fit ( ___ ) #Make a prediction on the validation data using the above trained model y_val_pred = lasso_reg . predict ( ___ ) Computing the MSE with Lasso regularization In [0]: # Again, calculate the validation MSE & print it mse_lasso = mean_squared_error ( ___ , ___ ) print ( \"Lasso validation MSE is\" , mse_lasso ) Obtaining the coefficients of the predictors In [108]: # Use the helper code below to make a dictionary of the predictors along with the coefficients associated with them lasso_coef = dict ( zip ( X . columns , np . transpose ( lasso_reg . coef_ ))) #Lasso regularisation coefficient values to plot lasso_x = list ( lasso_coef . keys ()) lasso_y = list ( lasso_coef . values ()) Implementing Ridge regularization In [109]: # Now, we do the same as above, but we use L2 regularisation # Again, use alpha=0.001 ridge_reg = Ridge ( ___ ) #Fit the model in the training data ridge_reg . fit ( ___ ) #Predict the model on the validation data y_val_pred = ridge_reg . predict ( ___ ) Computing the MSE with Ridge regularization In [0]: ### edTest(test_mse) ### # Calculate the validation MSE & print it mse_ridge = mean_squared_error ( ___ , ___ ) print ( \"Ridge validation MSE is\" , mse_ridge ) Obtaining the coefficients of the predictors In [111]: # Use the helper code below to make a dictionary of the predictors along with the coefficients associated with them ridge_coef = dict ( zip ( X . columns , np . transpose ( ridge_reg . coef_ ))) #Ridge regularisation coefficient values to plot ridge_x = list ( ridge_coef . keys ()) ridge_y = list ( ridge_coef . values ()) Plotting the graph In [0]: # Use the helper code below to visualise your results plt . rcdefaults () plt . barh ( lreg_x , lreg_y , 1.0 , align = 'edge' , color = \"#D3B4B4\" , label = \"Linear Regression\" ) plt . barh ( lasso_x , lasso_y , 0.75 , align = 'edge' , color = \"#81BDB2\" , label = \"Lasso regularisation\" ) plt . barh ( ridge_x , ridge_y , 0.25 , align = 'edge' , color = \"#7E7EC0\" , label = \"Ridge regularisation\" ) plt . grid ( linewidth = 0.2 ) plt . xlabel ( \"Coefficient\" ) plt . ylabel ( \"Predictors\" ) plt . legend ( loc = 'best' ) plt . show () Compare the results of linear regression with that of lasso and ridge regularization. Your answer here After marking, change the alpha values to 1, 10 and 1000. What happens to the coefficients when alpha increases? Your answer here","tags":"labs","url":"labs/lecture-11/notebook/"},{"title":"Lecture 11: Regularization","text":"In [9]: # Import required libraries % matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.linear_model import Ridge from sklearn.linear_model import Lasso plt . style . use ( 'seaborn-white' ) # These are custom functions made to help you visualise your results from helper import plot_functions from helper import plot_coefficients In [52]: # Open the file 'polynomial50.csv' as a dataframe df = pd . read_csv ( 'polynomial50.csv' ) In [53]: # Take a quick look at the data df . head () Out[53]: x f y 0 0.000000 1.000000 0.923951 1 0.020408 1.039176 1.028283 2 0.040816 1.075173 1.069739 3 0.061224 1.108144 1.077327 4 0.081633 1.138242 1.105688 In [54]: # Assign the values of the 'x' column as the predictor x = df [[ 'x' ]] . values # Assign the values of the 'y' column as the response y = df [ 'y' ] . values # Also assign the true value of the function (column 'f') to the variable f f = df [ 'f' ] . values In [0]: # Visualise the distribution of the x, y values & also the value of the true function f fig , ax = plt . subplots () # Plot x vs y values ax . plot ( ___ , ___ , '.' , label = 'Observed values' , markersize = 10 ) # Plot x vs true function value ax . plot ( ___ , ___ , 'k-' , label = 'Function description' ) # The code below is to annotate your plot ax . legend ( loc = 'best' ); ax . set_xlabel ( 'Predictor - $X$' , fontsize = 16 ) ax . set_ylabel ( 'Response - $Y$' , fontsize = 16 ) ax . set_title ( 'Predictor vs Response plot' , fontsize = 16 ); In [1]: # Split the data into train and validation sets with training size 80% and random_state = 109 x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.8 , random_state = 109 ) --------------------------------------------------------------------------- NameError Traceback (most recent call last) in 1 # Split the data into train and validation sets with training size 80% and random_state = 109 2 ----> 3 x_train , x_val , y_train , y_val = train_test_split ( x , y , train_size = 0.8 , random_state = 109 ) NameError : name 'train_test_split' is not defined In [2]: ### edTest(test_mse) ### fig , rows = plt . subplots ( 6 , 2 , figsize = ( 16 , 24 )) # Select the degree for polynomial features degree = __ # List of hyper-parameter values alphas = [ 0.0 , 1e-7 , 1e-5 , 1e-3 , 0.1 , 1 ] # Create two lists for training and validation error training_error , validation_error = [],[] # Compute the polynomial features train and validation sets x_poly_train = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) x_poly_val = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) for i , alpha in enumerate ( alphas ): l , r = rows [ i ] # For each i, fit a ridge regression on training set ridge = Ridge ( fit_intercept = False , alpha = ___ ) ridge . fit ( ___ , ___ ) # Predict on the validation set y_train_pred = ridge . predict ( ___ ) y_val_pred = ridge . predict ( ___ ) # Compute the training and validation errors mse_train = mean_squared_error ( ___ , ___ ) mse_val = mean_squared_error ( ___ , ___ ) # Add that value to the list training_error . append ( ___ ) validation_error . append ( ___ ) # Use helper functions plot_functions & plot_coefficients to visualise the plots plot_functions ( degree , ridge , l , df , alpha , x_val , y_val , x_train , y_train ) plot_coefficients ( ridge , r , alpha ) sns . despine () --------------------------------------------------------------------------- NameError Traceback (most recent call last) in 1 ### edTest(test_mse) ### 2 ----> 3 fig , rows = plt . subplots ( 6 , 2 , figsize = ( 16 , 24 ) ) 4 5 # Select the degree for polynomial features NameError : name 'plt' is not defined In [3]: ### edTest(test_hyper) ### # Find the best value of hyper parameter, which gives the least error on the validdata best_parameter = ___ print ( f 'The best hyper parameter value, alpha = { best_parameter } ' ) The best hyper parameter value, alpha = In [0]: # Now make the MSE polots # Plot the errors as a function of increasing d value to visualise the training and validation errors fig , ax = plt . subplots ( figsize = ( 12 , 8 )) # Plot the training errors for each alpha value ax . plot ( ___ , ___ , 's--' , label = 'Training error' , color = 'Darkblue' , linewidth = 2 ) # Plot the validation errors for each alpha value ax . plot ( ___ , ___ , 's-' , label = 'validation error' , color = '#9FC131FF' , linewidth = 2 ) # Draw a vertical line at the best parameter ax . axvline ( ___ , 0 , 0.5 , color = 'r' , label = f 'Min validation error at alpha = { best_parameter } ' ) ax . set_xlabel ( 'Value of Alpha' , fontsize = 15 ) ax . set_ylabel ( 'Mean Squared Error' , fontsize = 15 ) ax . set_ylim ([ 0 , 0.010 ]) ax . legend ( loc = 'upper left' , fontsize = 16 ) ax . set_title ( 'Mean Squared Error' , fontsize = 20 ) ax . set_xscale ( 'log' ) plt . tight_layout () Fine-tune $\\alpha$ value As you see above, the the best parameter value is at $\\alpha = 0.001$. However, you can further find a better solution by experimenting with the hyper-parameter values by narrowing the selection of alphas So after marking the exercise , Go back and remove 0, and 0.1 from the alphas list and replace with values close to 0.001 In [0]:","tags":"labs","url":"labs/lecture-11/notebook-2/"},{"title":"Lecture 11: Regularization","text":"In [1]: #Importing libraries % matplotlib inline import numpy as np import scipy as sp import matplotlib as mpl import matplotlib.cm as cm import matplotlib.pyplot as plt import pandas as pd from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures In [2]: #Helper function to set plot characteristics def make_plot (): fig , axes = plt . subplots ( figsize = ( 20 , 8 ), nrows = 1 , ncols = 2 ); axes [ 0 ] . set_ylabel ( \"$p_R$\" , fontsize = 18 ) axes [ 0 ] . set_xlabel ( \"$x$\" , fontsize = 18 ) axes [ 1 ] . set_xlabel ( \"$x$\" , fontsize = 18 ) axes [ 1 ] . set_yticklabels ([]) axes [ 0 ] . set_ylim ([ 0 , 1 ]) axes [ 1 ] . set_ylim ([ 0 , 1 ]) axes [ 0 ] . set_xlim ([ 0 , 1 ]) axes [ 1 ] . set_xlim ([ 0 , 1 ]) plt . tight_layout (); return axes In [3]: # Reading the file into a dataframe df = pd . read_csv ( \"noisypopulation.csv\" ) df . head () Out[3]: f x y 0 0.047790 0.00 0.011307 1 0.051199 0.01 0.010000 2 0.054799 0.02 0.007237 3 0.058596 0.03 0.000056 4 0.062597 0.04 0.010000 In [34]: ### edTest(test_data) ### # Set the predictor and response variable # Column x is the predictor and column y is the response variable. # Column f is the true function of the given data # Select the values of the columns x = df . ___ y = df . ___ f = df . ___ In [36]: ### edTest(test_poly) ### # Function to compute the Polynomial Features for the data x for the given degree d def polyshape ( d , x ): return PolynomialFeatures ( ___ ) . fit_transform ( ___ . reshape ( - 1 , 1 )) In [37]: ### edTest(test_linear) ### #Function to fit a Linear Regression model def make_predict_with_model ( x , y , x_pred ): #Create a Linear Regression model with fit_intercept as False lreg = ___ #Fit the model to the data x and y lreg . fit ( ___ , ___ ) #Predict on the x_pred data y_pred = lreg . predict ( ___ ) return lreg , y_pred In [38]: #Function to perform bootstrap and fit the data # degree is the maximum degree of the model # num_boot is the number of bootstraps # size is the number of random points selected from the data for each bootstrap # x is the predictor variable # y is the response variable def gen ( degree , num_boot , size , x , y ): #Create 2 lists to store the prediction and model predicted_values , linear_models = [], [] #Run the loop for the number of bootstrap value for i in range ( num_boot ): #Helper code to call the make_predict_with_model function to fit the data indexes = np . sort ( np . random . choice ( x . shape [ 0 ], size = size , replace = False )) #lreg and y_pred hold the model and predicted values of the current bootstrap lreg , y_pred = make_predict_with_model ( polyshape ( degree , x [ indexes ]), y [ indexes ], polyshape ( degree , x )) #Append the model and predicted values into the appropriate lists predicted_values . append ( ___ ) linear_models . append ( ___ ) #Return the 2 lists return predicted_values , linear_models In [39]: ### edTest(test_gen) ### # Call the function gen twice with x and y as the predictor and response variable respectively # The number of bootstraps should be 200 and the number of samples from the dataset should be 30 # Store the return values in appropriate variables # Get results for degree 1 predicted_1 , model_1 = gen ( ___ ); # Get results for degree 100 predicted_100 , model_100 = gen ( ___ ); In [0]: #Helper code to plot the data indexes = np . sort ( np . random . choice ( x . shape [ 0 ], size = 30 , replace = False )) plt . figure ( figsize = ( 12 , 8 )) axes = make_plot () #Plot for Degree 1 axes [ 0 ] . plot ( x , f , label = \"f\" , color = 'darkblue' , linewidth = 4 ) axes [ 0 ] . plot ( x , y , '.' , label = \"Population y\" , color = '#009193' , markersize = 8 ) axes [ 0 ] . plot ( x [ indexes ], y [ indexes ], 's' , color = 'black' , label = \"Data y\" ) for i , p in enumerate ( predicted_1 [: - 1 ]): axes [ 0 ] . plot ( x , p , alpha = 0.03 , color = '#FF9300' ) axes [ 0 ] . plot ( x , predicted_1 [ - 1 ], alpha = 0.3 , color = '#FF9300' , label = \"Degree 1 from different samples\" ) #Plot for Degree 100 axes [ 1 ] . plot ( x , f , label = \"f\" , color = 'darkblue' , linewidth = 4 ) axes [ 1 ] . plot ( x , y , '.' , label = \"Population y\" , color = '#009193' , markersize = 8 ) axes [ 1 ] . plot ( x [ indexes ], y [ indexes ], 's' , color = 'black' , label = \"Data y\" ) for i , p in enumerate ( predicted_100 [: - 1 ]): axes [ 1 ] . plot ( x , p , alpha = 0.03 , color = '#FF9300' ) axes [ 1 ] . plot ( x , predicted_100 [ - 1 ], alpha = 0.2 , color = '#FF9300' , label = \"Degree 100 from different samples\" ) axes [ 0 ] . legend ( loc = 'best' ) axes [ 1 ] . legend ( loc = 'best' ) #edgecolor='black', linewidth=3, facecolor='green', plt . show () After you mark the exercise, run the code again, but this time with degree 10 instead of 100. Do you see a decrease in variance? Why are the edges still so erractic? Your answer here Also check the values of the coefficients for each of your runs. Do you see a pattern? Your answer here","tags":"labs","url":"labs/lecture-11/notebook-3/"},{"title":"Lecture 11: Regularization","text":"Slides Bias/Variance Trade-off [PDF] Lasso & Ridge [PDF] Exercises Lecture 11: 1 - Bias Variance Tradeoff [Notebook] Lecture 11: 2 - Simple Lasso and Ridge Regularization [Notebook] Lecture 11: 3 - Hyper-parameter Tuning for Ridge Regression [Notebook]","tags":"lectures","url":"lectures/lecture11/"},{"title":"Lecture 10: Hypothesis Testing and Predictive CI","text":"In [27]: # Import libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.linear_model import Lasso from sklearn.linear_model import Ridge from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures from scipy import stats In [20]: # Read the file \"Advertising.csv\" as a dataframe df = pd . read_csv ( \"Advertising.csv\" ) df . head () In [21]: # Select a subdataframe of predictors X = df . drop ([ 'Sales' ], axis = 1 ) # Select the response variable y = df [ 'Sales' ] In [22]: #Fit a linear regression model, make sure to set normalize=True lreg = LinearRegression ( normalize = True ) lreg . fit ( X , y ) In [23]: coef_dict = dict ( zip ( df . columns [: - 1 ], np . transpose ( lreg . coef_ ))) predictors , coefficients = list ( zip ( * sorted ( coef_dict . items (), key = lambda x : x [ 1 ]))) In [24]: # Use the helper code below to visualise your coefficients fig , ax = plt . subplots () ax . barh ( predictors , coefficients , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . set_xlabel ( \"Coefficient\" ) ax . set_ylabel ( \"Predictors\" ) plt . show () In [25]: # Helper function # t statistic calculator def get_t ( arr ): means = np . abs ( arr . mean ( axis = 0 )) stds = arr . std ( axis = 0 ) return np . divide ( means , stds ) #,where=stds!=0) In [26]: # We now bootstrap for numboot times to find the distribution for the coefficients coef_dist = [] numboot = 1000 for i in range ( ___ ): # This is another way of making a bootstrap using df.sample method. # Take frac=1 and replace=True to get a bootstrap. df_new = df . sample ( frac = 1 , replace = True ) X = df_new . drop ( ___ , axis = 1 ) y = df_new [ ___ ] # Don't forget to normalize lreg = LinearRegression ( normalize = ___ ) lreg . fit ( ___ , ___ ) coef_dist . append ( lreg . coef_ ) coef_dist = np . array ( coef_dist ) In [339]: # We use the helper function from above to find the T-test values tt = get_t ( ___ ) n = df . shape [ 0 ] In [340]: tt_dict = dict ( zip ( df . columns [: - 1 ], tt )) predictors , tvalues = list ( zip ( * sorted ( tt_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Use the helper code below to visualise your coefficients fig , ax = plt . subplots () ax . barh ( predictors , tvalues , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . set_xlabel ( \"T-test values\" ) ax . set_ylabel ( \"Predictors\" ) plt . show () In [342]: ### edTest(test_conf) ### # We now go from t-test values to p values using scipy.stats T-distribution function pval = stats . t . sf ( tt , n - 1 ) * 2 # here we use sf i.e 'Survival function' which is 1 - CDF of the t distribution. # We also multiply by two because its a two tailed test. # Please refer to lecture notes for more information # Since p values are in reversed order, we find the 'confidence' which is 1-p conf = ___ In [343]: conf_dict = dict ( zip ( df . columns [: - 1 ], conf )) predictors , confs = list ( zip ( * sorted ( conf_dict . items (), key = lambda x : x [ 1 ]))) In [0]: # Use the helper code below to visualise your coefficients fig , ax = plt . subplots () ax . barh ( predictors , confs , align = 'center' , color = \"#336600\" , alpha = 0.7 ) ax . grid ( linewidth = 0.2 ) ax . axvline ( x = 0.95 , linewidth = 3 , linestyle = '--' , color = 'black' , alpha = 0.8 , label = '0.95' ) ax . set_xlabel ( \"$1-p$ value\" ) ax . set_ylabel ( \"Predictors\" ) ax . legend () plt . show () Relevance of predictors You may have observed above that TV and radio are significant but newspaper advertising is not. Re-run the entire exercise but drop radio from the analysis by using the following snippet. df = df.drop(['radio'],axis=1) Is newspaper still irrelevant by your analysis? Why? or why not? In [0]:","tags":"labs","url":"labs/lecture-10/notebook/"},{"title":"Lecture 10: Hypothesis Testing and Predictive CI","text":"In [1]: # Import libraries % matplotlib inline import pandas as pd import numpy as np import matplotlib.pyplot as plt from sklearn import preprocessing from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures from scipy import stats In [2]: # Read the `Advertising.csv` dataframe df = pd . read_csv ( 'Advertising.csv' ) In [3]: # Take a quick look at the data df . head () Out[3]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 In [134]: # This helper function computes the variance of the error term def error_func ( y , y_p ): n = len ( y ) return np . sqrt ( np . sum (( y - y_p ) ** 2 / ( n - 2 ))) In [147]: # select the number of bootstraps numboot = 1000 # Select the budget. We have used a 2d list to facilitate model prediction (sklearn.LinearRegression requires input as a 2d array) budget = [[ ___ ]] # Define an empty list that will store sales predictions for each bootstrap sales_dist = [] In [148]: # Running through each bootstrap, we fit a model, make predictions and compute sales which is appended to the list defined above for i in range ( ___ ): # Bootstrap using df.sample method. df_new = df . sample ( frac = ___ , replace = ___ ) x = df_new [[ ___ ]] y = df_new [ ___ ] linreg = LinearRegression () linreg . fit ( _ , _ ) prediction = linreg . predict ( budget ) y_pred = linreg . predict ( x ) error = np . random . normal ( 0 , error_func ( y , y_pred )) # The final sales prediction is the sum of the model prediction and the error term sales = ___ sales_dist . append ( np . float ( ___ )) In [137]: ### edTest(test_sales) ### # We sort the list containing sales predictions in ascending values sales_dist . sort () # find the 95% confidence interval using np.percentile function at 2.5% and 97.5% sales_CI = ( np . percentile ( ___ , ___ ), np . percentile ( ___ , ___ )) In [138]: # Use this helper function to plot the histogram of beta values along with the 95% confidence interval def plot_simulation ( simulation , confidence ): plt . hist ( simulation , bins = 30 , label = 'beta distribution' , align = 'left' , density = True , edgecolor = 'k' ) plt . axvline ( confidence [ 1 ], 0 , 1 , color = 'r' , label = 'Right Interval' ) plt . axvline ( confidence [ 0 ], 0 , 1 , color = 'red' , label = 'Left Interval' ) plt . xlabel ( 'Beta value' ) plt . ylabel ( 'Frequency' ) plt . legend ( frameon = False , loc = 'upper right' ) In [2]: # call the function above with the computed sales distribution and the confidence intervals from earlier plot_simulation ( sales_dist , sales_CI ) In [3]: # Print the computed values print ( f \"With a TV advertising budget of $ { budget [ 0 ][ 0 ] } ,\" ) print ( f \"we can expect an increase of sales anywhere between { sales_CI [ 0 ] : 0.2f } to { sales_CI [ 1 ] : .2f } \\ with a 95% confidence interval\" ) Post-exercise question Your sales prediction is based on the Simple-Linear regression model between TV and Sales . Now, re-run the above exercise but this time fit the model considering all variables in Advertising.csv . Keep the budget the same, i.e $1000 for 'TV' advertising. You may have to change the budget variable to something like [[1000,0,0]] for proper computation. Does your predicted sales interval change? Why, or why not? In [149]: # Your answer here","tags":"labs","url":"labs/lecture-10/notebook-2/"},{"title":"Lecture 10: Hypothesis Testing and Predictive CI","text":"CS-109A Introduction to Data Science Lecture 10 Additional Code: Multiple and Polynomial Regression (September 26, 2019 version) Harvard University Fall 2020 Instructors: Pavlos Protopapas, Kevin Rader, and Chris Tanner Lab Instructor: Chris Tanner and Eleni Kaxiras Authors: Rahul Dave, David Sondak, Will Claybaugh, Pavlos Protopapas, Chris Tanner In [7]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[7]: Table of Contents Learning Goals / Tip of the Week / Terminology Training/Validation/Testing Splits (slides + interactive warm-up) Polynomial Regression, and Revisiting the Cab Data Multiple regression and exploring the Football data A nice trick for forward-backwards Learning Goals After this lab, you should be able to Explain the difference between train/validation/test data and WHY we have each. Implement cross-validation on a dataset Implement arbitrary multiple regression models in both SK-learn and Statsmodels. Interpret the coefficent estimates produced by each model, including transformed and dummy variables In [8]: import numpy as np import pandas as pd import matplotlib.pyplot as plt import statsmodels.api as sm from statsmodels.api import OLS from sklearn import preprocessing from sklearn.preprocessing import PolynomialFeatures from sklearn.metrics import r2_score from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from pandas.plotting import scatter_matrix import seaborn as sns % matplotlib inline Extra Tip of the Week Within your terminal (aka console aka command prompt), most shell environments support useful shortcuts: press the [up arrow] to navigate through your most recent commands press [CTRL + A] to go to the beginning of the line press [CTRL + E] to go to the end of the line press [CTRL + K] to clear the line type `history` to see the last commands you've run Terminology Say we have input features $X$, which via some function $f()$, approximates outputs $Y$. That is, $Y = f(X) + \\epsilon$ (where $\\epsilon$ represents our unmeasurable variation (i.e., irreducible error). Inference : estimates the function $f$, but the goal isn't to make predictions for $Y$; rather, it is more concerned with understanding the relationship between $X$ and $Y$. Prediction : estimates the function $f$ with the goal of making accurate $Y$ predictions for some unseen $X$. We have recently used two highly popular, useful libraries, statsmodels and sklearn . statsmodels is mostly focused on the inference task. It aims to make good estimates for $f()$ (via solving for our $\\beta$'s), and it provides expansive details about its certainty. It provides lots of tools to discuss confidence, but isn't great at dealing with test sets. sklearn is mostly focused on the prediction task. It aims to make a well-fit line to our input data $X$, so as to make good $Y$ predictions for some unseen inputs $X$. It provides a shallower analysis of our variables. In other words, sklearn is great at test sets and validations, but it can't really discuss uncertainty in the parameters or predictions. R-squared : An interpretable summary of how well the model did. 1 is perfect, 0 is a trivial baseline model based on the mean $y$ value, and negative is worse than the trivial model. F-statistic : A value testing whether we're likely to see these results (or even stronger ones) if none of the predictors actually mattered. Prob (F-statistic) : The probability that we'd see these results (or even stronger ones) if none of the predictors actually mattered. If this probability is small then either A) some combination of predictors actually matters or B) something rather unlikely has happened coef : The estimate of each beta. This has several sub-components: std err : The amount we'd expect this value to wiggle if we re-did the data collection and re-ran our model. More data tends to make this wiggle smaller, but sometimes the collected data just isn't enough to pin down a particular value. t and P>|t| : similar to the F-statistic, these measure the probability of seeing coefficients this big (or even bigger) if the given variable didn't actually matter. Small probability doesn't necessarily mean the value matters [0.025 0.975] : Endpoints of the 95% confidence interval. This is a interval drawn in a clever way and which gives an idea of where the true beta value might plausibly live. (If you want to understand why \"there's a 95% chance the true beta is in the interval\" is wrong , start a chat with Will : ) Part 2: Polynomial Regression, and Revisiting the Cab Data Polynomial regression uses a linear model to estimate a non-linear function (i.e., a function with polynomial terms). For example: $y = \\beta_0 + \\beta_1x_i + \\beta_1x_i&#94;{2}$ It is a linear model because we are still solving a linear equation (the linear aspect refers to the beta coefficients). In [11]: # read in the data, break into train and test cab_df = pd . read_csv ( \"dataset_1.txt\" ) train_data , test_data = train_test_split ( cab_df , test_size =. 2 , random_state = 42 ) cab_df . head () Out[11]: TimeMin PickupCount 0 860.0 33.0 1 17.0 75.0 2 486.0 13.0 3 300.0 5.0 4 385.0 10.0 In [12]: cab_df . shape Out[12]: (1250, 2) In [13]: # do some data cleaning X_train = train_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 # transforms it to being hour-based y_train = train_data [ 'PickupCount' ] . values X_test = test_data [ 'TimeMin' ] . values . reshape ( - 1 , 1 ) / 60 # hour-based y_test = test_data [ 'PickupCount' ] . values def plot_cabs ( cur_model , poly_transformer = None ): # build the x values for the prediction line x_vals = np . arange ( 0 , 24 , . 1 ) . reshape ( - 1 , 1 ) # optionally use the passed-in transformer if poly_transformer != None : dm = poly_transformer . fit_transform ( x_vals ) else : dm = x_vals # make the prediction at each x value prediction = cur_model . predict ( dm ) # plot the prediction line, and the test data plt . plot ( x_vals , prediction , color = 'k' , label = \"Prediction\" ) plt . scatter ( X_test , y_test , label = \"Test Data\" ) # label your plots plt . ylabel ( \"Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () plt . show () In [14]: from sklearn.linear_model import LinearRegression fitted_cab_model0 = LinearRegression () . fit ( X_train , y_train ) plot_cabs ( fitted_cab_model0 ) In [15]: fitted_cab_model0 . score ( X_test , y_test ) Out[15]: 0.240661535615741 Exercise Questions : The above code uses sklearn . As more practice, and to help you stay versed in both libraries, perform the same task (fit a linear regression line) using statsmodels and report the $r&#94;2$ score. Is it the same value as what sklearn reports, and is this the expected behavior? In [16]: ### SOLUTION: # augment the data with a column vector of 1's train_data_augmented = sm . add_constant ( X_train ) test_data_augmented = sm . add_constant ( X_test ) # fit the model on the training data OLSModel = OLS ( train_data [ 'PickupCount' ] . values , train_data_augmented ) . fit () # get the prediction results ols_predicted_pickups_test = OLSModel . predict ( test_data_augmented ) r2_score_test = r2_score ( test_data [[ 'PickupCount' ]] . values , ols_predicted_pickups_test ) print ( r2_score_test ) 0.240661535615741 We can see that there's still a lot of variation in cab pickups that's not being captured by a linear fit. Further, the linear fit is predicting massively more pickups at 11:59pm than at 12:00am. This is a bad property, and it's the conseqeuence of having a straight line with a non-zero slope. However, we can add columns to our data for $TimeMin&#94;2$ and $TimeMin&#94;3$ and so on, allowing a curvy polynomial line to hopefully fit the data better. We'll be using sklearn 's PolynomialFeatures() function to take some of the tedium out of building the expanded input data. In fact, if all we want is a formula like $y \\approx \\beta_0 + \\beta_1 x + \\beta_2 x&#94;2 + ...$, it will directly return a new copy of the data in this format! In [17]: transformer_3 = PolynomialFeatures ( 3 , include_bias = False ) expanded_train = transformer_3 . fit_transform ( X_train ) # TRANSFORMS it to polynomial features pd . DataFrame ( expanded_train ) . describe () # notice that the columns now contain x, x&#94;2, x&#94;3 values Out[17]: 0 1 2 count 1000.000000 1000.000000 1000.000000 mean 11.717217 182.833724 3234.000239 std 6.751751 167.225711 3801.801966 min 0.066667 0.004444 0.000296 25% 6.100000 37.210833 226.996222 50% 11.375000 129.390694 1471.820729 75% 17.437500 304.066458 5302.160684 max 23.966667 574.401111 13766.479963 A few notes on PolynomialFeatures : The interface is a bit strange. PolynomialFeatures is a 'transformer' in sklearn. We'll be using several transformers that learn a transformation on the training data, and then we will apply those transformations on future data. With PolynomialFeatures, the .fit() is pretty trivial, and we often fit and transform in one command, as seen above with `.fit_transform() . You rarely want to include_bias (a column of all 1's), since sklearn will add it automatically. Remember, when using statsmodels, you can just .add_constant() right before you fit the data. If you want polynomial features for a several different variables (i.e., multinomial regression), you should call .fit_transform() separately on each column and append all the results to a copy of the data (unless you also want interaction terms between the newly-created features). See np.concatenate() for joining arrays. In [18]: fitted_cab_model3 = LinearRegression () . fit ( expanded_train , y_train ) print ( \"fitting expanded_train:\" , expanded_train ) plot_cabs ( fitted_cab_model3 , transformer_3 ) fitting expanded_train: [[6.73333333e+00 4.53377778e+01 3.05274370e+02] [2.18333333e+00 4.76694444e+00 1.04078287e+01] [1.41666667e+00 2.00694444e+00 2.84317130e+00] ... [1.96666667e+01 3.86777778e+02 7.60662963e+03] [1.17333333e+01 1.37671111e+02 1.61534104e+03] [1.42000000e+01 2.01640000e+02 2.86328800e+03]] Exercise Questions : Calculate the polynomial model's $R&#94;2$ performance on the test set. Does the polynomial model improve on the purely linear model? Make a residual plot for the polynomial model. What does this plot tell us about the model? In [19]: # ANSWER 1 expanded_test = transformer_3 . fit_transform ( X_test ) print ( \"Test R-squared:\" , fitted_cab_model3 . score ( expanded_test , y_test )) # NOTE 1: unlike statsmodels' r2_score() function, sklearn has a .score() function # NOTE 2: fit_transform() is a nifty function that transforms the data, then fits it Test R-squared: 0.3341251257077902 In [20]: # ANSWER 2: yes it does. In [21]: # ANSWER 3 (class discussion about the residuals) x_matrix = transformer_3 . fit_transform ( X_train ) prediction = fitted_cab_model3 . predict ( x_matrix ) residual = y_train - prediction plt . scatter ( X_train , residual , label = \"Residual\" ) plt . axhline ( 0 , color = 'k' ) plt . title ( \"Residuals for the Cubic Model\" ) plt . ylabel ( \"Residual Number of Taxi Pickups\" ) plt . xlabel ( \"Time of Day (Hours Past Midnight)\" ) plt . legend () Out[21]: Other features Polynomial features are not the only constucted features that help fit the data. Because these data have a 24 hour cycle, we may want to build features that follow such a cycle. For example, $sin(24\\frac{x}{2\\pi})$, $sin(12\\frac{x}{2\\pi})$, $sin(8\\frac{x}{2\\pi})$. Other feature transformations are appropriate to other types of data. For instance certain feature transformations have been developed for geographical data. Scaling Features When using polynomials, we are explicitly trying to use the higher-order values for a given feature. However, sometimes these polynomial features can take on values that are drastically large, making it difficult for the system to learn an appropriate bias weight due to its large values and potentially large variance. To counter this, sometimes one may be interested in scaling the values for a given feature. For our ongoing taxi-pickup example, using polynomial features improved our model. If we wished to scale the features, we could use sklearn 's StandardScaler() function: In [22]: # SCALES THE EXPANDED/POLY TRANSFORMED DATA # we don't need to convert to a pandas dataframe, but it can be useful for scaling select columns train_copy = pd . DataFrame ( expanded_train . copy ()) test_copy = pd . DataFrame ( expanded_test . copy ()) # Fit the scaler on the training data scaler = StandardScaler () . fit ( train_copy ) # Scale both the test and training data. train_scaled = scaler . transform ( expanded_train ) test_scaled = scaler . transform ( expanded_test ) # we could optionally run a new regression model on this scaled data fitted_scaled_cab = LinearRegression () . fit ( train_scaled , y_train ) fitted_scaled_cab . score ( test_scaled , y_test ) Out[22]: 0.33412512570778274 Part 3: Multiple regression and exploring the Football (aka soccer) data Let's move on to a different dataset! The data imported below were scraped by Shubham Maurya and record various facts about players in the English Premier League. Our goal will be to fit models that predict the players' market value (what the player could earn when hired by a new team), as estimated by https://www.transfermarkt.us . name : Name of the player club : Club of the player age : Age of the player position : The usual position on the pitch position_cat : 1 for attackers, 2 for midfielders, 3 for defenders, 4 for goalkeepers market_value : As on www.transfermarkt.us.on July 20th, 2017 page_views : Average daily Wikipedia page views from September 1, 2016 to May 1, 2017 fpl_value : Value in Fantasy Premier League as on July 20th, 2017 fpl_sel : % of FPL players who have selected that player in their team fpl_points : FPL points accumulated over the previous season region : 1 for England, 2 for EU, 3 for Americas, 4 for Rest of World nationality : Player's nationality new_foreign : Whether a new signing from a different league, for 2017/18 (till 20th July) age_cat : a categorical version of the Age feature club_id : a numerical version of the Club feature big_club : Whether one of the Top 6 clubs new_signing : Whether a new signing for 2017/18 (till 20th July) As always, we first import, verify, split, and explore the data. Part 3.1: Import and verification and grouping In [29]: league_df = pd . read_csv ( \"league_data.txt\" ) print ( league_df . dtypes ) # QUESTION: what would you guess is the mean age? mean salary? league_df . head () # turns out, it's a lot name object club object age int64 position object position_cat int64 market_value float64 page_views int64 fpl_value float64 fpl_sel object fpl_points int64 region float64 nationality object new_foreign int64 age_cat int64 club_id int64 big_club int64 new_signing int64 dtype: object Out[29]: name club age position position_cat market_value page_views fpl_value fpl_sel fpl_points region nationality new_foreign age_cat club_id big_club new_signing 0 Alexis Sanchez Arsenal 28 LW 1 65.0 4329 12.0 17.10% 264 3.0 Chile 0 4 1 1 0 1 Mesut Ozil Arsenal 28 AM 1 50.0 4395 9.5 5.60% 167 2.0 Germany 0 4 1 1 0 2 Petr Cech Arsenal 35 GK 4 7.0 1529 5.5 5.90% 134 2.0 Czech Republic 0 6 1 1 0 3 Theo Walcott Arsenal 28 RW 1 20.0 2393 7.5 1.50% 122 1.0 England 0 4 1 1 0 4 Laurent Koscielny Arsenal 31 CB 3 22.0 912 6.0 0.70% 121 2.0 France 0 4 1 1 0 In [30]: league_df . shape Out[30]: (461, 17) In [31]: league_df . describe () Out[31]: age position_cat market_value page_views fpl_value fpl_points region new_foreign age_cat club_id big_club new_signing count 461.000000 461.000000 461.000000 461.000000 461.000000 461.000000 460.000000 461.000000 461.000000 461.000000 461.000000 461.000000 mean 26.804772 2.180043 11.012039 763.776573 5.447939 57.314534 1.993478 0.034707 3.206074 10.334056 0.303688 0.145336 std 3.961892 1.000061 12.257403 931.805757 1.346695 53.113811 0.957689 0.183236 1.279795 5.726475 0.460349 0.352822 min 17.000000 1.000000 0.050000 3.000000 4.000000 0.000000 1.000000 0.000000 1.000000 1.000000 0.000000 0.000000 25% 24.000000 1.000000 3.000000 220.000000 4.500000 5.000000 1.000000 0.000000 2.000000 6.000000 0.000000 0.000000 50% 27.000000 2.000000 7.000000 460.000000 5.000000 51.000000 2.000000 0.000000 3.000000 10.000000 0.000000 0.000000 75% 30.000000 3.000000 15.000000 896.000000 5.500000 94.000000 2.000000 0.000000 4.000000 15.000000 1.000000 0.000000 max 38.000000 4.000000 75.000000 7664.000000 12.500000 264.000000 4.000000 1.000000 6.000000 20.000000 1.000000 1.000000 (Stratified) train/test split We want to make sure that the training and test data have appropriate representation of each region; it would be bad for the training data to entirely miss a region. This is especially important because some regions are rather rare. Exercise Questions : Use the train_test_split() function, while (a) ensuring the test size is 20% of the data, and; (2) using 'stratify' argument to split the data (look up documentation online), keeping equal representation of each region. This doesn't work by default, correct? What is the issue? Deal with the issue you encountered above. Hint: you may find numpy's .isnan() and panda's .dropna() functions useful! How did you deal with the error generated by train_test_split ? How did you justify your action? In [32]: ### SOLUTION: try : # Doesn't work: a value is missing train_data , test_data = train_test_split ( league_df , test_size = 0.2 , stratify = league_df [ 'region' ]) except : # Count the missing lines and drop them missing_rows = np . isnan ( league_df [ 'region' ]) print ( \"Uh oh, {} lines missing data! Dropping them\" . format ( np . sum ( missing_rows ))) league_df = league_df . dropna ( subset = [ 'region' ]) train_data , test_data = train_test_split ( league_df , test_size = 0.2 , stratify = league_df [ 'region' ]) Uh oh, 1 lines missing data! Dropping them In [33]: train_data . shape , test_data . shape Out[33]: ((368, 17), (92, 17)) Now that we won't be peeking at the test set, let's explore and look for patterns! We'll introduce a number of useful pandas and numpy functions along the way. Groupby Pandas' .groupby() function is a wonderful tool for data analysis. It allows us to analyze each of several subgroups. Many times, .groupby() is combined with .agg() to get a summary statistic for each subgroup. For instance: What is the average market value, median page views, and maximum fpl for each player position? In [34]: train_data . groupby ( 'position' ) . agg ({ 'market_value' : np . mean , 'page_views' : np . median , 'fpl_points' : np . max }) Out[34]: market_value page_views fpl_points position AM 29.134615 1413.0 218 CB 8.402308 273.0 178 CF 14.799020 841.0 224 CM 8.190217 360.0 225 DM 11.716667 513.5 131 GK 6.025000 419.0 141 LB 7.833333 288.0 177 LM 4.142857 347.0 99 LW 11.427778 467.0 264 RB 7.291667 234.0 142 RM 13.125000 1042.5 105 RW 11.312500 504.0 162 SS 12.666667 1161.0 180 In [35]: train_data . position . unique () Out[35]: array(['LM', 'GK', 'DM', 'RB', 'CB', 'LW', 'CF', 'RW', 'CM', 'AM', 'LB', 'RM', 'SS'], dtype=object) In [36]: train_data . groupby ([ 'big_club' , 'position' ]) . agg ({ 'market_value' : np . mean , 'page_views' : np . mean , 'fpl_points' : np . mean }) Out[36]: market_value page_views fpl_points big_club position 0 AM 14.791667 590.333333 60.000000 CB 4.584694 280.979592 40.734694 CF 8.576389 873.055556 52.138889 CM 5.668919 345.783784 40.135135 DM 7.875000 463.850000 47.750000 GK 4.011364 379.181818 50.045455 LB 4.657895 225.684211 44.736842 LM 4.000000 356.666667 45.166667 LW 6.738095 400.238095 50.285714 RB 4.511364 277.227273 51.045455 RM 6.250000 351.000000 2.500000 RW 8.489583 587.416667 56.541667 SS 7.600000 2165.400000 58.400000 1 AM 41.428571 2590.714286 152.714286 CB 20.093750 1085.625000 69.062500 CF 29.733333 2536.533333 89.400000 CM 18.555556 1634.222222 89.444444 DM 19.400000 1414.200000 69.600000 GK 9.716667 704.333333 50.833333 LB 15.375000 875.625000 76.750000 LM 5.000000 936.000000 26.000000 LW 27.841667 2153.833333 111.166667 RB 14.937500 987.375000 81.000000 RM 20.000000 2028.000000 94.000000 RW 28.250000 1415.750000 78.000000 SS 38.000000 2196.000000 180.000000 Exercise Question : Notice that the .groupby() function above takes a list of two column names. Does the order matter? What happens if we switch the two so that 'position' is listed before 'big_club'? In [37]: ### SOLUTION: train_data . groupby ([ 'position' , 'big_club' ]) . agg ({ 'market_value' : np . mean , 'page_views' : np . mean , 'fpl_points' : np . mean }) # in this case, our values are the same, as we are not aggregating anything differently; # however, our view / grouping is merely different. visually, it often makes most sense to # group such that the left-most (earlier) groupings have fewer distinct options than # the ones to the right of it, but it all depends on what you're trying to discern. Out[37]: market_value page_views fpl_points position big_club AM 0 14.791667 590.333333 60.000000 1 41.428571 2590.714286 152.714286 CB 0 4.584694 280.979592 40.734694 1 20.093750 1085.625000 69.062500 CF 0 8.576389 873.055556 52.138889 1 29.733333 2536.533333 89.400000 CM 0 5.668919 345.783784 40.135135 1 18.555556 1634.222222 89.444444 DM 0 7.875000 463.850000 47.750000 1 19.400000 1414.200000 69.600000 GK 0 4.011364 379.181818 50.045455 1 9.716667 704.333333 50.833333 LB 0 4.657895 225.684211 44.736842 1 15.375000 875.625000 76.750000 LM 0 4.000000 356.666667 45.166667 1 5.000000 936.000000 26.000000 LW 0 6.738095 400.238095 50.285714 1 27.841667 2153.833333 111.166667 RB 0 4.511364 277.227273 51.045455 1 14.937500 987.375000 81.000000 RM 0 6.250000 351.000000 2.500000 1 20.000000 2028.000000 94.000000 RW 0 8.489583 587.416667 56.541667 1 28.250000 1415.750000 78.000000 SS 0 7.600000 2165.400000 58.400000 1 38.000000 2196.000000 180.000000 Part 3.2: Linear regression on the football data This section of the lab focuses on fitting a model to the football (soccer) data and interpreting the model results. The model we'll use is $$\\text{market_value} \\approx \\beta_0 + \\beta_1\\text{fpl_points} + \\beta_2\\text{age} + \\beta_3\\text{age}&#94;2 + \\beta_4log_2\\left(\\text{page_views}\\right) + \\beta_5\\text{new_signing} +\\beta_6\\text{big_club} + \\beta_7\\text{position_cat}$$ We're including a 2nd degree polynomial in age because we expect pay to increase as a player gains experience, but then decrease as they continue aging. We're taking the log of page views because they have such a large, skewed range and the transformed variable will have fewer outliers that could bias the line. We choose the base of the log to be 2 just to make interpretation cleaner. Exercise Questions : Build the data and fit this model to it. How good is the overall model? Interpret the regression model. What is the meaning of the coefficient for: age and age$&#94;2$ $log_2($page_views$)$ big_club What should a player do in order to improve their market value? How many page views should a player go get to increase their market value by 10? In [38]: # Q1: we'll do most of it for you ... y_train = train_data [ 'market_value' ] y_test = test_data [ 'market_value' ] def build_football_data ( df ): x_matrix = df [[ 'fpl_points' , 'age' , 'new_signing' , 'big_club' , 'position_cat' ]] . copy () x_matrix [ 'log_views' ] = np . log2 ( df [ 'page_views' ]) # CREATES THE AGE SQUARED COLUMN x_matrix [ 'age_squared' ] = df [ 'age' ] ** 2 # OPTIONALLY WRITE CODE to adjust the ordering of the columns, just so that it corresponds with the equation above x_matrix = x_matrix [[ 'fpl_points' , 'age' , 'age_squared' , 'log_views' , 'new_signing' , 'big_club' , 'position_cat' ]] # add a constant x_matrix = sm . add_constant ( x_matrix ) return x_matrix # use build_football_data() to transform both the train_data and test_data train_transformed = build_football_data ( train_data ) test_transformed = build_football_data ( test_data ) fitted_model_1 = OLS ( endog = y_train , exog = train_transformed , hasconst = True ) . fit () fitted_model_1 . summary () # WRITE CODE TO RUN r2_score(), then answer the above question about the overall goodness of the model r2_score ( y_test , fitted_model_1 . predict ( test_transformed )) # The model is reasonably good. We're capturing about 64%-69% of the variation in market values, # and the test set confirms that we're not overfitting too badly. Out[38]: 0.6513108642702654 In [39]: # Q2: let's use the age coefficients to show the effect of age has on one's market value; # we can get the age and age&#94;2 coefficients via: agecoef = fitted_model_1 . params . age age2coef = fitted_model_1 . params . age_squared # let's set our x-axis (corresponding to age) to be a wide range from -100 to 100, # just to see a grand picture of the function x_vals = np . linspace ( - 100 , 100 , 1000 ) y_vals = agecoef * x_vals + age2coef * x_vals ** 2 # WRITE CODE TO PLOT x_vals vs y_vals plt . plot ( x_vals , y_vals ) plt . title ( \"Effect of Age\" ) plt . xlabel ( \"Age\" ) plt . ylabel ( \"Contribution to Predicted Market Value\" ) plt . show () # Q2A: WHAT HAPPENS IF WE USED ONLY AGE (not AGE&#94;2) in our model (what's the r2?); make the same plot of age vs market value # Q2B: WHAT HAPPENS IF WE USED ONLY AGE&#94;2 (not age) in our model (what's the r2?); make the same plot of age&#94;2 vs market value # Q2C: PLOT page views vs market value # SOLUTION page_view_coef = fitted_model_1 . params . log_views x_vals = np . linspace ( 0 , 15 ) y_vals = page_view_coef * x_vals plt . plot ( x_vals , y_vals ) plt . title ( \"Effect of Page Views\" ) plt . xlabel ( \"Page Views\" ) plt . ylabel ( \"Contribution to Predicted Market Value\" ) plt . show () # 3- Linear regression on non-experimental data can't determine causation, so we can't prove that # a given relationship runs in the direction we might think. For instance, doing whatever it # takes to get more page views probably doesn't meaningfully increase market value; it's likely # the causation runs in the other direction and great players get more views. Even so, we can use # page views to help us tell who is a great player and thus likely to be paid well. Part 3.3: Turning Categorical Variables into multiple binary variables Of course, we have an error in how we've included player position. Even though the variable is numeric (1,2,3,4) and the model runs without issue, the value we're getting back is garbage. The interpretation, such as it is, is that there is an equal effect of moving from position category 1 to 2, from 2 to 3, and from 3 to 4, and that this effect is probably between -0.5 to -1 (depending on your run). In reality, we don't expect moving from one position category to another to be equivalent, nor for a move from category 1 to category 3 to be twice as important as a move from category 1 to category 2. We need to introduce better features to model this variable. We'll use pd.get_dummies to do the work for us. In [40]: train_design_recoded = pd . get_dummies ( train_transformed , columns = [ 'position_cat' ], drop_first = True ) test_design_recoded = pd . get_dummies ( test_transformed , columns = [ 'position_cat' ], drop_first = True ) train_design_recoded . head () Out[40]: const fpl_points age age_squared log_views new_signing big_club position_cat_2 position_cat_3 position_cat_4 279 1.0 26 32 1024 9.870365 0 1 1 0 0 52 1.0 0 32 1024 6.686501 0 0 0 0 1 312 1.0 80 25 625 8.894818 0 0 1 0 0 57 1.0 0 36 1296 7.748193 0 0 0 0 0 151 1.0 31 27 729 7.857981 0 0 0 1 0 We've removed the original position_cat column and created three new ones. Why only three new columns? Why does pandas give us the option to drop the first category? Exercise Questions : If we're fitting a model without a constant, should we have three dummy columns or four dummy columns? Fit a model on the new, recoded data, then interpret the coefficient of position_cat_2 . In [41]: ### SOLUTION: resu = OLS ( y_train , train_design_recoded ) . fit () resu . summary () print ( \"r2:\" , r2_score ( y_test , resu . predict ( test_design_recoded ))) print ( \"position_cat_2 coef:\" , resu . params . position_cat_2 ) train_design_recoded . shape , y_train . shape r2: 0.6468080878333718 position_cat_2 coef: -1.7978920532876277 Out[41]: ((368, 10), (368,)) SOLUTION: If our model does not have a constant, we must include all four dummy variable columns. If we drop one, we're not modeling any effect of being in that category, and effectively assuming the dropped category's effect is 0. Being in position 2 (instead of position 1) has an impact between -1.54 and +2.38 on a player's market value. Since we're using an intercept, the dropped category becomes the baseline and the effect of any dummy variable is the effect of being in that category instead of the baseline category. Part 4: A nice trick for forward-backwards XOR (operator &#94;) is a logical operation that only returns true when input differ. We can use it to implement forward-or-backwards selection when we want to keep track of whet predictors are \"left\" from a given list of predictors. The set analog is \"symmetric difference\". From the python docs: s.symmetric_difference(t) s &#94; t new set with elements in either s or t but not both In [42]: set () &#94; set ([ 1 , 2 , 3 ]) Out[42]: {1, 2, 3} In [43]: set ([ 1 ]) &#94; set ([ 1 , 2 , 3 ]) Out[43]: {2, 3} In [44]: set ([ 1 , 2 ]) &#94; set ([ 1 , 2 , 3 ]) Out[44]: {3} Exercise Outline a step-forwards algorithm which uses this idea SOLUTION: Start with no predictors in a set, selected_predictors . Then the \"xor\" will give the set of all predictors. Go through them 1-by -1, seeing which has the highest score/ OR lowestaic/bic. Add this predictor to the selected_predictors . Now repeat. The xor will eliminate this predictor from the remaining predictors. In the next iteration we will pick the next predictor which when combined with the first one gibes the lowest aic/bic of all 2-predictor models. We repeat. We finally chose the best bic model from the 1 -predictor models, 2-predictor models, 3-predictor models and so on... BONUS EXERCISE: We have provided a spreadsheet of Boston housing prices (data/boston_housing.csv). The 14 columns are as follows: CRIM: per capita crime rate by town ZN: proportion of residential land zoned for lots over 25,000 sq.ft. INDUS: proportion of non-retail business acres per town CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) NOX: nitric oxides concentration (parts per 10 million) RM: average number of rooms per dwelling AGE: proportion of owner-occupied units built prior to 1940 DIS: weighted distances to ﬁve Boston employment centers RAD: index of accessibility to radial highways TAX: full-value property-tax rate per \\$10,000 PTRATIO: pupil-teacher ratio by town B: 1000(Bk−0.63)2 where Bk is the proportion of blacks by town LSTAT: % lower status of the population MEDV: Median value of owner-occupied homes in $1000s We can see that the input attributes have a mixture of units There are 450 observations. Exercise Using the above file, try your best to predict housing prices. (the 14th column) We have provided a test set data/boston_housing_test.csv but refrain from looking at the file or evaluating on it until you have finalized and trained a model. Load in the data. It is tab-delimited. Quickly look at a summary of the data to familiarize yourself with it and ensure nothing is too egregious. Use a previously-discussed function to automatically partition the data into a training and validation (aka development) set. It is up to you to choose how large these two portions should be. Train a basic model on just a subset of the features. What is the performance on the validation set? Train a basic model on all of the features. What is the performance on the validation set? Toy with the model until you feel your results are reasonably good. Perform cross-validation with said model, and measure the average performance. Are the results what you expected? Were the average results better or worse than that from your original 1 validation set? Experiment with other models, and for each, perform 10-fold cross-validation. Which model yields the best average performance? Select this as your final model. Use this model to evaulate your performance on the testing set. What is your performance (MSE)? Is this what you expected? In [0]: In [0]:","tags":"labs","url":"labs/lecture-10/notebook-3/"},{"title":"Lecture 10: Hypothesis Testing and Predictive CI","text":"Slides Hypothesis Testing [PDF] Hypothesis Testing [PPTX] Predictive CI [PDF] Predictive CI [PPTX] Exercises Lecture 10: 1 - Hypothesis testing [Notebook] Lecture 10: 2 - Prediction CI [Notebook] Lecture 10: Lecture 10 Additional Code: Multiple and Polynomial Regression (September 26, 2019 version) [Notebook]","tags":"lectures","url":"lectures/lecture10/"},{"title":"S-Section 03: Multiple Linear and Polynomial Regression","text":"CS109A Introduction to Data Science Standard Section 3: Multiple Linear Regression and Polynomial Regression Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Sean Murphy, Yinyu Ji In [ ]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) For this section, our goal is to get you familiarized with Multiple Linear Regression. We have learned how to model data with kNN Regression and Simple Linear Regression and our goal now is to dive deep into Linear Regression. Specifically, we will: Load in the titanic dataset from seaborn Learn a few ways to visualize distributions of variables using seaborn Practice single variable OLS and how to interpret coefficients in linear regression Practice multiple linear regression with interaction terms and polynomial regression terms Learn about bootstrapping to generate confidence intervals Understand the assumptions being made in a linear regression model (Bonus 1): look at some cool plots to raise your exploratory data analysis (EDA) game (Bonus 2): look at some example stats models code that produces equivalent results In [ ]: # Data and Stats packages import numpy as np import pandas as pd # Visualization packages import matplotlib.pyplot as plt import seaborn as sns sns . set () # Modeling from sklearn.metrics import r2_score from sklearn.metrics import mean_squared_error from sklearn.neighbors import KNeighborsRegressor from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.linear_model import LinearRegression Extending Linear Regression Working with the Titanic Dataset from Seaborn For our dataset, we'll be using the passenger list from the Titanic, which famously sank in 1912. Let's have a look at the data. Some descriptions of the data are at https://www.kaggle.com/c/titanic/data , and here's how seaborn preprocessed it . The task is to build a regression model to predict the fare , based on different attributes. Let's keep a subset of the data, which includes the following variables: age sex class embark_town alone fare (the response variable) In [ ]: # Load the dataset from seaborn titanic = sns . load_dataset ( \"titanic\" ) titanic . head () In [ ]: # checking for null values chosen_vars = [ 'age' , 'sex' , 'class' , 'embark_town' , 'alone' , 'fare' ] titanic = titanic [ chosen_vars ] titanic . info () Check the datatypes of each column and display the statistics (min, max, mean and any others) for all the numerical columns of the dataset. In [ ]: print ( titanic . dtypes ) titanic . describe () Drop all the non-null rows in the dataset. Is this always a good idea? In [ ]: titanic = titanic . dropna ( axis = 0 ) titanic . info () Now let us visualize the response variable. A good visualization of the distribution of a variable will enable us to answer three kinds of questions: What values are central or typical? (e.g., mean, median, modes) What is the typical spread of values around those central values? (e.g., variance/stdev, skewness) What are unusual or exceptional values (e.g., outliers) In [ ]: fig , ax = plt . subplots ( nrows = 1 , ncols = 3 , figsize = ( 24 , 6 )) ax = ax . ravel () sns . distplot ( titanic [ 'fare' ], ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Seaborn distplot' ) ax [ 0 ] . set_ylabel ( 'Normalized frequencies' ) sns . violinplot ( x = 'fare' , data = titanic , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Seaborn violin plot' ) ax [ 1 ] . set_ylabel ( 'Frequencies' ) sns . boxplot ( x = 'fare' , data = titanic , ax = ax [ 2 ]) ax [ 2 ] . set_title ( 'Seaborn box plot' ) ax [ 2 ] . set_ylabel ( 'Frequencies' ) fig . suptitle ( 'Distribution of count' ); How do we interpret these plots? Train-Test Split Let's split the data into train and test sets. In [ ]: titanic_train , titanic_test = train_test_split ( titanic , train_size = 0.7 , random_state = 42 ) # important for avoiding the infamous SettingwithCopyWarning titanic_train = titanic_train . copy () titanic_test = titanic_test . copy () print ( titanic_train . shape , titanic_test . shape ) Simple One-Variable Linear Regression Now, let's fit a simple model on the training data using LinearRegression from the sklearn library, predicting fare using age . We'll call this model_1 and take a look at its coefficient and R-squared. In [ ]: # assign predictor and response variables X_train = titanic_train [[ 'age' ]] y_train = titanic_train [ 'fare' ] X_test = titanic_test [[ 'age' ]] y_test = titanic_test [ 'fare' ] model_1 = LinearRegression () . fit ( X_train , y_train ) In [ ]: # check the slope/coefficient and intercept values print ( \"Coefficient of the model: \" , model_1 . coef_ ) print ( \"Intercept of the model: \" , model_1 . intercept_ ) # predict on test set y_test_pred = model_1 . predict ( X_test ) # get R-squared print ( \"R-squared of the model:\" , r2_score ( y_test , y_test_pred )) Variable Types In general, you should be able to distinguish between three kinds of variables: Continuous variables: such as fare or age Categorical variables: such as sex or alone . There is no inherent ordering between the different values that these variables can take on. These are sometimes called nominal variables. Read more here . Ordinal variables: such as class (first > second > third). There is some inherent ordering of the values in the variables, but the values are not continuous either. Note : While there is some inherent ordering in class , we will be treating it like a categorical variable. Let us now examine the sex column and see the value counts. In [ ]: titanic_train [ 'sex' ] . value_counts () Create a column sex_male that is 1 if the passenger is male, 0 if female. The value counts indicate that these are the two options in this particular dataset. Ensure that the datatype is int . In [ ]: titanic_train [ 'sex_male' ] = ( titanic_train . sex == 'male' ) . astype ( int ) titanic_train [ 'sex_male' ] . value_counts () Do we need a sex_female column, or a sex_others column? Why or why not? Now, let us look at class in greater detail. Let's one hot encode. One Hot Encoding In [ ]: titanic_train [ 'class_Second' ] = ( titanic_train [ 'class' ] == 'Second' ) . astype ( int ) titanic_train [ 'class_Third' ] = 1 * ( titanic_train [ 'class' ] == 'Third' ) # just another way to do it In [ ]: # do the same for the test set titanic_test_orig = titanic_test . copy () titanic_test = pd . get_dummies ( titanic_test , columns = [ 'sex' , 'class' ], drop_first = True ) titanic_test . head () Working with binary predictors Linear Regression with More Variables Now let's fit a linear regression including the new sex and class variables, calling this model model_2 . In [ ]: # assign predictor and response variables X_train = titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' ]] y_train = titanic_train [ 'fare' ] X_test = titanic_test [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' ]] y_test = titanic_test [ 'fare' ] model_2 = LinearRegression () . fit ( X_train , y_train ) In [ ]: # check the coefficients and intercept values print ( \"Coefficients of the model: \" , model_2 . coef_ ) print ( \"Intercept of the model: \" , model_2 . intercept_ ) # predict on test set y_test_pred = model_2 . predict ( X_test ) # get R-squared print ( \"R-squared of the model:\" , r2_score ( y_test , y_test_pred )) Interpreting These Results Which of the predictors do you think are important? Why? All else equal, what does being male do to the fare? Engineering Variables: Exploring Interactions In [ ]: sns . lmplot ( y = \"fare\" , x = \"age\" , hue = \"sex_male\" , data = titanic_train , height = 7.27 , aspect = 11.7 / 8.27 ) ax = plt . gca () ax . set_title ( \"Predicting Fare from Age + Age*Sex\" ) plt . show () The slopes seem to be different for male and female. What does that indicate? Let us now try to add an interaction effect into our model. In [ ]: # It seemed like sex interacted with age. Can we put that in our model? titanic_train [ 'sex_male_X_age' ] = titanic_train [ 'age' ] * titanic_train [ 'sex_male' ] titanic_test [ 'sex_male_X_age' ] = titanic_test [ 'age' ] * titanic_test [ 'sex_male' ] X_train = titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' ]] y_train = titanic_train [ 'fare' ] X_test = titanic_test [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' ]] y_test = titanic_test [ 'fare' ] model_3 = LinearRegression () . fit ( X_train , y_train ) In [ ]: # check the coefficients and intercept values print ( \"Coefficients of the model: \" , model_3 . coef_ ) print ( \"Intercept of the model: \" , model_3 . intercept_ ) # predict on test set y_test_pred = model_3 . predict ( X_test ) # get R-squared print ( \"R-squared of the model:\" , r2_score ( y_test , y_test_pred )) In [ ]: # It seemed like sex interacted with class. Can we put that in our model? titanic_train [ 'sex_male_X_class_Second' ] = titanic_train [ 'age' ] * titanic_train [ 'class_Second' ] titanic_train [ 'sex_male_X_class_Third' ] = titanic_train [ 'age' ] * titanic_train [ 'class_Third' ] titanic_test [ 'sex_male_X_class_Second' ] = titanic_test [ 'age' ] * titanic_test [ 'class_Second' ] titanic_test [ 'sex_male_X_class_Third' ] = titanic_test [ 'age' ] * titanic_test [ 'class_Third' ] X_train = titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' , 'sex_male_X_class_Second' , 'sex_male_X_class_Third' ]] y_train = titanic_train [ 'fare' ] X_test = titanic_test [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' , 'sex_male_X_class_Second' , 'sex_male_X_class_Third' ]] y_test = titanic_test [ 'fare' ] model_4 = LinearRegression () . fit ( X_train , y_train ) In [ ]: # check the coefficients and intercept values print ( \"Coefficients of the model: \" , model_4 . coef_ ) print ( \"Intercept of the model: \" , model_4 . intercept_ ) # predict on test set y_test_pred = model_4 . predict ( X_test ) # get R-squared print ( \"R-squared of the model:\" , r2_score ( y_test , y_test_pred )) What happened to the age and male terms? Prepare for Breakout Room 1 Boston Housing Prices Dataset Data Description: RM: Average number of rooms per dwelling LSTAT: Percent of housing population classified as \"lower status\" MEDV: Median value of owner-occupied homes in $1000's MEDV will be the response variable (For the curious: https://www.kaggle.com/c/boston-housing ) Load and inspect the data In [ ]: boston = pd . read_csv ( '../data/boston_housing.csv' ) boston . head () Inspect data by visualization (we can combine matplotlib with seaborn) In [ ]: plt . figure ( figsize = [ 16 , 4 ]) plt . subplot ( 1 , 3 , 1 ) sns . scatterplot ( x = \"RM\" , y = \"MEDV\" , data = boston ) plt . subplot ( 1 , 3 , 2 ) sns . scatterplot ( x = \"LSTAT\" , y = \"MEDV\" , data = boston ) plt . subplot ( 1 , 3 , 3 ) sns . scatterplot ( x = \"RM\" , y = \"LSTAT\" , data = boston ); Split the data and perform single variable linear regression with the predictors RM and LSTAT In [ ]: boston_train , boston_test = train_test_split ( boston , train_size = 0.7 , random_state = 109 ) boston_train = boston_train . copy () boston_test = boston_test . copy () # Single variable linear regression with RM model_boston_0a = LinearRegression () . fit ( boston_train [[ \"RM\" ]], boston_train . MEDV ) print ( \"R&#94;2 on training set of the model with RM:\" , r2_score ( boston_train . MEDV , model_boston_0a . predict ( boston_train [[ \"RM\" ]])) ) print ( \"R&#94;2 on testing set of the model with RM:\" , r2_score ( boston_test . MEDV , model_boston_0a . predict ( boston_test [[ \"RM\" ]])) ) print ( '' ) # Single variable linear regression with RM model_boston_0b = LinearRegression () . fit ( boston_train [[ \"LSTAT\" ]], boston_train . MEDV ) print ( \"R&#94;2 on training set of the model with LSTAT:\" , r2_score ( boston_train . MEDV , model_boston_0b . predict ( boston_train [[ \"LSTAT\" ]])) ) print ( \"R&#94;2 on testing set of the model with LSTAT:\" , r2_score ( boston_test . MEDV , model_boston_0b . predict ( boston_test [[ \"LSTAT\" ]])) ) # Store R2 on testing for later R2_0a = r2_score ( boston_test . MEDV , model_boston_0a . predict ( boston_test [[ \"RM\" ]])) R2_0b = r2_score ( boston_test . MEDV , model_boston_0b . predict ( boston_test [[ \"LSTAT\" ]])) Time for Breakout Room 1 Goal : Learn about how to include interaction terms in your linear regression model using the sklearn library. Directions: 1. Build an linear regression model using `RM` and `LSTAT` as predictors. - As before, fit the model on the training data. - Print the coefficients. - Report the R&#94;2 score on the test data. Did the performance improve? - Does the multiple regression model perform better than the two single regression models? 2. Build a model with `LSTAT`, `RM`, and an interaction term between `LSTAT` and `RM` - Print the coefficients. - Does the interaction term improve R&#94;2? Store R&#94;2 on testing sets for later investigation. In [ ]: # %load ../solutions/breakout_1_sol.py Engineering Variables: Exploring Polynomial Regression Perhaps we now believe that the fare also depends on the square of age. How would we include this term in our model? In [ ]: fig , ax = plt . subplots ( figsize = ( 12 , 6 )) ax . plot ( titanic_train [ 'age' ], titanic_train [ 'fare' ], 'o' ) x = np . linspace ( 0 , 80 , 100 ) ax . plot ( x , x , '-' , label = r '$y=x$' ) ax . plot ( x , 0.04 * x ** 2 , '-' , label = r '$y=c x&#94;2$' ) ax . set_title ( 'Plotting Age (x) vs Fare (y)' ) ax . set_xlabel ( 'Age (x)' ) ax . set_ylabel ( 'Fare (y)' ) ax . legend (); Create a model that predicts fare from all the predictors in model_4 + the square of age. Show the summary of this model. Call it model_5 . Remember to use the training data, titanic_train . In [ ]: titanic_train [ 'age&#94;2' ] = titanic_train [ 'age' ] ** 2 titanic_test [ 'age&#94;2' ] = titanic_test [ 'age' ] ** 2 X_train = titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' , 'sex_male_X_class_Second' , 'sex_male_X_class_Third' , 'age&#94;2' ]] y_train = titanic_train [ 'fare' ] X_test = titanic_test [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' , 'sex_male_X_class_Second' , 'sex_male_X_class_Third' , 'age&#94;2' ]] y_test = titanic_test [ 'fare' ] model_5 = LinearRegression () . fit ( X_train , y_train ) In [ ]: # check the coefficients and intercept values print ( \"Coefficients of the model: \" , model_5 . coef_ ) print ( \"Intercept of the model: \" , model_5 . intercept_ ) # predict on test set y_test_pred = model_5 . predict ( X_test ) # get R-squared print ( \"R-squared of the model:\" , r2_score ( y_test , y_test_pred )) Time for Breakout Room 2 Goal : Learn about how to include polynomial terms in your model. Directions: In all the cases print the coefficients and report the R&#94;2 in testing: 1. Build a polynomial regression model including the 2nd degree terms of both `LSTAT` and `RM` 2. Next, include the interaction term between `LSTAT` and `RM` in your model 3. Finally, include the 3rd degree terms of both `LSTAT` and `RM` in your model Can you see any improvement? Reviewing the models you have built thus far, which one would you choose and why? In [ ]: # %load ../solutions/breakout_2_sol.py In [ ]: # Performance of previous models: print ( 'R&#94;2: ' , R2_1 , R2_1_inter ) Model Selection A good model is the model that gives good performance on the testing set while it does not use too many predictors (features). Less features makes a model more stable. In [ ]: R2 = [ R2_0b , R2_0a , R2_1 , R2_1_inter , R2_2 , R2_2_inter , R2_3_inter ] In [ ]: plt . figure ( figsize = [ 8 , 6 ]) x = np . arange ( 0 , 7 , 1 ) plt . plot ( x , R2 , '-ob' ) xTick_label = [ 'R2_0b' , 'R2_0a' , 'R2_1' , 'R2_1_inter' , 'R2_2' , 'R2_2_inter' , 'R2_3_inter' ] plt . xticks ( x , xTick_label , rotation = 'vertical' ) plt . ylabel ( '$R&#94;2$' ) plt . xlabel ( 'Model' ) plt . title ( 'Model Performance' ) We observe that including the interaction term the performance jumps from ~60% to ~72%. Including higher polynomial terms we do not notice a significant improvement. Hence, the best model is the first order polynomial with interaction term. Here, we do not suggest a method for selecting the best model, just introduce the concept. We will be covering cross validation in depth next section! Regression Assumptions The answer to this question can be found on closer examimation of $\\epsilon$. What is $\\epsilon$? It is assumed that $\\epsilon$ is normally distributed with a mean of 0 and variance $\\sigma&#94;2$. But what does this tell us? Assumption 1: Linearity This is an implicit assumption as we claim that Y can be modeled through a linear combination of the predictors. Assumption 2: Independence of observations This comes from the randomness of $\\epsilon$. Assumption 3: Constant variance of $\\epsilon$ errors This means that if we plot our residuals , which are the differences between the true $Y$ and our predicted $\\hat{Y}$, they should look like they have constant variance and a mean of 0. Assumption 4: Normality We assume that the $\\epsilon$ is normally distributed, and we can show this in a histogram of the residuals. In [ ]: y_hat = model_5 . predict ( X_train ) residuals = titanic_train [ 'fare' ] - y_hat # plotting fig , ax = plt . subplots ( ncols = 2 , figsize = ( 16 , 5 )) ax = ax . ravel () ax [ 0 ] . set_title ( 'Plot of Residuals' ) ax [ 0 ] . scatter ( y_hat , residuals , alpha = 0.2 ) ax [ 0 ] . set_xlabel ( r '$\\hat {y} $' ) ax [ 0 ] . set_xlabel ( 'residuals' ) ax [ 1 ] . set_title ( 'Histogram of Residuals' ) ax [ 1 ] . hist ( residuals , alpha = 0.7 ) ax [ 1 ] . set_xlabel ( 'residuals' ) ax [ 1 ] . set_ylabel ( 'frequency' ); # Mean of residuals print ( 'Mean of residuals: {} ' . format ( np . mean ( residuals ))) What can you say about the assumptions of the model? Real data violate assumptions. So why use linear regression? Because linear regression has an analytical solution, so we guaranteed to find the optimal solutions and the solutions are computationally cheap to obtain. Bootstrapping What is a confidence interval? A confidence interval is a range of values that is likely to include a parameter of interest with some degree of certainty or \"confidence.\" How do we interpret 95% confidence intervals? If we were to compute 95% confidence intervals for each of K repeated samples, we would expect 0.95*K of those confidence intervals to contain the true parameter of interest. What is bootstrapping? Bootstrapping is a procedure for resampling a dataset with replacement to produce a distribution of the value of interest. Using the model we selected from above, let's do some bootstrapping to generate the confidence intervals for our coefficients! In [ ]: print ( \"Let's check out the coefficients of R2_1_inter\" ) print ( \"Beta0\" , \":\" , model_boston_1_inter . intercept_ ) for i in range ( 3 ): print ( \"Beta\" + str ( i + 1 ), \":\" , model_boston_1_inter . coef_ [ i ]) In [ ]: # number of bootstraps bootstrap = [] numboot = 200 for i in range ( numboot ): boston_sampled = boston . sample ( frac = 1 , replace = True ) boston_sampled [ \"LSTAT*RM\" ] = boston_sampled [ \"LSTAT\" ] * boston_sampled [ \"RM\" ] model_boston_1_inter = LinearRegression () . fit ( boston_sampled [[ \"LSTAT\" , \"RM\" , \"LSTAT*RM\" ]], boston_sampled . MEDV ) bootstrap . append ( model_boston_1_inter . coef_ ) bootstrap = np . array ( bootstrap ) fig , ax = plt . subplots ( 2 , 2 , figsize = ( 18 , 10 )) ax = ax . ravel () for i in range ( 3 ): betavals = bootstrap [:, i ] betavals . sort () x1 = np . percentile ( betavals , 2.5 ) x2 = np . percentile ( betavals , 97.5 ) x = np . linspace ( x1 , x2 , 500 ) counts , bins = np . histogram ( betavals ) y = counts . max () ax [ i ] . hist ( bootstrap [:, i ], bins = 10 , color = \"gold\" , alpha = 0.5 , edgecolor = 'black' , linewidth = 1 ) ax [ i ] . fill_between ( x , y , color = 'cornflowerblue' , alpha = 0.3 ) ax [ i ] . set_ylabel ( f 'Distribution of beta { i } ' , fontsize = 18 ) ax [ i ] . set_xlabel ( f 'Value of beta { i } ' , fontsize = 18 ) ax [ i ] . axvline ( x = np . mean ( betavals ), color = 'r' ) fig . delaxes ( ax [ 3 ]) fig . suptitle ( f '95 % confidence interval of R2_1_inter Coefficients' , fontsize = 24 ) sns . despine () End of Standard Section Extra: Visual exploration of predictors' correlations The dataset for this problem contains 10 simulated predictors and a response variable. In [ ]: # read in the data data = pd . read_csv ( '../data/dataset3.txt' ) data . head () In [ ]: # this effect can be replicated using the scatter_matrix function in pandas plotting sns . pairplot ( data ); Predictors x1, x2, x3 seem to be perfectly correlated while predictors x4, x5, x6, x7 seem correlated. In [ ]: data . corr () In [ ]: sns . heatmap ( data . corr ()) Extra: A Handy Matplotlib Guide source: http://matplotlib.org/faq/usage_faq.html See also this matplotlib tutorial. See also this violin plot tutorial. Using statsmodel OLS Exercise : You've done this before: make a simple model using the OLS package from the statsmodels library predicting fare using age using the training data. Name your model model_1 and display the summary In [ ]: from statsmodels.api import OLS import statsmodels.api as sm In [ ]: age_ca = sm . add_constant ( titanic_train [ 'age' ]) model_1 = OLS ( titanic_train [ 'fare' ], age_ca ) . fit () model_1 . summary () In [ ]: model_2 = sm . OLS ( titanic_train [ 'fare' ], sm . add_constant ( titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' ]])) . fit () model_2 . summary () In [ ]: titanic_train [ 'sex_male_X_age' ] = titanic_train [ 'age' ] * titanic_train [ 'sex_male' ] model_3 = sm . OLS ( titanic_train [ 'fare' ], sm . add_constant ( titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' ]]) ) . fit () model_3 . summary () In [ ]: # It seemed like gender interacted with age and class. Can we put that in our model? titanic_train [ 'sex_male_X_class_Second' ] = titanic_train [ 'age' ] * titanic_train [ 'class_Second' ] titanic_train [ 'sex_male_X_class_Third' ] = titanic_train [ 'age' ] * titanic_train [ 'class_Third' ] model_4 = sm . OLS ( titanic_train [ 'fare' ], sm . add_constant ( titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' , 'sex_male_X_class_Second' , 'sex_male_X_class_Third' ]]) ) . fit () model_4 . summary () In [ ]: titanic_train [ 'age&#94;2' ] = titanic_train [ 'age' ] ** 2 model_5 = sm . OLS ( titanic_train [ 'fare' ], sm . add_constant ( titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' , 'sex_male_X_class_Second' , 'sex_male_X_class_Third' , 'age&#94;2' ]]) ) . fit () model_5 . summary () predictors = sm . add_constant ( titanic_train [[ 'age' , 'sex_male' , 'class_Second' , 'class_Third' , 'sex_male_X_age' , 'sex_male_X_class_Second' , 'sex_male_X_class_Third' , 'age&#94;2' ]]) y_hat = model_5 . predict ( predictors ) residuals = titanic_train [ 'fare' ] - y_hat # plotting fig , ax = plt . subplots ( ncols = 2 , figsize = ( 16 , 5 )) ax = ax . ravel () ax [ 0 ] . set_title ( 'Plot of Residuals' ) ax [ 0 ] . scatter ( y_hat , residuals , alpha = 0.2 ) ax [ 0 ] . set_xlabel ( r '$\\hat {y} $' ) ax [ 0 ] . set_xlabel ( 'residuals' ) ax [ 1 ] . set_title ( 'Histogram of Residuals' ) ax [ 1 ] . hist ( residuals , alpha = 0.7 ) ax [ 1 ] . set_xlabel ( 'residuals' ) ax [ 1 ] . set_ylabel ( 'frequency' ); # Mean of residuals print ( 'Mean of residuals: {} ' . format ( np . mean ( residuals )))","tags":"sections","url":"sections/sec_3/"},{"title":"S-Section 03: Multiple Linear and Polynomial Regression","text":"Jupyter Notebooks S-Section 3: Multiple Linear and Polynomial Regression","tags":"sections","url":"sections/section3/"},{"title":"Advanced Section 1: Linear Algebra and Hypothesis Testing","text":"Slides A-Section 1: Linear Algebra and Hypothesis Testing [PPTX] A-Section 1: Linear Algebra and Hypothesis Testing [PDF]","tags":"a-sections","url":"a-sections/a-section1/"},{"title":"Lecture 9: Inference in Linear Regression","text":"In [1]: import pandas as pd import numpy as np import matplotlib.pyplot as plt from randomuniverse import RandomUniverse % matplotlib inline Reading the standard Advertising dataset In [2]: df = pd . read_csv ( 'Advertising_adj.csv' ) In [2]: df . head () In [1]: #Create two empty lists that will store the beta values beta0_list , beta1_list = [],[] #Choose the number of \"parallel\" Universes to generate the new dataset parallelUniverses = 1000 for i in range ( parallelUniverses ): df_new = RandomUniverse ( df ) # x is the predictor variable given by 'tv' values # y is the reponse variable given by 'sales' values x = ___ y = ___ #Find the mean of the x values xmean = x . ___ #Find the mean of the y values ymean = y . ___ # Using Linear Algebra as discussed in lecture for beta0 and beta1 beta1 = ___ beta0 = ___ # Append the calculated values of beta1 and beta0 beta0_list . ___ beta1_list . ___ In [3]: ### edTest(test_beta) ### beta0_mean = np . mean ( beta0_list ) beta1_mean = np . mean ( beta1_list ) Now we plot the histograms Returns a plot for a histogram In [4]: # plot histogram of fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax [ 0 ] . ___ ax [ 1 ] . ___ ax [ 0 ] . set_xlabel ( 'Beta 0' ) ax [ 1 ] . set_xlabel ( 'Beta 1' ) ax [ 0 ] . set_ylabel ( 'Frequency' ); Discussion Change the number of parallelUniverses and comment on what you observe. Discuss within the group why you see this behavior. Did you expect the spread to change? Why or why not? Fin","tags":"labs","url":"labs/lecture-9/notebook/"},{"title":"Lecture 9: Inference in Linear Regression","text":"In [1]: # import libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt from randomuniverse import RandomUniverse % matplotlib inline Reading the standard Advertising dataset In [2]: # Read the file \"Advertising_csv\" df = pd . read_csv ( 'Advertising_adj.csv' ) In [3]: # Take a quick look at the data df . head () In [4]: # Define a bootstrap function, which inputs a dataframe & outputs a bootstrapped dataframe def bootstrap ( df ): selectionIndex = np . random . randint ( ___ , size = ___ ) new_df = df . iloc [ ___ ] return new_df In [5]: # Create two empty lists to store beta values beta0_list , beta1_list = [],[] # For each instance of the for loop, call your bootstrap function, calculate the beta values # Store the beta values in the appropriate list #Choose the number of \"parallel\" Universes to generate the new dataset number_of_bootstraps = 1000 for i in range ( number_of_bootstraps ): df_new = bootstrap ( df ) # x is the predictor variable given by 'tv' values # y is the reponse variable given by 'sales' values x = ___ y = ___ #Find the mean of the x values xmean = x . ___ #Find the mean of the y values ymean = y . ___ # Using equations given and discussed in lecture compute the beta0 and beta1 values # Hint: use np.dot to perform the mulitplication operation beta1 = ___ beta0 = ___ # Append the calculated values of beta1 and beta0 beta0_list . append ( ___ ) beta1_list . append ( ___ ) In [0]: ### edTest(test_beta) ### #Compute the mean of the beta0 and beta1 lists beta0_mean = np . mean ( ___ ) beta1_mean = np . mean ( ___ ) In [1]: # plot histogram of beta0 and beta1 fig , ax = plt . subplots ( 1 , 2 , figsize = ( 18 , 8 )) ax [ 0 ] . ___ ax [ 1 ] . ___ ax [ 0 ] . set_xlabel ( ___ ) ax [ 1 ] . set_xlabel ( ___ ) ax [ 0 ] . set_ylabel ( 'Frequency' ); Compare the plots with the results from the RandomUniverse() function In [9]: # The below helper code will help you visualise the similarity between # the bootstrap function you wrote & the RandomUniverse() function from last exercise beta0_randUni , beta1_randUni = [],[] parallelUniverses = 1000 for i in range ( parallelUniverses ): df_new = RandomUniverse ( df ) xmean = df_new . tv . mean () ymean = df_new . sales . mean () # Using Linear Algebra result as discussed in lecture beta1 = np . dot (( x - xmean ) , ( y - ymean )) / (( x - xmean ) ** 2 ) . sum () beta0 = ymean - beta1 * xmean beta0_randUni . append ( beta0 ) beta1_randUni . append ( beta1 ) In [10]: # Use this helper code to plot the bootstrapped beta values & the ones from random universe def plotmulti ( list1 , list2 ): fig , axes = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 ), sharey = 'row' ) axes [ 0 ] . hist ( list1 ); axes [ 0 ] . set_xlabel ( 'Beta Distribution' ) axes [ 0 ] . set_ylabel ( 'Frequency' ) axes [ 0 ] . set_title ( 'Bootstrap' ) axes [ 1 ] . hist ( list2 ); axes [ 1 ] . set_xlabel ( 'Beta Distribution' ) axes [ 1 ] . set_title ( 'Random Universe' ) In [2]: # Just use the 'plotmulti' function above to compare the two histograms for beta0 plotmulti ( beta0_list , beta0_randUni ) In [3]: #Now compare for beta1 plotmulti ( beta1_list , beta1_randUni ) In [0]:","tags":"labs","url":"labs/lecture-9/notebook-2/"},{"title":"Lecture 9: Inference in Linear Regression","text":"In [1]: # import the libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline Reading the standard Advertising dataset In [2]: # Read the 'Advertising_adj.csv' file df = pd . read_csv ( 'Advertising_adj.csv' ) In [7]: # Use your bootstrap function from the previous exercise def bootstrap ( df ): selectionIndex = np . random . randint ( len ( df ), size = len ( df )) new_df = df . iloc [ selectionIndex ] return new_df In [8]: # Like last time, create a list of beta values using 1000 bootstraps of your original data beta0_list , beta1_list = [],[] numberOfBootstraps = 100 for i in range ( numberOfBootstraps ): df_new = bootstrap ( df ) xmean = df_new . tv . mean () ymean = df_new . sales . mean () beta1 = np . dot (( df_new . tv - xmean ) , ( df_new . sales - ymean )) / (( df_new . tv - xmean ) ** 2 ) . sum () beta0 = ymean - beta1 * xmean beta0_list . append ( beta0 ) beta1_list . append ( beta1 ) In [9]: ### edTest(test_sort) ### # Sort the two lists of beta values from lowest value to highest beta0_list . ___ ; beta1_list . ___ ; In [10]: ### edTest(test_beta) ### # Now we find the confidence interval # Find the 95% percent confidence interval using the percentile function beta0_CI = ( np . ___ , np . ___ ) beta1_CI = ( np . ___ , np . ___ ) In [0]: #Print the confidence interval of beta0 upto 3 decimal points print ( f 'The beta0 confidence interval is { ___ } ' ) In [0]: #Print the confidence interval of beta1 upto 3 decimal points print ( f 'The beta1 confidence interval is { ___ } ' ) In [15]: # Use this helper function to plot the histogram of beta values along with the 95% confidence interval def plot_simulation ( simulation , confidence ): plt . hist ( simulation , bins = 30 , label = 'beta distribution' , align = 'left' , density = True ) plt . axvline ( confidence [ 1 ], 0 , 1 , color = 'r' , label = 'Right Interval' ) plt . axvline ( confidence [ 0 ], 0 , 1 , color = 'red' , label = 'Left Interval' ) plt . xlabel ( 'Beta value' ) plt . ylabel ( 'Frequency' ) plt . title ( 'Confidence Interval' ) plt . legend ( frameon = False , loc = 'upper right' ) In [0]: # Plot for beta 0 plot_simulation ( _ , _ ) In [0]: #Plot for beta 1 plot_simulation ( _ , _ )","tags":"labs","url":"labs/lecture-9/notebook-3/"},{"title":"Lecture 9: Inference in Linear Regression","text":"Slides Into to Inference [PDF] Intro to Inference [PPTX] Inference & Bootstrap [PDF] Inference & Bootstrap [PPTX] Exercises Lecture 9: A.1 - Beta values for data from Random Universe [Notebook] Lecture 9: B.1 - Beta values for Data from Random Universe using Bootstrap [Notebook] Lecture 9: B.2 - Confidence Interval for Beta value [Notebook]","tags":"lectures","url":"lectures/lecture09/"},{"title":"Lecture 8: Probability","text":"In [0]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline In [0]: from scipy.stats import norm (a) Let $X\\sim N(500,75&#94;2)$. Determine $P(X\\geq 600)$. In [0]: ### edTest(test_norm_prob) ### prob = 1 - norm . cdf ( ___ , ___ , ___ ) prob (b) Plotting the normal distribution of $X\\sim N(500,75&#94;2)$. In [0]: # define parameters mu = ___ sigma = ___ # the 'dummy' x for plotting x = np . arange ( 200 , 800 ) # calculate the normal distribution at each value of x prob = norm . pdf ( ___ , mu , sigma ) # plot it plt . plot ( ___ , ___ ); plt . title ( r '$\\mathrm{N(\\mu=500, \\sigma&#94;2=75&#94;2)}$' ) plt . ylim (( 0 , 0.006 )) plt . show () Question: Does your answer to part (a) makes sense based on this curve? (c) Calculating simple likelihoods In [0]: ### edTest(test_likelihood) ### # define the data set x = [ 3 , 5 , 10 ] # sigma is known to be 2, an estimate for mu # is what we need to determine. Consider #the values (4, 4.01, 4.02, ..., 7.99). sigma = 2 mu = np . arange ( ___ , ___ , 0.01 ) # calculate the likelihood like = norm . pdf ( x [ 0 ], mu , sigma ) * ___ * ___ #plot it plt . plot ( mu , like , color = \"darkred\" ); plt . title ( 'Likelihood Function' ) plt . xlabel ( r '$\\mu$' ) plt . show () (d) Determine the maximum likelihood estimate for $\\mu$. In [0]: ### edTest(test_mle) ### # determine which value of mu aligns with where # the maximum of the likelihood function is mle = ___ [ np . argmax ( __ )] mle Question: How would you numerically maximize this function if both the mean and variance were unknown? How would you visualize the likelihood function? In [0]:","tags":"labs","url":"labs/lecture-8/notebook/"},{"title":"Lecture 8: Probability","text":"In [0]: # import libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline In [0]: # Read the data from 'movies.csv' to a dataframe movies = pd . read_csv ( 'movies.csv' ) In [0]: #Take a peak at the dataset movies . head () (a) Plot the scatterplot to predict 'domestic' revenue from the 'budget' cost of the movie. In [0]: # create the scatterplot to predict 'domestic' # from 'budget' plt . scatter ( ___ , ___ , marker = '.' ) plt . xlabel ( 'Budget (millions $)' ) plt . ylabel ( 'Domestic Gross (millions $)' ) plt . title ( 'Revenue vs. Budget for Major Domestic Movies since 1980' ) plt . show () Question: What stands out in the plots above?Does linear regression seems appropriate based on this scatterplot? (b) Use sklearn to get linear regression estimates. In [0]: ### edTest(test_sklearn_regress) ### from sklearn.linear_model import LinearRegression regress = LinearRegression () . fit ( ___ , ___ ) print ( regress . coef_ , regress . intercept_ ) (c) Fit a linear regression model using OLS from statsmodels instead, and compare. In [0]: import statsmodels.api as sm # you have to first create the X matrix with the # intercept included, then fit the model X = sm . add_constant ( movies [ 'budget' ]) ols1 = sm . OLS ( movies [ 'domestic' ], X ) . fit () ols1 . summary () Question: When would it be prefered to fit a regression model without an intercept? (d) Fit a second OLS model ( OLS2 ) with 'budget' and 'year' as predictors and compare to OLS1 . In [0]: ### edTest(test_ols2) ### X = sm . add_constant ( ___ ) ols2 = sm . OLS ( movies [ 'domestic' ], X ) . fit () ols2 . summary () Question: How does the coefficient estimate for budget compare in this multiple regression to the corresponding estimate in the simple regression model? Why is that the case? (e) Fit a model with the interaction term between budget and year (first need to define it) and the 'main effects' of the 2 predictors, and interpret the results. In [0]: ### edTest(test_interaction) ### #create the interaction term interaction = ___ * ___ movies [ 'interaction' ] = interaction # define the X matrix X = ___ #fit the model ols3 = sm . OLS ( ___ , ___ ) . fit () ols3 . summary () Question: How have the estimates changed in this model compared to the earlier ones (especially for budget)? Why is this the case? (e) Investigate the assumptions to this linear regression model ( OLS3 ) using the plots below. In [0]: # define predicted values (yhat) and residuals yhat = ols3 . predict () resid = ___ - ___ #plot the histogram of the residuals plt . ___ ( resid , bins = 20 ) plt . show () In [0]: # residual scatterplot plt . scatter ( ___ , ___ , marker = '.' ) plt . hlines ( 0 , xmin = 0 , xmax = 500 , color = 'red' ) plt . show () Question: What stands out in the plots above? Bonus Question: Confirm the log-likelihood evaluation for OLS1 (just plug in your estimates). In [0]: ### use ols1.params,ols1.mse_resid, and norm.logpdf as your basis from scipy.stats import norm","tags":"labs","url":"labs/lecture-8/notebook-2/"},{"title":"Lecture 8: Probability","text":"Slides Probability [PDF] Probability [PPTX] Exercises Lecture 8: 1 - Normal Distributions and Likelihood [Notebook] Lecture 8: 2 - Linear Regression in Statsmodels [Notebook]","tags":"lectures","url":"lectures/lecture08/"},{"title":"Lecture 7: Model Selection","text":"In [1]: #import the required libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures % matplotlib inline In [2]: # Read the data from 'poly.csv' to a dataframe df = pd . read_csv ( 'poly.csv' ) # Get the column values for x & y in numpy arrays x = df [[ 'x' ]] . ___ y = df [ 'y' ] . ___ --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) in 2 df = pd . read_csv ( 'poly.csv' ) 3 # Get the column values for x & y ----> 4 x = df [ [ 'x' ] ] . ___ 5 y = df [ 'y' ] . ___ /usr/lib/python3.8/site-packages/pandas/core/generic.py in __getattr__ (self, name) 5128 if self . _info_axis . _can_hold_identifiers_and_holds_name ( name ) : 5129 return self [ name ] -> 5130 return object . __getattribute__ ( self , name ) 5131 5132 def __setattr__ ( self , name : str , value ) -> None : AttributeError : 'DataFrame' object has no attribute '___' In [0]: # Take a quick look at the dataframe df . head () In [0]: # Plot x & y to visually inspect the data fig , ax = plt . subplots () ax . plot ( x , y , 'x' ) ax . set_xlabel ( '$x$ values' ) ax . set_ylabel ( '$y$ values' ) ax . set_title ( '$y$ vs $x$' ); In [16]: # Fit a linear model on the data model = ____ model . ___ ( ___ ) # Get the predictions on the entire data using the .predict() function y_lin_pred = model . predict ( ___ ) In [18]: ### edTest(test_deg) ### # Now, we try polynomial regression # GUESS the correct polynomial degree based on the above graph guess_degree = ___ # Generate polynomial features on the entire data x_poly = PolynomialFeatures ( degree = guess_degree ) . fit_transform ( ___ ) In [19]: #Fit a polynomial model on the data, using x_poly as features polymodel = LinearRegression () polymodel . fit ( _ , _ ) y_poly_pred = polymodel . predict ( ___ ) In [20]: # To visualise the results, sort the x values using the helper code below # Worth examining and understand the code idx = np . argsort ( x [:, 0 ]) x = x [ idx ] # Use the above index to get the appropriate predicted values for y # y values corresponding to sorted x y = y [ idx ] #Linear predicted values y_lin_pred = y_lin_pred [ idx ] #Non-linear predicted values y_poly_pred = y_poly_pred [ idx ] In [0]: # First plot x & y values using plt.scatter plt . scatter ( _ , _ , s = 10 , label = \"Data\" ) # Now, plot the linear regression fit curve plt . plot ( _ , _ , label = \"Linear fit\" ) # Also plot the polynomial regression fit curve plt . plot ( _ , _ , label = \"Polynomial fit\" ) #Assigning labels to the axes plt . xlabel ( \"x values\" ) plt . ylabel ( \"y values\" ) plt . legend () plt . show () In [10]: ### edTest(test_poly_predictions) ### #Calculate the residual values for the polynomial model poly_residuals = ___ In [0]: ### edTest(test_linear_predictions) ### #Calculate the residual values for the linear model lin_residuals = ___ In [0]: #Use the below helper code to plot residual values #Plot the histograms of the residuals for the two cases #Distribution of residuals fig , ax = plt . subplots ( 1 , 2 , figsize = ( 10 , 4 )) bins = np . linspace ( - 20 , 20 , 20 ) ax [ 0 ] . set_xlabel ( 'Residuals' ) ax [ 0 ] . set_ylabel ( 'Frequency' ) #Plot the histograms for the polynomial regression ax [ 0 ] . hist ( ___ , bins , label = ___ ) #Plot the histograms for the linear regression ax [ 0 ] . hist ( ___ , bins , label = ___ ) ax [ 0 ] . legend ( loc = 'upper left' ) # Distribution of predicted values with the residuals ax [ 1 ] . scatter ( y_poly_pred , poly_residuals , s = 10 ) ax [ 1 ] . scatter ( y_lin_pred , lin_residuals , s = 10 ) ax [ 1 ] . set_xlim ( - 75 , 75 ) ax [ 1 ] . set_xlabel ( 'Predicted values' ) ax [ 1 ] . set_ylabel ( 'Residuals' ) fig . suptitle ( 'Residual Analysis (Linear vs Polynomial)' ); Question: Do you think that polynomial degree is appropriate. Experiment with a degree of polynomial =2 and comment on what you observe for the residuals.","tags":"labs","url":"labs/lecture-7/notebook/"},{"title":"Lecture 7: Model Selection","text":"In [2]: # import libraries import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from pprint import pprint % matplotlib inline In [3]: # Read the file named \"colinearity.csv\" df = pd . read_csv ( \"colinearity.csv\" ) In [5]: #Take a quick look at the dataset df . head () Out[5]: x1 x2 x3 x4 y 0 -1.109823 -1.172554 -0.897949 -6.572526 -158.193913 1 0.288381 0.360526 2.298690 3.884887 198.312926 2 -1.059194 0.833067 0.285517 -1.225931 12.152087 3 0.226017 1.979367 0.744038 5.380823 190.281938 4 0.664165 -1.373739 0.317570 -0.437413 -72.681681 Creation of Linear Regression Objects In [9]: # Choose all the predictors as the variable 'X' (note capitalization of X for multiple features) X = df . drop ([ ___ ], axis = 1 ) # Choose the response variable 'y' for y values y = df . ___ In [13]: ### edTest(test_coeff) ### # Here we create a dictionary that will store the Beta values of each linear regression model linear_coef = [] for i in X : x = df [[ ___ ]] #Create a linear regression object linreg = ____ #Fit it with training values. #Remember to choose only one column at a time as the predictor variable linreg . fit ( ___ , ___ ) # Add the coefficient value of the model to the list linear_coef . append ( linreg . coef_ ) Multi-Linear Regression using all variables In [20]: # Here you must do a multi-linear regression with all predictors # use sklearn library to define a new model 'multi_linear' multi_linear = ____ # Fit the multi-linear regression on all features and the response multi_linear . fit ( ___ , ___ ) # append the coefficients (plural) of the model to a variable multi_coef multi_coef = multi_linear . coef_ Printing the individual $\\beta$ values In [24]: # Run this command to see the beta values of the linear regression models print ( 'By simple(one variable) linear regression for each variable:' , sep = ' \\n ' ) for i in range ( 4 ): pprint ( f 'Value of beta { i + 1 } = { linear_coef [ i ][ 0 ] : .2f } ' ) By simple(one variable) linear regression for each variable: 'Value of beta1 = 34.73' 'Value of beta2 = 68.63' 'Value of beta3 = 59.40' 'Value of beta4 = 20.92' In [26]: ### edTest(test_multi_coeff) ### #Now let's compare with the values from the multi-linear regression print ( 'By multi-Linear regression on all variables' ) for i in range ( 4 ): pprint ( f 'Value of beta { i + 1 } = { round ( multi_coef [ i ], 2 ) } ' ) By multi-Linear regression on all variables 'Value of beta1 = -24.61' 'Value of beta2 = 27.72' 'Value of beta3 = 37.67' 'Value of beta4 = 19.27' Why do you think the $\\beta$ values are different in the two cases? In [27]: corrMatrix = df [[ 'x1' , 'x2' , 'x3' , 'x4' ]] . corr () sns . heatmap ( corrMatrix , annot = True ) plt . show ()","tags":"labs","url":"labs/lecture-7/notebook-2/"},{"title":"Lecture 7: Model Selection","text":"In [3]: #import libraries % matplotlib inline import operator import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error Reading the dataset In [2]: #Read the file \"dataset.csv\" as a dataframe filename = \"dataset.csv\" df = pd . read_csv ( filename ) In [0]: # Assign the values to the predictor and response variables x = df [[ 'x' ]] . ___ y = df . y . ___ Train-validation split In [5]: ### edTest(test_random) ### #Split the dataset into train and validation sets with 75% Training set and 25% validation set. #Set random_state=1 x_train , x_val , y_train , y_val = train_test_split ( ___ ) Computing the train and validation error in terms of MSE In [8]: ### edTest(test_regression) ### # To iterate over the range, select the maximum degree of the polynomial maxdeg = ___ # Create two empty lists to store training and validation MSEs training_error , validation_error = [],[] #Run a for loop through the degrees of the polynomial, fit linear regression, predict y values and calculate the training and testing errors and update it to the list for d in range ( maxdeg ): #Compute the polynomial features for the train and validation sets x_poly_train = PolynomialFeatures ( d ) . fit_transform ( ___ ) x_poly_val = PolynomialFeatures ( d ) . fit_transform ( ___ ) lreg = LinearRegression () lreg . fit ( x_poly_train , y_train ) y_train_pred = lreg . predict ( ___ ) y_val_pred = lreg . predict ( ___ ) #Compute the train and validation MSE training_error . append ( mean_squared_error ( ___ )) validation_error . append ( mean_squared_error ( ___ )) Finding the best degree In [0]: ### edTest(test_best_degree) ### #The best degree is the model with the lowest validation error min_mse = min ( validation_error ) best_degree = validation_error . index ( min_mse ) print ( \"The best degree of the model is\" , best_degree ) Plotting the error graph In [0]: # Plot the errors as a function of increasing d value to visualise the training and testing errors fig , ax = plt . subplots () #Plot the training error with labels ax . plot ( ___ ) #Plot the validation error with labels ax . plot ( ___ ) # Set the plot labels and legends ax . set_xlabel ( 'Degree of Polynomial' ) ax . set_ylabel ( 'Mean Squared Error' ) ax . legend ( loc = 'best' ) ax . set_yscale ( 'log' ) plt . show () Once you have marked your exercise, run again with Random_state = 0 Do you see any change in the results with change in the random state? If so, what do you think is the reason behind it? Your answer here","tags":"labs","url":"labs/lecture-7/notebook-3/"},{"title":"Lecture 7: Model Selection","text":"In [39]: #import libraries % matplotlib inline import operator import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import cross_validate from sklearn.model_selection import train_test_split from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error Reading the dataset In [40]: #Read the file \"dataset.csv\" as a dataframe filename = \"dataset.csv\" df = pd . read_csv ( filename ) In [41]: # Assign the values to the predictor and response variables x = df [[ 'x' ]] . values y = df . y . values Train-validation split In [42]: ### edTest(test_random) ### #Split the data into train and validation sets with 75% for training and with a random_state=1 x_train , x_val , y_train , y_val = train_test_split ( ___ ) Computing the MSE In [43]: ### edTest(test_regression) ### # To iterate over the range, select the maximum degree of the polynomial maxdeg = 10 # Create three empty lists to store training, validation and cross-validation MSEs training_error , validation_error , cross_validation_error = [],[],[] #Run a for loop through the degrees of the polynomial, fit linear regression, predict y values and calculate the training and testing errors and update it to the list for d in range ( ___ ): #Compute the polynomial features for the entire data, train data and validation data x_poly_train = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) x_poly_val = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) x_poly = PolynomialFeatures ( ___ ) . fit_transform ( ___ ) #Get a Linear Regression object lreg = LinearRegression () #Perform cross-validation on the entire data with 10 folds and get the mse_scores mse_score = cross_validate ( ___ ) #Fit model on the training set lreg . fit ( ___ ) #Predict of the training and validation set y_train_pred = lreg . predict ( ___ ) y_val_pred = lreg . predict ( ___ ) #Compute the train and validation MSE training_error . append ( mean_squared_error ( ___ )) validation_error . append ( mean_squared_error ( ___ )) #Compute the mean of the cross validation error and store in list #Remember to take into account the sign of the MSE metric returned by the cross_validate function cross_validation_error . append ( ___ ) Finding the best degree In [0]: ### edTest(test_best_degree) ### #The best degree with the lowest validation error min_mse = min ( ___ ) best_degree = validation_error . index ( ___ ) #The best degree with the lowest cross-validation error min_cross_val_mse = min ( ___ ) best_cross_val_degree = cross_validation_error . index ( ___ ) print ( \"The best degree of the model using validation is\" , best_degree ) print ( \"The best degree of the model using cross-validation is\" , best_cross_val_degree ) Plotting the error graph In [0]: # Plot the errors as a function of increasing d value to visualise the training and validation errors fig , ax = plt . subplots () #Plot the training error with labels ax . plot ( range ( maxdeg ), training_error , label = 'Training error' ) #Plot the cross-validation error with labels ax . plot ( range ( maxdeg ), cross_validation_error , label = 'Cross-Validation error' ) # Set the plot labels and legends ax . set_xlabel ( 'Degree of Polynomial' ) ax . set_ylabel ( 'Mean Squared Error' ) ax . legend ( loc = 'best' ) ax . set_yscale ( 'log' ) plt . show () Once you have marked your exercise, run again with Random_state = 0 Do you see any change in the results with change in the random state? If so, what do you think is the reason behind it? Your answer here","tags":"labs","url":"labs/lecture-7/notebook-4/"},{"title":"Lecture 7: Model Selection","text":"Slides Model Selection [PDF] Model Selection [PPTX] Exercises Lecture 7: A.1 - Linear and Polynomial Regression with Residual Analysis [Notebook] Lecture 7: A.2 - Multi-collinearity vs Model Predictions [Notebook] Lecture 7: B.1 - Best Degree of Polynomial with Train and Validation sets [Notebook] Lecture 7: B.2 - Best Degree of Polynomial using Cross-validation [Notebook]","tags":"lectures","url":"lectures/lecture07/"},{"title":"S-Section 02: kNN and Linear Regression","text":"CS109A Introduction to Data Science Standard Section 2: kNN and Linear Regression Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Hayden Joy, Lauren Baker, and Kaela Nelson In [1]: #RUN THIS CELL import requests from IPython.core.display import HTML styles = requests . get ( \"http://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: For this section, our goal is to get you familiarized with k-Nearest Neighbors (kNN) and Linear Regression. In the course thus far, we have discussed some aspects of dealing with data, including scraping data from the web, organizing it using dictionaries and Pandas dataframes, and visualizing it using Matplotlib plotting functionality. Now we're moving on to data modeling! It is useful to make models to fit and predict data. Why? To understand the underlying behavior of your data. By the end of this section, you should feel comfortable: Performing exploratory data analysis (EDA) on dataset Splitting this dataset into a training and test set (and understanding why you need to do this!) Applying simple models (kNN and Linear Regression) to your data using sklearn and statsmodels packages Using these models to understand relationships between the response variable and the predictors (also can be called features or descriptors) Evaluating model performance using metrics such as $R&#94;2$ For this section we will be using the following packages: In [2]: #Matrices, Dataframe and Plotting Operations import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt % matplotlib inline #Model packages from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor from sklearn.linear_model import LinearRegression from sklearn import metrics from statsmodels.api import OLS import statsmodels.api as sm Load in the Bikeshare dataset and perform preliminary EDA Here, we will work with a dataset collected from the Capital Bikeshare program in Washington D.C. This dataset contains over two years of data on the total number of bike rentals per day, as well as 10 attributes describing the day and its weather (see below for description of these variables as recorded in the dataset). The data set is provided in the file 'bikeshare.csv'. The task is to build a regression model to predict the total number of bike rentals in a given day (known as the response variable) based on attributes about the day (known as the descriptors). Such a demand forecasting model would be useful in planning the number of bikes that need to be available in the system on any given day, and also in monitoring traffic in the city. Description of variables season (1 = spring, 2 = summer, 3 = fall, 4 = winter) month (1 through 12, with 1 denoting Jan) holiday (1 = the day is a holiday, 0 = otherwise) day_of_week (0 through 6, with 0 denoting Sunday) workingday (1 = the day is neither a holiday or weekend, 0 = otherwise) weather 1: Clear, Few clouds, Partly cloudy, Partly cloudy 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog temp (temperature in Celsius) atemp (apparent, or relative outdoor, or real feel temperature, in Celsius) humidity (relative humidity) windspeed (wind speed) count (response variable i.e. total number of bike rentals on the day) Load the BikeShare dataset In [3]: bikeshare = pd . read_csv ( '../data/bikeshare.csv' ) print ( \"Length of Dataset:\" , len ( bikeshare )) display ( bikeshare . head ()) Length of Dataset: 731 Unnamed: 0 season month holiday day_of_week workingday weather temp atemp humidity windspeed count 0 0 2.0 5.0 0.0 2.0 1.0 2.0 24.0 26.0 76.5833 0.118167 6073.0 1 1 4.0 12.0 0.0 2.0 1.0 1.0 15.0 19.0 73.3750 0.174129 6606.0 2 2 2.0 6.0 0.0 4.0 1.0 1.0 26.0 28.0 56.9583 0.253733 7363.0 3 3 4.0 12.0 0.0 0.0 0.0 1.0 0.0 4.0 58.6250 0.169779 2431.0 4 4 3.0 9.0 0.0 3.0 1.0 3.0 23.0 23.0 91.7083 0.097021 1996.0 Drop unnecessary columns In [4]: bikeshare = bikeshare . drop ( columns = [ 'Unnamed: 0' ]) print ( \"Length of Dataset:\" , len ( bikeshare )) display ( bikeshare . head ()) Length of Dataset: 731 season month holiday day_of_week workingday weather temp atemp humidity windspeed count 0 2.0 5.0 0.0 2.0 1.0 2.0 24.0 26.0 76.5833 0.118167 6073.0 1 4.0 12.0 0.0 2.0 1.0 1.0 15.0 19.0 73.3750 0.174129 6606.0 2 2.0 6.0 0.0 4.0 1.0 1.0 26.0 28.0 56.9583 0.253733 7363.0 3 4.0 12.0 0.0 0.0 0.0 1.0 0.0 4.0 58.6250 0.169779 2431.0 4 3.0 9.0 0.0 3.0 1.0 3.0 23.0 23.0 91.7083 0.097021 1996.0 Use the describe feature of Pandas to summarize data In [5]: display ( bikeshare . describe ()) season month holiday day_of_week workingday weather temp atemp humidity windspeed count count 731.000000 731.000000 731.000000 731.000000 731.000000 731.000000 731.000000 731.000000 731.000000 731.000000 731.000000 mean 2.496580 6.519836 0.028728 2.997264 0.683995 1.395349 16.229822 18.885089 62.789406 0.190486 4552.354309 std 1.110807 3.451913 0.167155 2.004787 0.465233 0.544894 11.531101 10.278475 14.242910 0.077498 2021.971365 min 1.000000 1.000000 0.000000 0.000000 0.000000 1.000000 -11.000000 -6.000000 0.000000 0.022392 22.000000 25% 2.000000 4.000000 0.000000 1.000000 0.000000 1.000000 6.000000 10.000000 52.000000 0.134950 3116.000000 50% 3.000000 7.000000 0.000000 3.000000 1.000000 1.000000 16.000000 20.000000 62.666700 0.180975 4575.000000 75% 3.000000 10.000000 0.000000 5.000000 1.000000 2.000000 26.000000 27.000000 73.020850 0.233214 6048.000000 max 4.000000 12.000000 1.000000 6.000000 1.000000 3.000000 39.000000 42.000000 97.250000 0.507463 10045.000000 Note, we can also use the groupby function to look at mean stats, aggregated by month in this case In [6]: bikeshare . groupby ( 'month' ) . mean () Out[6]: season holiday day_of_week workingday weather temp atemp humidity windspeed count month 1.0 1.000000 0.048387 2.870968 0.645161 1.419355 -0.080645 4.145161 58.582827 0.206303 2498.306452 2.0 1.000000 0.035088 3.000000 0.684211 1.368421 3.912281 7.807018 56.746468 0.215684 2824.315789 3.0 1.354839 0.000000 3.096774 0.725806 1.451613 9.580645 13.080645 58.847503 0.222699 3822.241935 4.0 2.000000 0.033333 3.000000 0.666667 1.416667 14.583333 17.783333 58.806312 0.234482 4348.600000 5.0 2.000000 0.032258 2.903226 0.693548 1.403226 22.532258 24.661290 68.895831 0.182989 5268.790323 6.0 2.333333 0.000000 3.100000 0.716667 1.233333 28.150000 29.216667 57.580552 0.185420 5547.033333 7.0 3.000000 0.032258 2.935484 0.661290 1.193548 32.580645 33.338710 59.787632 0.166059 5563.677419 8.0 3.000000 0.000000 3.000000 0.741935 1.306452 29.629032 30.096774 63.773010 0.172918 5584.887097 9.0 3.266667 0.033333 3.050000 0.666667 1.500000 23.850000 25.466667 71.471437 0.165945 5653.316667 10.0 4.000000 0.032258 2.919355 0.677419 1.564516 15.580645 18.693548 69.376087 0.175205 5199.225806 11.0 4.000000 0.066667 3.033333 0.666667 1.350000 8.316667 12.116667 62.487648 0.183801 4454.633333 12.0 2.935484 0.032258 3.064516 0.661290 1.532258 5.451613 9.612903 66.604052 0.176609 3781.741935 Plot the count of bike rentals by month In [7]: plt . figure ( figsize = [ 8 , 5 ]) plt . plot ( bikeshare . groupby ( 'month' ) . mean ()[ 'count' ], '-ob' ) plt . xlabel ( 'Month' ) plt . ylabel ( 'Count' ) plt . title ( 'Total Count of Bikeshare Rentals per Month' ) plt . xlim ([ 0 , 13 ]) plt . show () Is there a difference between temp and a_temp? Let's plot them both In [52]: plt . figure ( figsize = [ 8 , 5 ]) plt . plot ( bikeshare [ 'temp' ], bikeshare [ 'atemp' ], '.-b' , alpha = 0.5 ) # toggle alpha to 1 plt . xlabel ( 'Temp' ) plt . ylabel ( 'A-Temp' ) plt . title ( 'A-Temp vs. Temp' ) plt . show () What did we do wrong here? Why does the plot look like this? Sorting! Whenever your plot makes zig-zag changes across the scale, it is because matplotlib is trying to connect the points sequentially from the top (using a line plot) and skipping across the scale when $x_{i+1}$ is lower than $x_{i}$. So let's sort. In [9]: # Sorting new = bikeshare . sort_values ([ 'temp' ]) plt . figure ( figsize = [ 8 , 5 ]) plt . plot ( new [ 'temp' ], new [ 'atemp' ], '-b' , alpha = 1 ) plt . xlabel ( 'Temp' ) plt . ylabel ( 'A-Temp' ) plt . title ( 'A-Temp vs Temp' ) plt . show () It still looks weird, why? Let's have a closer look at the dataframe: In [10]: display ( new . head ( 10 )) season month holiday day_of_week workingday weather temp atemp humidity windspeed count 176 1.0 1.0 0.0 6.0 0.0 1.0 -11.0 -6.0 40.0000 0.171970 981.0 367 1.0 1.0 0.0 1.0 1.0 1.0 -9.0 -4.0 49.1739 0.158330 1416.0 265 1.0 1.0 0.0 0.0 0.0 1.0 -9.0 -5.0 43.6522 0.246600 986.0 346 1.0 1.0 0.0 3.0 1.0 2.0 -8.0 -3.0 41.4583 0.184700 2368.0 612 1.0 2.0 0.0 3.0 1.0 2.0 -7.0 -2.0 49.4783 0.188839 1605.0 82 1.0 2.0 0.0 0.0 0.0 1.0 -7.0 -5.0 46.4583 0.409212 1529.0 475 1.0 2.0 0.0 4.0 1.0 1.0 -6.0 -2.0 43.7391 0.221935 1538.0 343 1.0 1.0 0.0 0.0 0.0 1.0 -6.0 -4.0 43.4167 0.361950 822.0 270 1.0 1.0 0.0 2.0 1.0 1.0 -6.0 -3.0 44.1250 0.365671 2236.0 457 1.0 1.0 0.0 5.0 1.0 1.0 -5.0 1.0 53.7826 0.126548 6079.0 There are multiple atemp values for each temp value, which if not sorted will bounce around at the same x-value. Thus, we need to sort both axes simultaneously. In [11]: new = bikeshare . sort_values ([ 'temp' , 'atemp' ]) plt . figure ( figsize = [ 8 , 5 ]) plt . plot ( new [ 'temp' ], new [ 'atemp' ], '-b' ) plt . xlabel ( 'Temp' ) plt . ylabel ( 'A-Temp' ) plt . title ( 'A-Temp vs Temp' ) plt . show () By plotting efficiently, we found an anomaly we would have otherwise overlooked. It looks like there is a problem with the data around temp greater than 30 and atemp less than 10 . Show all rows in the dataframe where the temp is greater than 30 and the atemp is less than 10 In [12]: display ( bikeshare [( bikeshare [ 'temp' ] > 30 ) & ( bikeshare [ 'atemp' ] < 10 )]) season month holiday day_of_week workingday weather temp atemp humidity windspeed count 188 3.0 8.0 0.0 5.0 1.0 1.0 31.0 4.0 57.0833 0.231354 7148.0 Anomaly! atemp and temp are usually lineary related except at this one datapoint. Now, we get to make a judgement call as to whether we should keep the datapoint. For this example, we will drop this datapoint, but we will come back to the question of how to manage abnormal/missing data after the lecture on Missing Data and Imputation. In [13]: bikeshare = bikeshare . drop ([ 188 ]) We can now try what we wrote and we should end up with no rows in the dataframe where the temp is greater than 30 and the atemp is less than 10 In [14]: display ( bikeshare [( bikeshare [ 'temp' ] > 30 ) & ( bikeshare [ 'atemp' ] < 10 )]) season month holiday day_of_week workingday weather temp atemp humidity windspeed count This EDA enabled us to spot this anomoly and clean the data (dropping a data point that may have influenced the model). Splitting up the data into a training set and a test set using the 'train_test_split' function from sklearn Now that we have an idea of what the data looks like, we want to predict the count. We will be randomly splitting up the data into a training and a testing set. Scikit learn (sklearn) has a function that does this for us, called \"train_test_split.\" What is the need for training and testing data sets? The training set will be used to train the model, while the testing set will be used to quantify how well that model does on data it has never seen before. Evaluating the accuracy of a model on a testing set ensures that the model doesn't overfit our training data. Why random split? Randomization helps manage uncontrollable confounding variables within the data. Let us first create a function that will randomly split the data up into a 70-30 split, with 70% of the data going into the training set In [15]: # from sklearn.model_selection import train_test_split train_data , test_data = train_test_split ( bikeshare , test_size = 0.30 , random_state = 42 ) print ( \"Length of Training set = \" , len ( train_data )) print ( \"Length of Testing set = \" , len ( test_data )) Length of Training set = 511 Length of Testing set = 219 Random state of 42 is arbitrary (Pavlos favorite number, google for \"the number of universe\") but fixing this value will produce the same randomization of data every time (useful for homework grading, research, etc.). Calculate the ratio of the number of points in the training set to the number of points in the testing set to see if we have split the data correctly In [16]: print ( 'The training data length is the' , 100 * len ( train_data ) / len ( bikeshare ), ' % o f the total dataset length.' ) print ( 'The testing data length is the' , 100 * len ( test_data ) / len ( bikeshare ), ' % o f the total dataset length.' ) The training data length is the 70.0 % of the total dataset length. The testing data length is the 30.0 % of the total dataset length. Time for Break Out Room 1 Goal: Practice using train_test_split and visualizing data using matplotlib Directions: Load \"sim_data.csv\" file into dataframe Inspect data Drop any unncessary columns Split data into 80% training data and 20% test data (\"80-20 train-test split\") Plot training and test data in single plot Color training data in blue and test data in red. Also, use different markers for the two sets, e.g. o and * Include a legend, title, and axes labels! Hint: dont forget to sort! In [17]: # your code here In [3]: # %load ../solutions/breakout_1_sol.py # read in data data = pd . read_csv ( \"../data/sim_data.csv\" ) # drop Unnamed column data . drop ( columns = [ \"Unnamed: 0\" ], inplace = True ) # split into training and testing with 80/20 split, random_state=42 train_data1 , test_data1 = train_test_split ( data , test_size = 0.20 , random_state = 42 ) # plot results 80/20 split plt . figure ( figsize = [ 8 , 5 ]) plt . scatter ( train_data1 . x , train_data1 . y , c = \"blue\" , marker = 'o' , label = \"train data\" ) plt . scatter ( test_data1 . x , test_data1 . y , c = \"red\" , marker = '*' , label = \"test data\" ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( \"80/20 Split\" ) plt . legend () plt . show () Now let's do some modeling General sklearn model fitting code-structure : #Split Data into Train and Test Set x_train, y_train = training_data.drop('Response_Variable', axis=1), training_data['Response_Variable'] x_test, y_test = test_data.drop('Response_Variable', axis=1), test_data['Response_Variable'] #Define Model model = sklearn_model_name(hyper_parameter1 = value1, hyper_parameter2 = value2) #Fit Model model.fit(x_train, y_train) #Get Prediction y_pred_train = model.predict(x_train) y_pred_test = model.predict(x_test) #Evaluate Model r2_train = model.score(y_train, y_pred_train) r2_test = model.score(y_test, y_pred_test) #Print Results print(\"Score for Model (Training):\", r2_train) print(\"Score for Model (Testing) :\", r2_test) Every model has a list of hyperparameters that can be set using sklearn for the specific problem. We find optimal hyperparameters through exploration (one way is cross-validation, which we will discuss soon). model.fit calculates the parameters of your model corresponding to the training data and hyperparameters you provided. model.predict(X) is the standard method called to make the model predict values for a specific X. Depending on if you feed x_train or x_test, you will get a y_prediction_train or y_prediction_test respectively. Evaluation of model can vary according to the task at hand i.e. Regression or Classification. For Regression, $R&#94;2$ Score is standard while for Classification, Accuracy (%) is standard. kNN Regression Using sklearn to implement kNN We will use the temperature parameter to predict total bike rental count. We can use sklearn to implement kNN, fit the model, and use various metrics to assess our accuracy. In [20]: # from sklearn.neighbors import KNeighborsRegressor # Set kNN hyperparameter: k = 10 # First, we create the classifier object: neighbors = KNeighborsRegressor ( n_neighbors = k ) # Then, we fit the model using x_train as training data and y_train as target values: neighbors . fit ( train_data [[ 'temp' ]], train_data [ 'count' ]) # Retrieve our predictions: prediction_knn = neighbors . predict ( test_data [[ 'temp' ]]) # This returns the mean accuracy on the given test data and labels, or in other words, # the R squared value -- A constant model that always predicts the expected value of y, # disregarding the input features, would get a R&#94;2 score of 1. r2_train = neighbors . score ( train_data [[ 'temp' ]], train_data [ 'count' ]) r2_test = neighbors . score ( test_data [[ 'temp' ]], test_data [ 'count' ]) print ( \"Length of Test Data:\" , len ( test_data [ 'count' ])) print ( \"R&#94;2 Score of kNN on training set:\" , r2_train ) print ( \"R&#94;2 Score of kNN on testing set: \" , r2_test ) Length of Test Data: 219 R&#94;2 Score of kNN on training set: 0.36710873429408564 R&#94;2 Score of kNN on testing set: 0.19290625155095908 In [21]: # SubPlots fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 6 )) # train data axes [ 0 ] . set_ylim ([ 0 , 10000 ]) axes [ 0 ] . plot ( train_data [ 'temp' ], train_data [ 'count' ], 'bo' , alpha = 0.5 , label = 'Data' ) sorted_temp = train_data . sort_values ([ 'temp' ]) prediction_knn = neighbors . predict ( sorted_temp [[ 'temp' ]]) axes [ 0 ] . plot ( sorted_temp [ 'temp' ], prediction_knn , 'k-' , linewidth = 5 , markersize = 10 , label = 'Prediction' ) axes [ 0 ] . set_xlabel ( 'Temperature' , fontsize = 15 ) axes [ 0 ] . set_ylabel ( '# of Rentals' , fontsize = 15 ) axes [ 0 ] . set_title ( \"Train Set\" , fontsize = 18 ) axes [ 0 ] . legend ( loc = 'upper right' , fontsize = 12 ) # test data axes [ 1 ] . set_ylim ([ 0 , 10000 ]) axes [ 1 ] . plot ( test_data [ 'temp' ], test_data [ 'count' ], 'r*' , alpha = 0.5 , label = 'Data' ) sorted_temp = test_data . sort_values ([ 'temp' ]) prediction_knn = neighbors . predict ( sorted_temp [[ 'temp' ]]) axes [ 1 ] . plot ( sorted_temp [ 'temp' ], prediction_knn , 'g-' , linewidth = 5 , markersize = 10 , label = 'Prediction' ) axes [ 1 ] . set_xlabel ( 'Temperature' , fontsize = 15 ) axes [ 1 ] . set_ylabel ( '# of Rentals' , fontsize = 15 ) axes [ 1 ] . set_title ( \"Test Set\" , fontsize = 18 ) axes [ 1 ] . legend ( loc = 'upper right' , fontsize = 12 ) fig . suptitle ( \"kNN Regression (k= {} ): Temp vs Rental Count\" . format ( k ), fontsize = 20 ) plt . show () Time for Break Out Room 2 Goal: Practice using sklearn's kNN regression Directions: Use same dataset from Break Out Room 1 (\"sim_data.csv\") Perform 70-30 train-test split using a random state of 42 Create a function that implements kNN regression with your choice of k (explore a few different k's) Predict on both training and test data For all kNN models generated, plot the following on the same plot: Original train data = blue Original test data = red Predicted train data = black Predicted test data = green Calculate $R&#94;2$ score Hints: dont forget to sort! can make plot colors more transparent using \"alpha\" and lines thicker using \"linewidth\" In [22]: # your code here In [24]: # %load ../solutions/breakout_2_sol.py # from sklearn.neighbors import KNeighborsRegressor # split into 70/30, random_state=42 sim_train_data , sim_test_data = train_test_split ( data , test_size = 0.30 , random_state = 42 ) def knn_model ( k , train_data , test_data ): # create the classifier object neighbors = KNeighborsRegressor ( n_neighbors = k ) # fit the model using x_train as training data and y_train as target values neighbors . fit ( train_data [[ 'x' ]], train_data [ 'y' ]) sorted_train = train_data . sort_values ([ 'x' ]) sorted_test = test_data . sort_values ([ 'x' ]) # Retreieve our predictions: train_preds = neighbors . predict ( sorted_train [[ 'x' ]]) test_preds = neighbors . predict ( sorted_test [[ 'x' ]]) # find r&#94;2 r2_train = neighbors . score ( train_data [[ 'x' ]], train_data [ 'y' ]) r2_test = neighbors . score ( test_data [[ 'x' ]], test_data [ 'y' ]) print ( \"R&#94;2 Score of kNN on training set with k= {} :\" . format ( k ), r2_train ) print ( \"R&#94;2 Score of kNN on testing set: with k= {} \" . format ( k ), r2_test ) return sorted_train , sorted_test , train_preds , test_preds , r2_train , r2_test def plot_predictions_same_plot ( k , train_data , test_data , train_preds , test_preds ): # plot all results on same plot plt . figure ( figsize = [ 8 , 6 ]) plt . plot ( train_data [ 'x' ], train_data [ 'y' ], 'bo' , alpha = 0.5 , label = 'Train Set' ) plt . plot ( train_data [ 'x' ], train_preds , 'k-' , linewidth = 2 , markersize = 10 , label = 'Train Preds' ) plt . plot ( test_data [ 'x' ], test_data [ 'y' ], 'r*' , alpha = 0.5 , label = 'Test Set' ) plt . plot ( test_data [ 'x' ], test_preds , 'g-' , linewidth = 2 , markersize = 10 , label = 'Test Preds' ) plt . xlabel ( 'x' ) plt . ylabel ( 'y' ) plt . title ( \"x vs y kNN Regression (k= {} )\" . format ( k )) plt . legend () plt . show () knn_train_preds = [] knn_test_preds = [] knn_r2_train_scores = [] knn_r2_test_scores = [] for k in [ 1 , 10 , 70 ]: sim_sorted_train , sim_sorted_test , sim_train_preds , sim_test_preds , knn_r2_train , knn_r2_test = knn_model ( k , sim_train_data , sim_test_data ) plot_predictions_same_plot ( k , sim_sorted_train , sim_sorted_test , sim_train_preds , sim_test_preds ) knn_train_preds . append ( sim_train_preds ) knn_test_preds . append ( sim_test_preds ) knn_r2_train_scores . append ( knn_r2_train ) knn_r2_test_scores . append ( knn_r2_test ) R&#94;2 Score of kNN on training set with k=1: 1.0 R&#94;2 Score of kNN on testing set: with k=1 -0.24443896120147302 R&#94;2 Score of kNN on training set with k=10: 0.41569204879718624 R&#94;2 Score of kNN on testing set: with k=10 0.3960264344425989 R&#94;2 Score of kNN on training set with k=70: -2.220446049250313e-16 R&#94;2 Score of kNN on testing set: with k=70 -0.0167373824706607 Linear Regression We just went over the kNN prediction method. Now, we will fit the same data using a linear regression model. What is the main difference between a kNN model and linear regression model? Linear regression specifies the model (whatever the data is, the model will fit a linear line) whereas kNN learns the model and fits the best curve. Advantages of linear regression models are that they are very fast and yield an exact optimal solution. For a more in-depth discussion on generalized linear models, please see the Advanced Section on this. We will use the same training/testing dataset as before and create linear regression objects. We can do this using sklearn (as we did for kNN) as well as with another package called statsmodels . In [25]: # Label data as X,Y for ease x_train , y_train = train_data [ 'temp' ], train_data [ 'count' ] x_test , y_test = test_data [ 'temp' ], test_data [ 'count' ] You can also split into train test by x and y using train test split X_train, X_test, y_train, y_test = train_test_split( bikeshare['temp']), bikeshare['count'] ) Linear Regression using sklearn Fit a Linear Regression model using sklearn and take a look at the model's parameters In [26]: from sklearn.linear_model import LinearRegression lr_sklearn = LinearRegression () . fit ( x_train . values . reshape ( - 1 , 1 ), y_train ) # x data must be 2D array print ( 'Coefficients:' , lr_sklearn . coef_ ) print ( 'Intercept:' , lr_sklearn . intercept_ ) Coefficients: [94.6299874] Intercept: 2977.527482393689 Note: only one coefficient here since only using one descriptor variable (temp) Use model to predict on training and testing data and plot prediction In [27]: # predict y_preds_train = lr_sklearn . predict ( x_train . values . reshape ( - 1 , 1 )) y_preds_test = lr_sklearn . predict ( x_test . values . reshape ( - 1 , 1 )) # plot predictions fig , axes = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) axes = axes . ravel () axes [ 0 ] . scatter ( x_train , y_train , color = 'b' , alpha = 0.5 , label = 'Data' ) axes [ 0 ] . plot ( x_train , y_preds_train , 'k' , linewidth = 5 , label = 'Prediction' ) axes [ 0 ] . set_title ( 'Train Set' , fontsize = 18 ) axes [ 1 ] . scatter ( x_test , y_test , color = 'r' , marker = '*' , alpha = 0.5 , label = 'Data' ) axes [ 1 ] . plot ( x_test , y_preds_test , 'g' , linewidth = 5 , label = 'Prediction' ) axes [ 1 ] . set_title ( 'Test Set' , fontsize = 18 ) for i , ax in enumerate ( axes ): ax . set_ylim ( 0 , 10000 ) ax . set_xlabel ( 'Temperature' , fontsize = 15 ) ax . set_ylabel ( '# of Rentals' , fontsize = 15 ) ax . legend ( loc = 'upper right' , fontsize = 12 ) fig . suptitle ( \"Linear Regression: Temp vs Rental Count\" , fontsize = 20 ) plt . show () Compute performance metrics for both training and testing In [28]: # from sklearn import metrics # Mean Squared Error (MSE) print ( \"MSE Train: {:.3f} \" . format ( metrics . mean_squared_error ( y_train , y_preds_train ))) print ( \"MSE Test: {:.3f} \" . format ( metrics . mean_squared_error ( y_test , y_preds_test ))) # R&#94;2 score print ( \"R&#94;2 Train: {:.3f} \" . format ( metrics . r2_score ( y_train , y_preds_train ))) print ( \"R&#94;2 Test: {:.3f} \" . format ( metrics . r2_score ( y_test , y_preds_test ))) MSE Train: 2845979.037 MSE Test: 3357507.577 R&#94;2 Train: 0.296 R&#94;2 Test: 0.189 Recall that more accurate models will have higher $R&#94;2$ scores (value of 1 is perfect fitted line) and lower MSEs (meaning lower error). Notice that the $R&#94;2$ for training is higher and MSE is lower than that of the test set, indicating some overfitting to the training set. For more info on these, check out sklearn metrics documentation. Take a look at the end of the notebook for calculations of MSE and $R&#94;2$ metrics by hand. Linear Regression using statsmodels Fit a Linear Regression model using statsmodels and print out the coefficients of temp and const Note : OLS = ordinary least squares Must add constants to X data since an intercept is not included by default (unlike sklearn linear regression) statsmodels OLS first entry is the response variable (Y) followed by X whereas sklearn uses X followed by Y structure In [29]: # from statsmodels.api import OLS # import statsmodels.api as sm # Add constant to x data x_train_ca = sm . add_constant ( x_train ) x_test_ca = sm . add_constant ( x_test ) # Create Linear Regression object model = sm . OLS ( y_train , x_train_ca ) # Fit results = model . fit () print ( results . params ) const 2977.527482 temp 94.629987 dtype: float64 Use the fitted model (saved as \"results\") to predict on train and test data and plot prediction In [30]: # Plotting our model fig , axes = plt . subplots ( 1 , 2 , figsize = ( 20 , 6 )) axes = axes . ravel () # train data axes [ 0 ] . plot ( x_train , y_train , 'bo' , alpha = 0.5 , label = 'Data' ) sorted_temp = train_data . sort_values ([ 'temp' ]) prediction_lr = results . predict ( sm . add_constant ( sorted_temp [[ 'temp' ]])) axes [ 0 ] . plot ( sorted_temp [ 'temp' ], prediction_lr , 'k-' , linewidth = 5 , label = 'Prediction' ) axes [ 0 ] . set_title ( 'Train Set' , fontsize = 18 ) # test data axes [ 1 ] . plot ( x_test , y_test , 'r*' , alpha = 0.5 , label = 'Data' ) sorted_temp = test_data . sort_values ([ 'temp' ]) prediction_lr = results . predict ( sm . add_constant ( sorted_temp [[ 'temp' ]])) axes [ 1 ] . plot ( sorted_temp [ 'temp' ], prediction_lr , 'g-' , linewidth = 5 , label = 'Prediction' ) axes [ 1 ] . set_title ( 'Test Set' , fontsize = 18 ) for i , ax in enumerate ( axes ): ax . set_ylim ( 0 , 10000 ) ax . set_xlabel ( 'Temperature' , fontsize = 15 ) ax . set_ylabel ( '# of Rentals' , fontsize = 15 ) ax . legend ( loc = 'upper right' , fontsize = 12 ) fig . suptitle ( 'Linear Regression: Temp vs Rental Count (statsmodels)' , fontsize = 20 ) plt . show () Check out $R&#94;2$ (remember 1 is perfect prediction) In [31]: print ( \"R&#94;2 Score Train (statsmodels linear regression):\" , metrics . r2_score ( y_train , results . predict ( x_train_ca ))) print ( \"R&#94;2 Score Test (statsmodels linear regression):\" , metrics . r2_score ( y_test , results . predict ( x_test_ca ))) R&#94;2 Score Train (statsmodels linear regression): 0.29634842804618666 R&#94;2 Score Test (statsmodels linear regression): 0.18898167147912737 Notice that these $R&#94;2$ scores are indeed the same as what we found above using sklearn's linear regression. Check out results.summary() and see the nice table that statsmodels provides! In [32]: results . summary () Out[32]: OLS Regression Results Dep. Variable: count R-squared: 0.296 Model: OLS Adj. R-squared: 0.295 Method: Least Squares F-statistic: 214.4 Date: Fri, 18 Sep 2020 Prob (F-statistic): 9.18e-41 Time: 13:12:41 Log-Likelihood: -4522.2 No. Observations: 511 AIC: 9048. Df Residuals: 509 BIC: 9057. Df Model: 1 Covariance Type: nonrobust coef std err t P>|t| [0.025 0.975] const 2977.5275 127.983 23.265 0.000 2726.088 3228.967 temp 94.6300 6.463 14.641 0.000 81.932 107.328 Omnibus: 13.052 Durbin-Watson: 2.062 Prob(Omnibus): 0.001 Jarque-Bera (JB): 13.710 Skew: 0.397 Prob(JB): 0.00105 Kurtosis: 2.888 Cond. No. 33.9 Notes: [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. Time for Break Out Room 3 Goal: Compare kNN and linear regression for the same dataset Directions: Use same dataset from previous Break Out Rooms, with 70-30 train-test split and random state of 42 Create a function that implements linear regression with sklearn or statsmodels Predict on both training and test data Create 2 subplots with the following plotted: Subplot 1: Train set Plot training data in blue Plot linear regression prediction in black Plot kNN prediction (using k = 10) in magenta ('m') Subplot 2: Test set Plot testing data in red Plot linear regression prediction in green Plot kNN prediction (using k = 10) in yellow ('y') Calculate $R&#94;2$ scores for both train and test sets for both kNN and linear regression Hints: don't forget sort! plt.subplots(...) creates subplots In [34]: # %load ../solutions/breakout_3_sol.py # linear regression # from statsmodels.api import OLS # import statsmodels.api as sm def linreg_model ( train_data , test_data ): # sort sorted_train = train_data . sort_values ([ 'x' ]) sorted_test = test_data . sort_values ([ 'x' ]) x_train , x_test , y_train , y_test = sorted_train [ 'x' ], sorted_test [ 'x' ], sorted_train [ 'y' ], sorted_test [ 'y' ] # Add constant to x data x_train_ca = sm . add_constant ( x_train ) x_test_ca = sm . add_constant ( x_test ) # Create Linear Regression object linreg_model = sm . OLS ( y_train , x_train_ca ) # Fit results = linreg_model . fit () # predict train_preds = results . predict ( x_train_ca ) test_preds = results . predict ( x_test_ca ) # find r&#94;2 r2_train = metrics . r2_score ( y_train , results . predict ( x_train_ca )) r2_test = metrics . r2_score ( y_test , results . predict ( x_test_ca )) return train_preds , test_preds , r2_train , r2_test def plot_predictions2 ( k , train_data , test_data , knn_train_preds , knn_test_preds , linreg_train_preds , linreg_test_preds ): # SubPlots fig , axes = plt . subplots ( nrows = 1 , ncols = 2 , figsize = ( 20 , 6 )) axes [ 0 ] . plot ( train_data [ 'x' ], train_data [ 'y' ], 'bo' , alpha = 0.5 , label = 'Data' ) axes [ 0 ] . plot ( train_data [ 'x' ], knn_train_preds , 'm-' , linewidth = 2 , markersize = 10 , label = 'KNN Preds' ) axes [ 0 ] . plot ( train_data [ 'x' ], linreg_train_preds , 'k-' , linewidth = 2 , markersize = 10 , label = 'Linreg Preds' ) axes [ 0 ] . set_xlabel ( 'x' ) axes [ 0 ] . set_ylabel ( 'y' ) axes [ 0 ] . set_title ( \"Train Data\" ) axes [ 0 ] . legend () axes [ 1 ] . plot ( test_data [ 'x' ], test_data [ 'y' ], 'r*' , alpha = 0.5 , label = 'Data' ) axes [ 1 ] . plot ( test_data [ 'x' ], knn_test_preds , 'y-' , linewidth = 2 , markersize = 10 , label = 'KNN Preds' ) axes [ 1 ] . plot ( test_data [ 'x' ], linreg_test_preds , 'g-' , linewidth = 2 , markersize = 10 , label = 'Test Preds' ) axes [ 1 ] . set_xlabel ( 'x' ) axes [ 1 ] . set_ylabel ( 'y' ) axes [ 1 ] . set_title ( \"Test Data\" ) axes [ 1 ] . legend () fig . suptitle ( \"KNN vs Linear Regression\" ) plt . show () # get predictions linreg_train_preds , linreg_test_preds , linreg_r2_train , linreg_r2_test = linreg_model ( sim_train_data , sim_test_data ) # plot linreg predictions side by side with knn predictions k = 10 plot_predictions2 ( k , sim_sorted_train , sim_sorted_test , knn_train_preds [ 1 ], knn_test_preds [ 1 ], linreg_train_preds , linreg_test_preds ) # print r2 scores for knn with k=10 and linreg print ( \"R&#94;2 Score of kNN on training set with k= {} :\" . format ( k ), knn_r2_train_scores [ 1 ]) print ( \"R&#94;2 Score of kNN on testing set: with k= {} \" . format ( k ), knn_r2_test_scores [ 1 ]) print ( \"R&#94;2 Score of linear regression on training set\" , linreg_r2_train ) print ( \"R&#94;2 Score of linear regression on testing set\" , linreg_r2_test ) R&#94;2 Score of kNN on training set with k=10: 0.41569204879718624 R&#94;2 Score of kNN on testing set: with k=10 0.3960264344425989 R&#94;2 Score of linear regression on training set 0.3816018646727135 R&#94;2 Score of linear regression on testing set 0.46326892325303626 Understanding model uncertainty Confidence Intervals In Data Science, a confidence interval (CI) is a type of interval estimate, computed from the statistics of the observed data, that might contain the true value of an unknown population parameter. Simply speaking, a Confidence Interval is a range of values we are fairly sure our true value lies in. It is important to remind ourselves here that Confidence Intervals belong to a parameter and not a statistic. Thus, they represent the window in which the true value exists for the entire population when all we have is a sample. See if you can implement a 95% confidence interval using statsmodels In [35]: # Confidence Interval using Stats Model Summary thresh = 0.05 intervals = results . conf_int ( alpha = thresh ) # Renaming column names first_col = str ( thresh / 2 * 100 ) + \"%\" second_col = str (( 1 - thresh / 2 ) * 100 ) + \"%\" intervals = intervals . rename ( columns = { 0 : first_col , 1 : second_col }) display ( intervals ) 2.5% 97.5% const 2726.088195 3228.966770 temp 81.932160 107.327814 In the above block of code, results.conf_int(alpha=thresh) returns a dataframe with columns 0 and 1. We explained Confidence Intervals above where because we assume normal symetric distribution of data, the 95% Confidence Interval means there's 2.5% chance of the true value lying below the values in Column 0 and 2.5% chance of the true value lying above Column 1. End of Standard Section Extra: Train-Test Split using a mask In [36]: #Function to Split data into Train and Test Set def split_data ( data ): #Calculate Length of Dataset length = len ( data ) #Define Split split = 0.7 #Set a random Seed For Shuffling np . random . seed ( 9001 ) #Generate a Mask with a X:Y Split mask = np . random . rand ( length ) < split #Separate train and test data data_train = data [ mask ] data_test = data [ ~ mask ] #Return Separately return data_train , data_test In [37]: #Split data using defined function train_data_manual , test_data_manual = split_data ( bikeshare ) print ( \"Length of Training set:\" , len ( train_data_manual )) print ( \"Length of Testing set:\" , len ( test_data_manual )) Length of Training set: 507 Length of Testing set: 223 In [38]: ## Check that the ratio between test and train sets is right test_data_manual . shape [ 0 ] / ( test_data_manual . shape [ 0 ] + train_data_manual . shape [ 0 ]) Out[38]: 0.30547945205479454 Extra: Implementing the kNN Algorithm by hand To really understand how the kNN algorithm works, it helps to go through the algorithm line by line in code. In [39]: #kNN Algorithm def knn_algorithm ( train , test , k ): #Create any empty list to store our predictions in predictions = [] #Separate the response and predictor variables from training and test set: train_x = train [ 'temp' ] train_y = train [ 'count' ] test_x = test [ 'temp' ] test_y = test [ 'count' ] for i , ele in enumerate ( test_x ): #For each test point, store the distance between all training points and test point distances = pd . DataFrame (( train_x . values - ele ) ** 2 , index = train . index ) distances . columns = [ 'dist' ] #display(distances) #Then, we sum across the columns per row to obtain the Euclidean distance squared ##distances = vec_distances.sum(axis = 1) #Sort the distances to training points (in ascending order) and take first k points nearest_k = distances . sort_values ( by = 'dist' ) . iloc [: k ] #For simplicity, we omitted the square rooting of the Euclidean distance because the #square root function preserves order. #Take the mean of the y-values of training set corresponding to the nearest k points k_mean = train_y [ nearest_k . index ] . mean () #Add on the mean to our predicted y-value list predictions . append ( k_mean ) #Create a dataframe with the x-values from test and predicted y-values predict = test . copy () predict [ 'predicted_count' ] = pd . Series ( predictions , index = test . index ) return predict Now to run the algorithm on our dataset with $k = 5$: In [40]: #Run the kNN function k = 5 predicted_knn = knn_algorithm ( train_data , test_data , k ) predicted_knn . head () Out[40]: season month holiday day_of_week workingday weather temp atemp humidity windspeed count predicted_count 469 2.0 5.0 0.0 5.0 1.0 2.0 17.0 20.0 86.3333 0.179725 9696.0 4616.4 148 3.0 8.0 0.0 4.0 1.0 1.0 30.0 30.0 42.3750 0.164796 4792.0 6534.4 303 1.0 1.0 0.0 0.0 0.0 1.0 3.0 6.0 31.1250 0.240050 3243.0 3060.2 356 1.0 3.0 0.0 6.0 0.0 1.0 3.0 6.0 35.0417 0.225750 4118.0 3060.2 516 2.0 4.0 0.0 2.0 1.0 1.0 23.0 27.0 39.0417 0.273629 6691.0 6713.2 We want to have a way to evaluate our predictions from the kNN algorithm with $k=5$. One way is to compute the $R&#94;2$ coefficient. Let's create a function for that: In [41]: #Test predictions in comparison to true value of test set def evaluate ( predicted , true ): #Find the squared error: squared_error = ( predicted [ 'predicted_count' ] - true [ 'count' ]) ** 2 #Finding the mean squared error: error_var = squared_error . sum () sample_var = (( true [ 'count' ] - true [ 'count' ] . mean ()) ** 2 ) . sum () r = ( 1 - ( error_var / sample_var )) return r Then let's apply this function to our predictions: In [42]: print ( \"Length of Test Data:\" , len ( test_data )) print ( \"R&#94;2 Score of kNN test:\" , evaluate ( predicted_knn , test_data )) Length of Test Data: 219 R&#94;2 Score of kNN test: 0.15985957302330167 In [43]: predicted_knn_train = knn_algorithm ( test_data , train_data , k ) print ( \"R&#94;2 Score of kNN train:\" , evaluate ( predicted_knn_train , train_data )) R&#94;2 Score of kNN train: 0.21984316699072837 Extra: Computing different performance metrics by hand Now, we will compute metrics that can be used to assess fit. Note: sklearn.metrics is class of functions that consists of all the metrics we care about to evaluate our models. While it is not hard to implement them yourself, it is helpful to go through http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics . In [44]: model = sm . OLS ( y_train , x_train_ca ) results = model . fit () #Predict on train and test y_pred_train = results . predict ( x_train_ca ) y_pred_test = results . predict ( x_test_ca ) #Calc squared error squared_error_train = ( y_pred_train - y_train ) ** 2 squared_error_test = ( y_pred_test - y_test ) ** 2 #Calc mean squared error error_var_train = squared_error_train . mean () error_var_test = squared_error_test . mean () #Calc variance sample_var_train = (( y_train - y_train . mean ()) ** 2 ) . mean () sample_var_test = (( y_test - y_test . mean ()) ** 2 ) . mean () #Calc R&#94;2 r_sq_train = 1 - error_var_train / sample_var_train r_sq_test = 1 - error_var_test / sample_var_test print ( 'MSE train:' , error_var_train , 'R&#94;2 train:' , r_sq_train ) print ( 'MSE test:' , error_var_test , 'R&#94;2 test:' , r_sq_test ) MSE train: 2845979.0374342627 R&#94;2 train: 0.29634842804618666 MSE test: 3357507.577228926 R&#94;2 test: 0.18898167147912737 In [ ]:","tags":"sections","url":"sections/notebook/section2-b/"},{"title":"S-Section 02: kNN and Linear Regression","text":"Jupyter Notebooks S-Section 2: kNN and Linear Regression","tags":"sections","url":"sections/section2/"},{"title":"Lecture 35: Interpreting Prediction Models","text":"CS109A Introduction to Data Science Lecture 35 (Interpreting Machine Learning Models) Harvard University Fall 2020 Instructors: Pavlos Protopapas, Kevin Rader, and Chris Tanner In [1]: import pandas as pd import sys import numpy as np import scipy as sp import sklearn as sk import matplotlib.pyplot as plt #from sklearn.linear_model import LogisticRegression #from sklearn.decomposition import PCA from sklearn import tree from sklearn import ensemble # Here are the decision trees from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier import tensorflow as tf print ( tf . __version__ ) # You should see a 2.0.0 here! # sns.set(style=\"ticks\") # %matplotlib inline 2.2.0 Part 1: Data Wrangling In [2]: heart_df = pd . read_csv ( 'data/Heart.csv' ) In [3]: print ( heart_df . shape ) heart_df . head () (303, 15) Out[3]: Unnamed: 0 Age Sex ChestPain RestBP Chol Fbs RestECG MaxHR ExAng Oldpeak Slope Ca Thal AHD 0 1 63 1 typical 145 233 1 2 150 0 2.3 3 0.0 fixed No 1 2 67 1 asymptomatic 160 286 0 2 108 1 1.5 2 3.0 normal Yes 2 3 67 1 asymptomatic 120 229 0 2 129 1 2.6 2 2.0 reversable Yes 3 4 37 1 nonanginal 130 250 0 0 187 0 3.5 3 0.0 normal No 4 5 41 0 nontypical 130 204 0 2 172 0 1.4 1 0.0 normal No In [4]: heart_df . describe () Out[4]: Unnamed: 0 Age Sex RestBP Chol Fbs RestECG MaxHR ExAng Oldpeak Slope Ca count 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 303.000000 299.000000 mean 152.000000 54.438944 0.679868 131.689769 246.693069 0.148515 0.990099 149.607261 0.326733 1.039604 1.600660 0.672241 std 87.612784 9.038662 0.467299 17.599748 51.776918 0.356198 0.994971 22.875003 0.469794 1.161075 0.616226 0.937438 min 1.000000 29.000000 0.000000 94.000000 126.000000 0.000000 0.000000 71.000000 0.000000 0.000000 1.000000 0.000000 25% 76.500000 48.000000 0.000000 120.000000 211.000000 0.000000 0.000000 133.500000 0.000000 0.000000 1.000000 0.000000 50% 152.000000 56.000000 1.000000 130.000000 241.000000 0.000000 1.000000 153.000000 0.000000 0.800000 2.000000 0.000000 75% 227.500000 61.000000 1.000000 140.000000 275.000000 0.000000 2.000000 166.000000 1.000000 1.600000 2.000000 1.000000 max 303.000000 77.000000 1.000000 200.000000 564.000000 1.000000 2.000000 202.000000 1.000000 6.200000 3.000000 3.000000 In [5]: X = heart_df [[ 'Age' , 'Sex' , 'ChestPain' , 'RestBP' , 'Chol' , 'Fbs' , 'RestECG' , 'MaxHR' , 'ExAng' , 'Oldpeak' , 'Slope' , 'Ca' , 'Thal' ]] y = 1 * ( heart_df [ 'AHD' ] == 'Yes' ) In [6]: #X['ChestPain']=X['ChestPain'].astype('category') #X['ChestPain']=X['ChestPain'].cat.codes #X['Thal']=X['Thal'].astype('category') #X['Thal']=X['Thal'].cat.codes In [7]: X = X . assign ( ChestPain = X [ 'ChestPain' ] . astype ( 'category' ) . cat . codes ) X = X . assign ( Thal = X [ 'Thal' ] . astype ( 'category' ) . cat . codes ) In [8]: X . describe () X [ 'Ca' ] = X [ 'Ca' ] . fillna ( 0 ) In [9]: from sklearn.model_selection import train_test_split itrain , itest = train_test_split ( range ( X . shape [ 0 ]), train_size = 0.80 ) X_train = X . iloc [ itrain , :] X_test = X . iloc [ itest , :] y_train = y . iloc [ itrain ] y_test = y . iloc [ itest ] Q1.1 : How were the categorical variables handled? How were missing values treated? Were these wise choices? *your answer here Part 2: Fitting Five ML Models In [10]: # fit a possibly underfit (depth = 3) decision tree classifier dt3 = tree . DecisionTreeClassifier ( max_depth = 3 ) dt3 . fit ( X_train , y_train ) # fit an overfit (depth = 10) decision tree classifier dt10 = tree . DecisionTreeClassifier ( max_depth = 10 ) dt10 . fit ( X_train , y_train ) Out[10]: DecisionTreeClassifier(max_depth=10) In [11]: # Evaluate using AUC print ( \"AUC on train for dt3:\" , sk . metrics . roc_auc_score ( y_train , dt3 . predict_proba ( X_train )[:, 1 ])) print ( \"AUC on test for dt3:\" , sk . metrics . roc_auc_score ( y_test , dt3 . predict_proba ( X_test )[:, 1 ])) print ( \"AUC on train for dt10:\" , sk . metrics . roc_auc_score ( y_train , dt10 . predict_proba ( X_train )[:, 1 ])) print ( \"AUC on test for dt10:\" , sk . metrics . roc_auc_score ( y_test , dt10 . predict_proba ( X_test )[:, 1 ])) AUC on train for dt3: 0.9026515151515151 AUC on test for dt3: 0.9008620689655171 AUC on train for dt10: 1.0 AUC on test for dt10: 0.818426724137931 In [12]: # fit random forest and adaboost models np . random . seed ( 109 ) randomforest = RandomForestClassifier ( n_estimators = 100 , max_features = 'sqrt' , max_depth = 10 ) randomforest . fit ( X_train , y_train ); adaboost = AdaBoostClassifier ( base_estimator = DecisionTreeClassifier ( max_depth = 4 ), n_estimators = 1000 , learning_rate =. 8 ) adaboost . fit ( X_train , y_train ); In [13]: # evaluate using AUC print ( \"AUC on train for randomforest:\" , sk . metrics . roc_auc_score ( --- , --- ) print ( \"AUC on test for randomforest:\" , sk . metrics . roc_auc_score ( --- , --- ) print ( \"AUC on train for adaboost:\" , sk . metrics . roc_auc_score ( --- , --- ) print ( \"AUC on test for adaboost:\" , sk . metrics . roc_auc_score ( --- , --- ) AUC on train for randomforest: 1.0 AUC on test for randomforest: 0.9617456896551724 AUC on train for adaboost: 1.0 AUC on test for adaboost: 0.9267241379310345 In [14]: # build a NN model tf . random . set_seed ( 109 ) NN_model = [] nodes_layers = [ 15 , 15 ] #reset the model NN_model = tf . keras . models . Sequential () # input layers NN_model . add ( tf . keras . layers . Dense ( nodes_layers [ 0 ], activation = 'tanh' , input_shape = ( X_train . shape [ 1 ],))) # hidden layers for s in nodes_layers [ 1 :]: print ( s ) NN_model . add ( tf . keras . layers . Dense ( units = s , activation = 'tanh' )) # output layer NN_model . add ( tf . keras . layers . Dense ( 1 , activation = 'sigmoid' )) # Summary NN_model . summary () 15 Model: \"sequential\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 15) 210 _________________________________________________________________ dense_1 (Dense) (None, 15) 240 _________________________________________________________________ dense_2 (Dense) (None, 1) 16 ================================================================= Total params: 466 Trainable params: 466 Non-trainable params: 0 _________________________________________________________________ In [15]: # compile it and run it # your code here X_test_std = (( X_test - X_train . mean ( axis = 0 )) / ( X_train . std ( axis = 0 ) + 0.2 )) X_train_std = (( X_train - X_train . mean ( axis = 0 )) / ( X_train . std ( axis = 0 ) + 0.2 )) batch_size = 32 epochs = 100 #opt = tf.keras.optimizers.SGD(lr=0.002,clipvalue=0.7) NN_model . compile ( optimizer = 'sgd' , loss = 'binary_crossentropy' , metrics = [ 'acc' ]) # fit it history_basic = NN_model . fit ( X_train_std , y_train , batch_size =--- , epochs =--- , validation_split =. 3 , verbose = False ) In [16]: print ( \"AUC on train for NN_model:\" , sk . metrics . roc_auc_score ( --- , --- ) print ( \"AUC on test for NN_model:\" , sk . metrics . roc_auc_score ( --- , --- ) AUC on train for NN_model: 0.9013774104683195 AUC on test for NN_model: 0.9461206896551724 Q2.1 : Which model performs best? Which models are overfit? How do you know? *your answer here Part 3: Variable Importance In [41]: #Default Variable Importance plt . figure ( figsize = ( 24 , 6 )) #plt.set_xticks() #plt.set_xticklabels(X.columns) num = 10 plt . subplot ( 1 , 4 , 1 ) dt3_importances = dt3 . feature_importances_ order = np . flip ( np . argsort ( dt3_importances ))[ 0 : num ] plt . barh ( range ( num ), dt3_importances [ order ], tick_label = X . columns [ order ]); plt . title ( \"Relative Variable Importance for dt4\" ) plt . subplot ( 1 , 4 , 2 ) dt10_importances = dt10 . feature_importances_ order = np . flip ( np . argsort ( dt10_importances ))[ 0 : num ] plt . barh ( range ( num ), dt10_importances [ order ], tick_label = X . columns [ order ]); plt . title ( \"Relative Variable Importance for dt10\" ) plt . subplot ( 1 , 4 , 3 ) rf_importances = --- order = --- plt . barh ( --- , --- ); plt . title ( \"Relative Variable Importance for rf\" ) plt . subplot ( 1 , 4 , 4 ) adaboost_importances = adaboost . feature_importances_ adaboost_importances = pd . Series ( adaboost_importances ) . fillna ( 0 ) order = np . flip ( np . argsort ( adaboost_importances ))[ 0 : num ] plt . barh ( range ( num ), adaboost_importances [ order ], tick_label = X . columns [ order ]); plt . title ( \"Relative Variable Importance for adaboost\" ); /Users/kevinrader/opt/anaconda3/lib/python3.7/site-packages/sklearn/tree/_classes.py:590: RuntimeWarning: invalid value encountered in true_divide return self.tree_.compute_feature_importances() Q3.1 : How do these variable importance measures compare for these 4 models? Which predictor is most important in general? How is it related to AHD ? *your answer here Part 4: Using Eli-5 In [18]: import eli5 /Users/kevinrader/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.scorer module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API. warnings.warn(message, FutureWarning) /Users/kevinrader/opt/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_selection.base module is deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API. warnings.warn(message, FutureWarning) In [57]: #permutation importance for the random forest from eli5.sklearn import PermutationImportance seed = 42 perm = PermutationImportance ( randomforest , random_state = seed , n_iter = 10 ) . fit ( X_test , y_test ) eli5 . show_weights ( perm , feature_names = X . columns . tolist ()) #eli5.explain_weights(perm, feature_names = X_train.columns.tolist()) Out[57]: Weight Feature 0.0918 ± 0.0626 Ca 0.0820 ± 0.0672 Thal 0.0574 ± 0.0643 ChestPain 0.0443 ± 0.0330 Sex 0.0393 ± 0.0334 MaxHR 0.0328 ± 0.0486 Age 0.0230 ± 0.0262 ExAng 0.0180 ± 0.0230 Oldpeak 0.0164 ± 0.0293 Slope 0.0148 ± 0.0177 Chol 0.0000 ± 0.0388 RestBP 0 ± 0.0000 RestECG 0 ± 0.0000 Fbs In [58]: #permutation importance for the NN model: need to define a scoring metric since there is no tf.score() by default def aucscore ( model , X , y ): y_pred = model . predict ( X ) return sk . metrics . roc_auc_score ( y , y_pred ) aucscore ( NN_model , X_train_std , y_train ) perm = PermutationImportance ( NN_model , random_state = seed , n_iter = 10 , scoring =--- ) . fit ( --- , --- ) eli5 . show_weights ( --- , --- ) Out[58]: Weight Feature 0.1529 ± 0.0508 MaxHR 0.0243 ± 0.0272 RestBP 0.0229 ± 0.0329 Age 0.0170 ± 0.0258 Chol 0.0138 ± 0.0055 ChestPain 0.0045 ± 0.0032 RestECG 0.0045 ± 0.0028 Ca 0.0014 ± 0.0020 ExAng 0.0013 ± 0.0019 Thal 0.0012 ± 0.0033 Slope 0.0003 ± 0.0008 Fbs 0.0002 ± 0.0017 Sex -0.0016 ± 0.0019 Oldpeak Q4.1 : How do the permutation importance measures compare to the default variable importance in the random forest? How does the NN model compare to the random forest? *your answer here Part 5: Plotting Predictions In [59]: yhat_rf_train = randomforest . predict_proba ( X_train )[:, 1 ] plt . scatter ( X_train [[ 'Age' ]], yhat_rf_train ); yhat_rf_test = randomforest . predict_proba ( X_test )[:, 1 ] plt . scatter ( X_test [[ 'Age' ]], yhat_rf_test , marker = 'x' ); plt . title ( \"Predicted Probabilities vs. Age from the RF in train and test\" ); In [65]: yhat_nn_train = NN_model . predict ( --- ) plt . scatter ( --- , --- ); yhat_nn_test = NN_model . predict ( --- ) plt . scatter ( --- , --- ); plt . title ( \"Predicted Probabilities vs. Age from NN in train and test\" ); Q5.1 How do the random forest and NN model compare in the interpretation of Age with AHD? Which is more reliable? *your answer here In [27]: # Create the data frame of means to do the prediction means1 = X_train . mean ( axis = 0 ) means_df = ( means1 . to_frame ()) . transpose () # Do the prediction at all observed ages Ages = np . arange ( np . min ( X [ 'Age' ]), np . max ( X [ 'Age' ])) means_df = pd . concat ([ means_df ] * Ages . size , ignore_index = True ) means_df [ 'Age' ] = Ages In [37]: #plots at means yhat_nn = NN_model . predict ( means_df ) plt . scatter ( X_train [ 'Age' ], y_train ) plt . plot ( means_df [ 'Age' ], yhat_nn , color = \"red\" ) plt . title ( \"Predicted Probabilities vs. Age from NN in train\" ); In [38]: #Plots for all observations. And then averaged yhat_nns = [] for i in range ( 0 , X_train . shape [ 0 ]): obs = X_train . iloc [ i ,:] . to_frame () . transpose () obs_df = pd . concat ([ obs ] * Ages . size , ignore_index = True ) obs_df [ 'Age' ] = Ages yhat_nn = NN_model . predict_proba ( obs_df ) yhat_nns . append ( yhat_nn . transpose ()) plt . plot ( obs_df [ 'Age' ], yhat_nn , color = 'blue' , alpha = 0.05 ) plt . plot ( obs_df [ 'Age' ], np . mean ( yhat_nns , axis = 0 )[ 0 ], color = 'red' , linewidth = 2 ); plt . ylim ( 0 , 1 ) plt . title ( \"Predicted Probabilities vs. Age from NN in train for all observations\" ); Q5.1 Interpret the two plots above. What is the difference in the interpretations? Is there anyu evidence of interaction effects between Age and the other predictors? How do you know? *your answer here Part 6: Using LIME In [31]: # pip install lime import lime In [42]: from lime.lime_tabular import LimeTabularExplainer #explainer = LimeTabularExplainer(X_train)#class_names = [0,1]) explainer = LimeTabularExplainer ( X_train . values , feature_names = X_train . columns , class_names = [ 0 , 1 ], mode = 'classification' ) In [46]: idx = 42 exp = explainer . explain_instance ( X_train . values [ idx ], randomforest . predict_proba , num_features = 13 ) #X_train.values[idx].size) print ( 'Observation #: %d ' % idx ) print ( 'Probability(AHD) =' , randomforest . predict_proba ( X_train )[ idx ][ 1 ]) print ( 'True class: %s ' % y_train [ idx ]) Observation #: 42 Probability(AHD) = 0.32 True class: 0 In [48]: ### Plot the results # exp.as_list() exp . as_pyplot_figure (); In [0]: # change the observation number and see what changes. idx = --- exp = explainer . explain_instance ( X_train . values [ idx ], randomforest . predict_proba , num_features = 13 ) print ( 'Observation #: %d ' % idx ) print ( 'Probability(AHD) =' , randomforest . predict_proba ( X_train )[ idx ][ 1 ]) print ( 'True class: %s ' % y_train [ idx ]) In [0]: ### Plot the results # exp.as_list() exp . as_pyplot_figure (); Q6.1 Interpret the LIME results above. Do they agree with the other interpretations for the random forest model seen so far? your answer here","tags":"labs","url":"labs/lecture-35/notebook/"},{"title":"Lecture 6: Linear Regression","text":"In [10]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the dataset In [11]: # Data set used in this exercise :Advertising.csv data_filename = 'Advertising.csv' # Read the data using pandas libraries df = pd . read_csv ( data_filename ) In [3]: # Create a new dataframe called `df_new` witch the columns 'TV' and 'sales' df_new = df [[ 'TV' , 'sales' ]] In [4]: # Plot the data plt . plot ( df_new . TV , df_new . sales , '*' , label = 'data' ) plt . xlabel ( 'TV' ) plt . ylabel ( 'Sales' ) plt . legend () Out[4]: Beta Estimation In [0]: ### edTest(test_betas) ### # Estimate beta0 by observing the value of y when x = 0 beta0 = ___ # Estimate beta1 - Check the slope for guidance beta1 = ___ In [0]: # Calculate prediction of x using beta0 and beta1 y_predict = ___ Plotting the graph In [0]: # Plot the predicted values as well as the data plt . plot ( df_new . TV , df_new . sales , '*' , label = 'data' ) plt . plot ( df_new . TV , y_predict , label = 'model' ) plt . xlabel ( 'TV' ) plt . ylabel ( 'Sales' ) plt . legend () MSE Computation In [0]: ### edTest(test_mse) ### # Calculate the MSE MSE = ___ # Print the results print ( \"My MSE is: {0} \" . format ( MSE ))","tags":"labs","url":"labs/lecture-6/notebook/"},{"title":"Lecture 6: Linear Regression","text":"In [13]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the dataset In [14]: # Data set used in this exercise data_filename = 'Advertising.csv' # Read data file using pandas libraries df = pd . read_csv ( data_filename ) In [16]: # Take a quick look at the data df . head () Out[16]: TV Radio Newspaper sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 In [17]: # Create a new dataframe called `df_new`. witch the columns ['TV' and 'sales']. df_new = df [[ 'TV' , 'sales' ]] Beta and MSE Computation In [6]: # Set beta0 beta0 = 2.2 In [12]: # Create lists to store the MSE and beta1 mse_list = ___ beta1_list = ___ In [0]: ### edTest(test_beta) ### # This loops runs from -2 to 3.0 with an increment of 0.1 i.e a total of 51 steps for beta1 in ___ : # Calculate prediction of x using beta0 and beta1 y_predict = ___ # Calculate Mean Squared Error mean_squared_error = ___ # Append the new MSE in the list that you created above mse_list . ___ # Also append beta1 values in the list beta1_list . ___ Plotting the graph In [0]: ### edTest(test_mse) ### # Plot MSE as a function of beta1 plt . plot ( beta1_list , mse_list ) plt . xlabel ( 'Beta1' ) plt . ylabel ( 'MSE' ) Go back and change your $\\beta_0$ value and report your new optimal $\\beta_1$ value and new lowest $MSE$ Is the MSE lower than before, or more? In [0]: # Your answer here","tags":"labs","url":"labs/lecture-6/notebook-2/"},{"title":"Lecture 6: Linear Regression","text":"In [2]: # import required libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.metrics import r2_score % matplotlib inline In [3]: # Read the 'Advertising.csv' dataset data_filename = 'Advertising.csv' # Read data file using pandas libraries df = pd . read_csv ( data_filename ) In [4]: # Take a quick look at the data df . head () Out[4]: TV Radio Newspaper sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 In [6]: # Assign TV advertising as predictor variable 'x' and sales as response variable 'y' x = df [[ \"TV\" ]] y = df [ \"sales\" ] In [13]: # divide the data into training and validation sets x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = 0.8 ) In [14]: # Use the sklearn function 'LinearRegression' to fit on the training set model = LinearRegression () model . fit ( ___ , ___ ) # Now predict on the test set y_pred_test = model . predict ( ___ ) In [15]: ### edTest(test_mse) ### # Now compute the MSE with the predicted values and print it mse = mean_squared_error ( ___ , ___ ) print ( f 'The test MSE is { ___ } ' ) The test MSE is 10.1024550349862 In [16]: # Make a plot of the data along with the predicted linear regression fig , ax = plt . subplots () ax . scatter ( x , y , label = 'data points' ) ax . plot ( ___ , ___ , color = 'red' , linewidth = 2 , label = 'model predictions' ) ax . set_xlabel ( 'Advertising' ) ax . set_ylabel ( 'Sales' ) ax . legend () Out[16]: Mindchow Rerun the code but this time change the training size to 60%. Did your test $MSE$ improve or get worse? In [17]: # your answer here In [0]:","tags":"labs","url":"labs/lecture-6/notebook-3/"},{"title":"Lecture 6: Linear Regression","text":"In [3]: #import necessary libraries import pandas as pd import matplotlib.pyplot as plt import numpy as np from sklearn.linear_model import LinearRegression from sklearn.metrics import mean_squared_error from sklearn.model_selection import train_test_split from sklearn import preprocessing from sklearn.metrics import mean_squared_error from prettytable import PrettyTable Reading the dataset In [5]: #Read the file \"Advertising.csv\" df = pd . read_csv ( \"Advertising.csv\" ) In [6]: #Take a quick look at the data to list all the predictors df . head () Out[6]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 Create different multi predictor models In [18]: ### edTest(test_mse) ### #List to store the MSE values mse_list = [] #List of all predictor combinations to fit the curve cols = [[ 'TV' ],[ 'Radio' ],[ 'Newspaper' ],[ 'TV' , 'Radio' ],[ 'TV' , 'Newspaper' ],[ 'Radio' , 'Newspaper' ],[ 'TV' , 'Radio' , 'Newspaper' ]] for i in cols : #Set each of the predictors from the previous list as x x = df [ ___ ] #\"Sales\" column is the reponse variable y = df [ ___ ] #Splitting the data into train-test sets with 80% training data and 20% testing data. #Set random_state as 0 xtrain , xtest , ytrain , ytest = train_test_split ( ___ ) #Create a LinearRegression object and fit the model lreg = LinearRegression () lreg . fit ( ___ ) #Predict the response variable for the test set y_pred = lreg . predict ( ___ ) #Compute the MSE MSE = mean_squared_error ( ___ ) #Append the MSE to the list mse_list . append ( ___ ) Display the MSE with predictor combinations In [20]: t = PrettyTable ([ 'Predictors' , 'MSE' ]) #Loop to display the predictor combinations along with the MSE value of the corresponding model for i in range ( len ( mse_list )): t . add_row ([ cols [ i ], mse_list [ i ]]) print ( t ) Comment on the trend of MSE values with changing predictor(s) combinations. Your answer here","tags":"labs","url":"labs/lecture-6/notebook-4/"},{"title":"Lecture 6: Linear Regression","text":"Slides Linear Regression [PDF] Multi & Poly Regression Regression [PDF] Exercises Lecture 6: A.1 - Guesstimate the β values [Notebook] Lecture 6: A.2 - MSE for varying β1 values [Notebook] Lecture 6: A.3 - Linear Regression using sklearn [Notebook] Lecture 6: B.1 - Simple Multi-linear Regression [Notebook]","tags":"lectures","url":"lectures/lecture06/"},{"title":"Lecture 5: kNN & Linear Regression","text":"Title Exercise: A.1 - Plotting the data Description The aim of this exercise is to plot TV Ads vs Sales based on the Advertisement dataset which should look similar to the graph given below. Instructions: Read the Advertisement data and view the top rows of the dataframe to get an understanding of the data and columns Select the first 7 observations and the columns TV and sales . Create a scatter plot TV budget vs sales like in the lecture. Hints: pd.read_csv(filename) : Returns a pandas dataframe containing the data and labels from the file data. df.iloc[] : Returns a subset of the dataframe that is contained in the row range passed as the argument. np.linspace() : Returns evenly spaced numbers over a specified interval. df.head() : Returns the first 5 rows of the dataframe with the column names plt.scatter() : A scatter plot of y vs. x with varying marker size and/or colour plt.xlabel() : This is used to specify the text to be displayed as the label for the x-axis Note: This exercise is auto-graded and you can try multiple attempts. In [1]: import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Reading the Advertisement dataset In [ ]: # Data set used in this exercise data_filename = 'Advertising.csv' # Read advertising.csv file using the pandas library df = pd . read_csv ( ___ ) In [ ]: # Get a quick look of the data and columns df . ___ In [ ]: ### edTest(test_pandas) ### # Select the first 7 rows df_new = df . ___ In [ ]: # Print your new dataframe to see if you have selected 7 rows correctly print ( ___ ) Plotting the graph In [ ]: # Use a scatter plot for TV vs Sales plt . ___ # Add axis labels for clarity (x : TV budget, y : Sales) plt . ___ plt . ___ Post-exercise question Instead of just plotting seven points, experiment to plot all points In [ ]:","tags":"labs","url":"labs/lecture-5/notebook/"},{"title":"Lecture 5: kNN & Linear Regression","text":"Title Exercise: A.2 - Simple kNN regression Description The goal of this exercise is to re-create the plots below from the lecture. Instructions: Part 1 KNN by hand for k=1 Read the Advertisement data Get a subset of the data from row 5 to row 13 Apply the kNN algorithm by hand and plot the first graph as given above. Part 2 Using sklearn package Read the entire Advertisement dataset Split the data into train and test sets using train_test_split() function Select k_list as possible k values ranging from 1 to 70. For each value of k in k_list : Use sklearn KNearestNeighbors() to fit train data Predict on the test data Use the helper code to get the second plot above for k=1,10,70 Hints: np.argsort() : Returns the indices that would sort an array. df.iloc[] : Returns a subset of the dataframe that is contained in the column range passed as the argument df.values : Returns a Numpy representation of the DataFrame. pd.idxmin() : Returns index of the first occurrence of minimum over requested axis. np.min() : Returns the minimum along a given axis. np.max() : Returns the maximum along a given axis. np.zeros() : Returns a new array of given shape and type, filled with zeros. train_test_split(X,y) : Split arrays or matrices into random train and test subsets. np.linspace() : Returns evenly spaced numbers over a specified interval. KNeighborsRegressor(n_neighbors=k_value) : Regression-based on k-nearest neighbors. Note: This exercise is auto-graded and you can try multiple attempts. In [23]: # import required libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline In [24]: # Read data filename = 'Advertising.csv' df_adv = pd . read_csv ( filename ) In [25]: # take a quick look of the dataset df_adv . head ( 6 ) Out[25]: TV Radio Newspaper Sales 0 230.1 37.8 69.2 22.1 1 44.5 39.3 45.1 10.4 2 17.2 45.9 69.3 9.3 3 151.5 41.3 58.5 18.5 4 180.8 10.8 58.4 12.9 5 8.7 48.9 75.0 7.2 Part 1: KNN by hand for $k=1$ In [27]: # Get a subset of the data rows 6 to 13 and only TV advertisement. # The first row in the dataframe is the first row and not the zeroth row. data_x = df_adv . TV . iloc [ 5 : 13 ] data_y = df_adv . Sales . iloc [ 5 : 13 ] # Sort the data idx = np . argsort ( ___ ) . values # Get indices ordered from lowest to highest values # Get the actual data in the order from above and turn them into numpy arrays. data_x = data_x . iloc [ ___ ] . ___ data_y = data_y . iloc [ ___ ] . ___ File \" \" , line 4 data_y = df_adv.Sales.iloc[] &#94; SyntaxError : invalid syntax In [26]: ### edTest(test_findnearest) ### # Define a function that finds the index of the nearest neighbor # and returns the value of the nearest neighbor. Note that this # is just for k = 1 and the distance function is simply the # absolute value. def find_nearest ( array , value ): idx = pd . Series ( ___ ) . ___ # hint: use pd.idxmin() return idx , array [ idx ] In [27]: # Create some artificial x-values (might not be in the actual dataset) x = np . linspace ( np . min ( data_x ), np . max ( data_x )) # Initialize the y-values to zero y = np . zeros ( ( len ( x ))) In [28]: # Apply the KNN algorithm. Try to predict the y-value at a given x-value # Note: You may have tried to use the `range' method in your code. Enumerate # is far better in this case. # Try to understand why. for i , xi in enumerate ( x ): y [ i ] = data_y [ find_nearest ( data_x , xi )[ 0 ]] Plotting the data In [29]: # Plot your solution plt . plot ( x , y , '-.' ) # Plot the original data using black x's. plt . plot ( ___ , ___ , 'kx' ) plt . title ( '' ) plt . xlabel ( 'TV budget in $1000' ) plt . ylabel ( 'Sales in $1000' ) Part 2: KNN for $k\\ge1$ using sklearn In [2]: # import train_test_split and KNeighborsRegressor from sklearn from sklearn.model_selection import train_test_split from sklearn.neighbors import KNeighborsRegressor In [34]: ### Reading the complete Advertising dataset # This time you are expected to read the entire dataset data_filename = 'Advertising.csv' # Read advertising.csv file using the pandas library (using pandas.read_csv) df = pd . read_csv ( data_filename ) # Choose sales as your response variable 'y' and 'TV' as your 'predictor variable' x = df [[ ___ ]] y = df [ ___ ] In [35]: ### edTest(test_shape) ### # Split the dataset in training and testing with 60% training set and 40% testing set # with random state = 42 x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = ___ ) In [36]: ### edTest(test_nums) ### # Choosing k_value_min = ___ k_value_max = ___ # creating list of integer k values betwwen k_value_min and k_value_max using linspace k_list = np . linspace ( k_value_min , k_value_max , 70 ) In [28]: fig , ax = plt . subplots ( figsize = ( 10 , 6 )) j = 0 # Looping over k values for k_value in k_list : # creating KNN Regression model model = KNeighborsRegressor ( n_neighbors = int ( ___ )) # fitting model model . fit ( ___ , ___ ) # test predictions y_pred = model . predict ( ___ ) ## Plotting colors = [ 'grey' , 'r' , 'b' ] if k_value in [ 1 , 10 , 70 ]: xvals = np . linspace ( x . min (), x . max (), 100 ) ypreds = model . predict ( xvals ) ax . plot ( xvals , ypreds , '-' , label = f 'k = { int ( k_value ) } ' , linewidth = j + 2 , color = colors [ j ]) j += 1 ax . legend ( loc = 'lower right' , fontsize = 20 ) ax . plot ( x_train , y_train , 'x' , label = 'test' , color = 'k' ) ax . set_xlabel ( 'TV budget in $1000' , fontsize = 20 ) ax . set_ylabel ( 'Sales in $1000' , fontsize = 20 ) plt . tight_layout () --------------------------------------------------------------------------- NameError Traceback (most recent call last) in 2 j = 0 3 # Looping over k values ----> 4 for k_value in k_list : 5 6 # creating KNN Regression model NameError : name 'k_list' is not defined In the plotting code above, re-run ax.plot(x_train, y_train,'x',label='test',color='k') but this time with x_test and y_test . According to you works , which k value is the best. Why? Your answer here In [ ]:","tags":"labs","url":"labs/lecture-5/notebook-2/"},{"title":"Lecture 5: kNN & Linear Regression","text":"Title Exercise: B.1 - Finding best k in kNN regression Description The goal here is to find the value of k of the best performing model based on the test MSE. Instructions: Read the data into a dataframe object using pandas.read_csv Select the sales column as the response variable and TV budget column as the predictor variable Make a train-test split using sklearn.model_selection.train_test_split Create a list of integer k values using numpy.linspace For each value of k Fit a knn regression on train set Calculate MSE on test set and store it Plot the test MSE values for each k Find the k value associated with the lowest test MSE Hints: train_test_split(X,y) : Split arrays or matrices into random train and test subsets. np.linspace() : Returns evenly spaced numbers over a specified interval. KNeighborsRegressor(n_neighbors=k_value) : Regression-based on k-nearest neighbors. mean_squared_error() : Computes the mean squared error regression loss. dict.keys() : returns a view object that displays a list of all the keys in the dictionary. dict.values() : returns a list of all the values available in a given dictionary. dict.items() : returns a list of dict's (key, value) tuple pairs Note: This exercise is auto-graded and you can try multiple attempts. In [1]: import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.neighbors import KNeighborsRegressor from sklearn.metrics import mean_squared_error import matplotlib.pyplot as plt from sklearn.metrics import r2_score % matplotlib inline Reading the standard Advertising dataset In [ ]: # Data set used in this exercise data_filename = 'Advertising.csv' # Read advertising.csv file using the pandas library (using pandas.read_csv) df = pd . read_csv ( data_filename ) In [ ]: # again, take a quick look at the data df . head () In [3]: # Select the 'TV' column as predictor variable and 'Sales' column as response variable x = df [[ ___ ]] y = df [ ___ ] Train-Test split In [21]: ### edTest(test_shape) ### # Split the dataset in training and testing with 60% training set and 40% testing set x_train , x_test , y_train , y_test = train_test_split ( ___ , ___ , train_size = ___ , random_state = 66 ) In [28]: ### edTest(test_nums) ### # Choosing k range from 1 to 70 k_value_min = 1 k_value_max = 70 # creating list of integer k values betwwen k_value_min and k_value_max using linspace k_list = np . linspace ( k_value_min , k_value_max , num = 70 , dtype = int ) Model fit In [29]: # hint mean_squared_error In [ ]: fig , ax = plt . subplots ( figsize = ( 10 , 6 )) # creating a dictionary for storing k value against MSE fit {k: MSE@k} knn_dict = {} j = 0 # Looping over k values for k_value in k_list : # creating KNN Regression model model = KNeighborsRegressor ( n_neighbors = int ( ___ )) # fitting model model . fit ( x_train , y_train ) # predictions y_pred = model . predict ( ___ ) # Calculating MSE MSE = ____ #Storing the MSE values of each k value in a dictionary knn_dict [ k_value ] = ___ ## Plotting colors = [ 'grey' , 'r' , 'b' ] if k_value in [ 1 , 10 , 70 ]: xvals = np . linspace ( x . min (), x . max (), 100 ) ypreds = model . predict ( xvals ) ax . plot ( xvals , ypreds , '-' , label = f 'k = { int ( k_value ) } ' , linewidth = j + 2 , color = colors [ j ]) j += 1 ax . legend ( loc = 'lower right' , fontsize = 20 ) ax . plot ( x_train , y_train , 'x' , label = 'test' , color = 'k' ) ax . set_xlabel ( 'TV budget in $1000' , fontsize = 20 ) ax . set_ylabel ( 'Sales in $1000' , fontsize = 20 ) plt . tight_layout () Graph plot In [ ]: # Plot k against MSE plt . figure ( figsize = ( 8 , 6 )) plt . plot ( ___ , ___ , 'k.-' , alpha = 0.5 , linewidth = 2 ) plt . xlabel ( 'k' , fontsize = 20 ) plt . ylabel ( 'MSE' , fontsize = 20 ) plt . title ( 'Test $MSE$ values for different k values - KNN regression' , fontsize = 20 ) plt . tight_layout () Find the best knn model In [ ]: ### edTest(test_mse) ### # Looking for k with minimum MSE min_mse = min ( ___ ) best_model = ___ # HINT YOU MAY USE LIST COMPREHENSION print ( \"The best k value is \" , best_model , \"with a MSE of \" , min_mse ) How good is your model? From the options below, how would you classify your model? Good Satisfactory Bad In [ ]: # Your answer here In [ ]: # Run this cell to calculate the R2_score of your best model model = KNeighborsRegressor ( n_neighbors = best_model [ 0 ]) model . fit ( x_train , y_train ) y_pred_test = model . predict ( x_test ) print ( f \"The R2 score for your model is { r2_score ( y_test , y_pred_test ) } \" ) After observing the $R&#94;2$ value, how would you now classify your model? Good Satisfactory Bad In [1]: # your answer here","tags":"labs","url":"labs/lecture-5/notebook-3/"},{"title":"Lecture 5: kNN & Linear Regression","text":"Slides kNN Regression (Part A) Slides [PDF] kNN Regression (Part A) Slides [PPTX] kNN Regression (Part B) Slides [PDF] kNN Regression (Part B) Slides [PPTX] Linear Regression (Part C) Slides [PDF] Linear Regression (Part C) Slides [PPTX] Exercises Lecture 5: A.1 - Plotting the data [Notebook] Lecture 5: A.2 - Simple kNN regression [Notebook] Lecture 5: B.1 - Finding best k in kNN regression [Notebook]","tags":"lectures","url":"lectures/lecture05/"},{"title":"Lecture 4: EDA + PANDAS","text":"CS109A Introduction to Data Science Detailed Examples: Data Collection, Parsing, and Quick Analyses Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Title Extra Practice + Solutions! Description This exercise will not be graded; in fact, it's not even submittable, and it's definitely not mandatory to work on it. But, if you would like extra practice , we crafted this notebook which is very similar in nature to the homework. More importantly, it's very realistic to real-world scenarios whereby one would explore and analyze data -- before modelling is involved. We have not included an auto-grader, so you cannot test your solutions. However, we provide the solutions, so you can manually check if your outputs are on par with ours. The solutions are visible from the tab up top (right-side) in this window. In [ ]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2020-CS109A/master/themes/static/css/cs109.css\" ) . text HTML ( styles ) Overview In this notebook, your goal is to gain further practice with acquiring, parsing, cleaning, and analyzing data. Since real-world problems often require gathering information from a variety of sources, including the Internet, web scraping is a highly useful skill to have. To do this, we will scrape IMDb data on the highest-paid actors and actresses, extracting various key data points and using PANDAS to learn how to aggregate the data in useful ways. Learning Objectives Get started using Jupyter Notebooks, which are incredibly popular, powerful, and will be our medium of programming for the duration of CS109A and CS109B. Become familiar with how to scrape and use data from online sources. Gain experience with data exploration and simple analysis. Become comfortable with PANDAS as a means of storing and working with data. Feel well-prepared to complete HW1. Notes Exercise responsible scraping . Web servers can become slow or unresponsive if they receive too many requests from the same source in a short amount of time. In your code, use a delay of 2 seconds between requests. This helps to not get blocked by the target website -- imagine how frustrating it would be to have this occur. Section 1 of this homework involves saving the scraped web pages to your local machine. Web scraping requests can take several minutes. Depending on one's project, it could even take hours, days, or last indefinitely (Google crawling the entire Web). As you run a Jupyter Notebook, it maintains a running state of memory. Thus, the order in which you run cells matters and plays a crucial role; it can be easy to make mistakes based on when you run different cells as you develop and test your code. Before submitting every Jupyter Notebook homework assignment, be sure to restart your Jupyter Notebook and run the entire notebook from scratch, all at once (i.e., \"Kernel -> Restart & Run All\") In [ ]: # import the necessary libraries import re import requests import pandas as pd import numpy as np from time import sleep from bs4 import BeautifulSoup Table of Contents Practice with regex Obtaining IMDb Data Fetching website data via requests BeautifulSoup Obtain actor url + salary Scrape rest of data Loading and Exploring Data Saving & Loading Data with Pandas Cleaning data (rename columns + change types) Slicing & sorting data Calculating summary statistics (min, max, mean, etc) pd.cut , df.groupby , and bar plots Exploring age vs salary Exploring salary vs sex Exploring awards vs sex Exploring awards vs sex part II Exploring composer credits 0. Practice with regex Being able to scrape, parse, and analyze simple website data is very useful in a variety of settings. Here, we look at a U.S. Senate vote on confirming a nominee to be a U.S. District Judge: https://www.senate.gov/legislative/LIS/roll_call_lists/roll_call_vote_cfm.cfm?congress=116&session=2&vote=00157 We provide the scraping. Your task is: Write the BeautifulSoup to grab the ‘vote by positon' section for both \"Yea\" and \"Nay\". Write a regex to extract each senator's name for the Yeas and Nays. In [ ]: url = \"https://www.senate.gov/legislative/LIS/roll_call_lists/roll_call_vote_cfm.cfm?congress=116&session=2&vote=00157\" s = requests . Session () page = s . get ( url ) In [ ]: page In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Explanation of regex: '\\)(.*?)\\s\\(' I noticed the names were listed between parentheses i.e. ...)Barrasso (R... . So I decided to match for the text in between parentheses with a space before the open parentheses, i.e. )abc ( . The regex searches returns a list of all matches to the following condition: match any string of any length that comes after \")\" but before \" (\". 1. Obtaining IMDb Data Here, we are interested in analyzing several data points for famous actors and actresses on IMDb. IMDb provides relevant data that includes the names, sexes, and various awards of actors and actresses. Visit https://www.imdb.com/list/ls026028927/ to find a list of the highest-paid actors and actresses. Each actor In this exercise, we will focus on automating the downloading of each actor's data (via Requests ). First, as we will do for every Jupyter Notebook, let's import necessary packages that we will use throughout the notebook (i.e., run the cell below). In [ ]: # we define this for convenience, as every actor's url begins with this prefix base_url = 'https://www.imdb.com' extension = '/list/ls026028927/' Here, we fetch the webpage and construct a BeautifulSoup object (HTML parser) from it. In [ ]: actors_page = requests . get ( base_url + extension ) bs_page = BeautifulSoup ( actors_page . content , \"html.parser\" ) bs_page Exercise 1.1: In the cell below, write code that uses the BeautifulSoup object to parse through the home page in order to extract the **link and salary** for every actor. Specifically, populate the `info` dictionary by setting each key to be the actor name and the value to be **a dictionary of data**, with keys `url` and `salary`. When complete, there will be 30 keys to the `info` dictionary. As an example, within `info`, one of your pairs should be: ``'Adam Sandler': {'salary': '$50.5 million', 'url': 'https://www.imdb.com/name/nm0001191'}`` **HINT:** There are _many_ solutions, but you may find it easiest to use Regular Expression(s) In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Exercise 1.2: In the cell below, write code that uses the BeautifulSoup object to parse through each actor's url to extract these additional variables and add them to `info`: - birth date - sex - height - producer credits - actor credits - soundtrack credits - composer credits - award wins - award nominations Save this as a Pandas dataframe called `df` where each row represents a specific actor. Check `df.shape`: you should have 30 rows and 11 columns. **HINT:** To get award wins and nominations, you can append `'/awards'` to your url and request that link. Remember to scrape responsibly! **HINT:** Actor credits are listed under \"Actor\" for males and \"Actress\" for females. Make sure you scrape that correctly. In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE 2. Loading and Exploring Data Now, let's actually use the data! Here, we ask you to perform a few operations using PANDAS on our new dataset. Exercise 2.1: Saving & Loading Data Save the dataframe as a csv, and then load it back again, saving it under a different name. Compare the number of columns of your new dataframe to `df` above. If it is different, why? In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE The newly loaded dataframe turned the index of df into a new Unnamed column. Exercise 2.2: Cleaning Data First, to clean up the data frame, we will rename column headers that contain a space, i.e. changing \"actor credits\" to \"actor_credits\". This is standard practice and will make references to column names easier in the future. Also, change all numerical columns to be of type `\"int\"` or `\"float\"`, whichever is more appropriate, by using the `astype` function. In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Exercise 2.3: Slicing & Sorting Data Use `df` for all future exercises. Create a subset of `df` of female actresses with more than 50 wins. Sort it by actor credits, then producer credits. So, if there is a tie in actor credits, the person with more producer credits should come 1st. Sort in decreasing order (most actor credits goes first). In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Exercise 2.4: Data Summary Statistics Calculate summary statistics for the actors' salaries and total credits (actor + producer + composer + soundtrack). In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Exercise 2.5: Group By Use the pandas `cut` function to group the actors by total credits into bins of 50 (i.e. 1-50, 51-100, etc.). Then use the `groupby` function to find the mean number of nominations for each bin and report it in a plot. In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Exercise 2.6: Exploring Age vs Salary One interesting question is looking at age vs salary : Is there a peak age that acquires the highest salary?; Are older actors paid less? In order to answer these questions, we must first extract age from the birthdate. We can do this by converting the birthdate to a datetime object. More info on datetime objects: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html . In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Now we explore the age statistics. In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE In [ ]: # find the actor/actress associated with oldest age # YOUR CODE HERE # END OF YOUR CODE HERE Observe the results. What do you notice about the two youngest actors/actresses? What do you notice about the oldest? The youngest are both female, and have significantly less credits than the oldest, who happens to be male. But notice that Jennifer Lawrence does have more wins than Emma Watson and Samuel Jackson. Do you think that it makes sense that more wins/awards correlates with a higher salary? Let's look a little further in depth in the age range by splitting up age by quartiles according to the data. We can do this with pandas' built in .describe() function. In [ ]: df [ \"age\" ] . describe () We the bin age groups based on these quartile summary statistics. In [ ]: quartile_1 = df [ df . age <= 41 ] quartile_2 = df [( df . age > 41 ) & ( df . age <= 49 )] quartile_3 = df [( df . age > 49 ) & ( df . age <= 53 )] quartile_4 = df [ df . age > 53 ] In [ ]: # look at mean salary within each age quartile # YOUR CODE HERE # END OF YOUR CODE HERE We can see that quartile 2 (ages 42 - 49) has higher average salary than quartile 3 (ages 50 - 53), but quartile 4 still has highest average salary (ages 54 - 71). Why might this be? Exercise 2.7: Exploring Salary vs sex Now, lets look at salary vs sex. This could be useful to look into any possible gender bias. In [ ]: # salary vs sex # look at mean salary among male and females # YOUR CODE HERE # END OF YOUR CODE HERE In [ ]: # look at min and max salary among male and females # YOUR CODE HERE # END OF YOUR CODE HERE What do you notice from these statistics? It looks like females have a lower salary in all three summary statistics. Why do you think this is? Do you think there are other factors that could be affecting this? If so, what else in the data could be indicative? It could also be helpful to additionally look the average total number of credits per gender. In [ ]: # look at mean salary among male and females # YOUR CODE HERE # END OF YOUR CODE HERE In [ ]: # look at min and max salary among male and females # YOUR CODE HERE # END OF YOUR CODE HERE Notice that although females have lower average salary than males, they also tend to have fewer credits. Does this tell us something about how the number of credits correlates with salary? How could you explore this further? Exercise 2.8: Exploring awards by sex Which actress has the most wins? How many wins does she have? Which actor has the most wins? How many wins does he have? In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Exercise 2.9: Exploring awards by sex, Part II What is the mean number of wins for an actor/actress with less than 65 actor credits? With greater than or equal to 65 actor credits? In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE Exercise 2.10: Exploring composer credits How many actors/actresses have at least 1 composer credit? In [ ]: # YOUR CODE HERE # END OF YOUR CODE HERE","tags":"labs","url":"labs/lecture-4/notebook/"},{"title":"Lecture 4: EDA + PANDAS","text":"CS109A Introduction to Data Science Lecture 4, Exercise 1: EDA with PANDAS Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Title Exercise 1: EDA Description In this exercise, we get hands-on practice with EDA. In [1]: import pandas as pd NOTE : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. EDA with PANDAS top50.csv is a dataset found online (Kaggle.com) that contains information about the 50 most popular songs on Spotify in 2019. NOTE: This music dataset is used purely for illustrative, educational purposes. The data, including song titles, may include explicit language. Harvard, including myself and the rest of the CS109 staff, does not endorse any of the entailed contents or the songs themselves, and we apologize if it is offensive to anyone in anyway. Each row represents a distinct song. The columns (in order) are: ID: a unique ID (i.e., 1-50) TrackName: Name of the Track ArtistName: Name of the Artist Genre: the genre of the track BeatsPerMinute: The tempo of the song. Energy: The energy of a song - the higher the value, the more energetic. song Danceability: The higher the value, the easier it is to dance to this song. Loudness: The higher the value, the louder the song. Liveness: The higher the value, the more likely the song is a live recording. Valence: The higher the value, the more positive mood for the song. Length: The duration of the song (in seconds). Acousticness: The higher the value, the more acoustic the song is. Speechiness: The higher the value, the more spoken words the song contains. Popularity: The higher the value, the more popular the song is. Below, fill in the blank to create a new PANDAS DataFrame from the top50.csv file. In [36]: ### edTest(test_a) ### df = _____ Run .describe() to display summary statistics. In [31]: df . describe () Q1: Do you notice any suspicious features (aka columns)? Anything that is worth investigating for correctness? Any features of the data seem unclear or foreign to you? your answer here Another broad sanity check is to inspect the data types of each column, to ensure it's as expected. Below, fill in the blank to display the data types of each column. Specifically, when printing var_types to the screen, you should see: Unnamed: 0 int64 \\ TrackName object \\ ArtistName object \\ Genre object \\ BeatsPerMinute int64 \\ Energy int64 \\ Danceability int64 \\ Loudness int64 \\ Liveness int64 \\ Valence int64 \\ Length int64 \\ Acousticness int64 \\ Speechiness int64 \\ Popularity int64 In [32]: ### edTest(test_b) ### var_types = _____ var_types # displays it to the screen Hmm, do we have any missing values, though? Write code that determines if any value within the entire DataFrame is Null. This should be represented by the has_null variable. To be clear, has_null should have a value of True if there exists a Null value within the DataFrame. Otherwise, has_null should be False. In [35]: ### edTest(test_c) ### has_null = False # initializes to False # your code here # (that checks all cells and potentially updates `has_null`) # end of your code After exploring the values within the TrackName , ArtistName , and Genre columns, I noticed something looked peculiar. Specifically, one TrackName value isn't like any of the others. This was truly the case; I did not alter the name of this track, it's just how the dataset was created. Identify this strange TrackName value, either by manual inspection or writing code to assist you. Regardless of your approach, set the variable mystery_track to have that value. For example, if you thought the very first Track, Senorita was the unusual value, then mystery_track should be equal to the String Senorita . In [54]: ### edTest(test_d) ### mystery_track = ______ # must be a String data type While TrackNames and ArtistNames can be anything one desires, Genres have pre-defined categories/names, and there is a finite number of them. So, let's inspect the data to ensure that there is no spurious Genre. One indication that there is an invalid Genre is if only one song is of that Genre. Moreover, if there exists some Genre that is too all-encompassing, then we would expect to see a large percentage of the songs have that Genre. To this effect, use value_counts() to return the # of songs that fall within each Genre. Save this to the variable named genre_counts . Printing genre_counts should yield the following: dance pop 8 \\ pop 7 \\ latin 5 \\ edm 3 \\ canadian hip hop 3 \\ country rap 2 \\ brostep 2 \\ reggaeton flow 2 \\ reggaeton 2 \\ electropop 2 \\ dfw rap 2 \\ canadian pop 2 \\ panamanian pop 2 \\ atl hip hop 1 \\ australian pop 1 \\ escape room 1 \\ boy band 1 \\ r&b en espanol 1 \\ trap music 1 \\ pop house 1 \\ big room 1 \\ Name: Genre, dtype: int64 In [55]: ### edTest(test_e) ### genre_counts = ______ Notice there are several Genres with a count of only 1. Given that our dataset is of only 50 songs, this isn't too surprising. However, I've never heard of some of these Genres. If any of these Genres are also unfamiliar to you, use the Internet to verify that they're actually valid Genres. Below, write what you did to verify such, along with your thoughts on if they are all valid or not. your answer here Think of other steps you could take to explore the data, not only to ensure the data's legitimacy but perhaps elements that you're interested in and what you would do to address such. Discuss this with your group and be prepared to discuss this with the rest of the class in the main Zoom room.","tags":"labs","url":"labs/lecture-4/notebook-2/"},{"title":"Lecture 4: EDA + PANDAS","text":"CS109A Introduction to Data Science Lecture 4, Exercise 2: EDA and Advanced PANDAS Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner In [1]: import pandas as pd import seaborn as sns pd . set_option ( 'display.width' , 500 ) pd . set_option ( 'display.max_columns' , 100 ) NOTE : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. Advanced PANDAS We will use the same top50.csv file from Exercise 1. Below, fill in the blank to create a new PANDAS DataFrame from the top50.csv file. In [2]: ### edTest(test_a) ### df = _____ Run the cell below to print the summary statistics and display the column names In [3]: print ( df . columns ) df . describe () Fill in the blank to set longest_song equal to the TrackName of the longest song (the song that has the highest Length value) In [4]: ### edTest(test_b) ### longest_song = __________ longest_song Fill in the blank to set loud_song equal to the TrackName of the song that has the highest Loudness value amongst the 10 most popular songs. To be clear, by highest we mean the least negative. So, we would say that -2 is louder than -10. In [5]: ### edTest(test_c) ### loud_song = _______________ loud_song Say you are interested in songs that last 3-4 minutes, anything shorter doesn't have time to build up a narrative, and anything longer gets boring. You have a very particular taste but are open to listening to what other people are listening to a lot, but you don't even know what genre that is! Fill in the blank below (or use multiple lines if you prefer) to set new_genre to be the genre that is associated with the most songs that are between 3 and 4 minutes in duration (including songs that are exactly 3 minutes and 4 minutes). In [6]: ### edTest(test_d) ### new_genre = ____________________ new_genre Below, fill in the blank to make a new DataFrame genre_counts that groups all songs by their Genre and aggregates all other columns by their median value . Then, sort by their Popularity (in ascending order). As an example, if df had only 3 \"pop\" songs, and their Danceability values were 70,82, and 90, and df also contained 5 \"latin\" songs, and their Danceability values were 80,82,90,91,99. Then genre_counts would have 1 row for the Genre \"pop\", with a Danceability value of 82. genre_counts would also have 1 row for the Genre \"latin\", with a Danceability value of 90. Then, we'd sort the DataFrame by the Popularity (in ascending order). For example, your DataFrame should look as follows (snippet): In [7]: ### edTest(test_e) ### new_df = _______________ new_df Below, fill in the blank to set the variable speechy_song to the TrackName of the song that has the highest Speechiness value In [8]: ### edTest(test_f) ### speechy_song = _______________ speechy_song END OF GRADED SECTION EXTRA EXAMPLES FOLLOW: Combining multiple DataFrames As mentioned, often times one dataset doesn't contain all of the information you are interested in -- in which case, you need to combine data from multiple files. This also means you need to verify the accuracy (per above) of each dataset. spotify_aux.csv contains the same 50 songs as top50.csv ; however, it only contains 3 columns: Track Name Artist Name Explicit Language (boolean valued) Note, that 3rd column is just random Boolean values, but pretend as if it's correct. The point of this section is to demonstrate how to merge columns together. Let's load spotify_aux.csv into a DataFrame: In [9]: explicit_lyrics = pd . read_csv ( \"spotify_aux.csv\" ) explicit_lyrics Let's merge it with our df DataFrame that is storing top50.csv . .merge() is a Pandas function that stitches together DataFrames by their columns. .concat() is a Pandas function that stitches together DataFrames by their rows (if you pass axis=1 as a flag, it will be column-based) In [10]: # 'on='' specifies the column used as the shared key df_combined = pd . merge ( explicit_lyrics , df , on = 'TrackName' ) df_combined We see that all columns from both DataFrames have been added. That's nice, but having duplicate ArtistName and TrackName is unecessary. Since merge() uses DataFrames as the passed-in objects, we can simply pass merge() a stripped-down copy of ExplicitLanguage , which helps merge() not add any redundant fields. In [11]: df_combined2 = pd . merge ( explicit_lyrics [[ 'TrackName' , 'ExplicitLanguage' ]], df , on = 'TrackName' ) df_combined2 You'll notice we don't have the same number of rows as our original df DataFrame. Why is that? What happened? How can we quickly identify which rows are missing -- especially if our dataset had thousands of rows. Discuss with your group. Related to merge, are the useful functions: concat() aggregate() append() Plotting DataFrames As a very simple example of how one can plot elements of a DataFrame, we turn to Pandas' built-in plotting: In [12]: scatter_plot = df . plot . scatter ( x = 'Danceability' , y = 'Popularity' , c = 'DarkBlue' ) scatter_plot Out[12]: This shows the lack of a correlation between the Danceability of a song and its popularity, based on just the top 50 songs, of course. Please feel free to experiment with plotting other items of interest. Print the songs that are faster than the average Top 50 and more popular than the average Top 50? In [13]: avg_speed = df [ 'BeatsPerMinute' ] . mean () avg_popularity = df [ 'Popularity' ] . mean () df [( df [ 'BeatsPerMinute' ] > avg_speed ) & ( df [ 'Popularity' ] > avg_popularity )] In [ ]:","tags":"labs","url":"labs/lecture-4/notebook-3/"},{"title":"Lecture 4: Advanced PANDAS","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 4: EDA with PANDAS [Notebook] Lecture 4: EDA and Advanced PANDAS [Notebook] Lecture 4: Data Collection, Parsing, and Quick Analyses [Notebook]","tags":"lectures","url":"lectures/lecture04/"},{"title":"S-Section 01: Introduction to Web Scraping","text":"CS109A Introduction to Data Science Standard Section 1: Introduction to Web Scraping Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Section Leaders : Marios Mattheakis, Hayden Joy In [1]: ## RUN THIS CELL TO GET THE RIGHT FORMATTING import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: Section Learning Objectives When we're done today, you will approach messy real-world data with confidence that you can get it into a format that you can manipulate. Specifically, our learning objectives are: Understand the tree-like structure of an HTML document and use that structure to extract desired information Use Python data structures such as lists, dictionaries, and Pandas DataFrames to store and manipulate information Practice using Python packages such as BeautifulSoup and Pandas , including how to navigate their documentation to find functionality. Identify some other (semi-)structured formats commonly used for storing and transferring data, such as JSON and CSV In [2]: import numpy as np import matplotlib.pyplot as plt import pandas as pd from bs4 import BeautifulSoup import requests import json from IPython.display import HTML In [3]: # Setting up 'requests' to make HTTPS requests properly takes some extra steps... we'll skip them for now. % matplotlib inline requests . packages . urllib3 . disable_warnings () import warnings warnings . filterwarnings ( \"ignore\" ) Section Data Analysis Questions Is science becoming more collaborative over time? How about literature? Are there a few \"geniuses\" or lots of hard workers? One way we might answer those questions is by looking at Nobel Prizes. We could ask questions like: 1) Has anyone won a prize more than once? 2) How has the total number of recipients changed over time? 3) How has the number of recipients per award changed over time? To answer these questions, we'll need data: who received what award and when . Before we dive into acquiring this data the way we've been teaching in class, let's pause to ask: what are 5 different approaches we could take to acquiring Nobel Prize data ? When possible: find a structured dataset (.csv, .json, .xls) After a google search we stumble upon this dataset on github . It is also in the section folder named github-nobel-prize-winners.csv . We use pandas to read it: In [4]: df = pd . read_csv ( \"../data/github-nobel-prize-winners.csv\" ) df . head () #pandas is a very useful package Out[4]: year discipline winner desc 0 1901 chemistry Jacobus H. van 't Hoff in recognition of the extraordinary services h... 1 1901 literature Sully Prudhomme in special recognition of his poetic compositi... 2 1901 medicine Emil von Behring for his work on serum therapy, especially its ... 3 1901 peace Henry Dunant NaN 4 1901 peace Frédéric Passy NaN Or you may want to read an xlsx file: (Potential missing package; you might need to run the following command in your terminal first: !conda install xlrd ) In [7]: ! conda install --yes xlrd Collecting package metadata (current_repodata.json): done Solving environment: done ==> WARNING: A newer version of conda exists. <== current version: 4.7.10 latest version: 4.8.4 Please update conda by running $ conda update -n base -c defaults conda ## Package Plan ## environment location: /home/chris/anaconda3/envs/cs109a added / updated specs: - xlrd The following NEW packages will be INSTALLED: xlrd pkgs/main/linux-64::xlrd-1.2.0-py37_0 Preparing transaction: done Verifying transaction: done Executing transaction: done In [8]: df = pd . read_excel ( \"../data/github-nobel-prize-winners.xlsx\" ) df . tail () Out[8]: year discipline winner desc 848 2007 medicine Oliver Smithies for their discoveries of principles for introd... 849 2007 peace Intergovernmental Panel on Climate Change (IPCC) for their efforts to build up and disseminate ... 850 2007 peace Albert Arnold (Al) Gore Jr. for their efforts to build up and disseminate ... 851 2007 physics Albert Fert for the discovery of Giant Magnetoresistance 852 2007 physics Peter GrÃ¼nberg for the discovery of Giant Magnetoresistance introducing types In [ ]: #type(df.winner) #type(df) Research Question 1: Did anyone recieve the Nobel Prize more than once? How would you check if anyone recieved more than one nobel prize? In [ ]: # initialize the list storing all the names name_winners = [] for name in df . winner : # Check if we already encountered this name: if name in name_winners : # if so, print the name print ( name ) else : # otherwise append the name to the list name_winners . append ( name ) We don't want to print \"No Prize was Awarded\" all the time. In [ ]: # Your code here # list storing all the names name_winners = [] for name in df . winner : # Check if we already encountered this name: if name in name_winners and name : # if so, print the name print ( name ) else : # otherwise append the name to the list name_winners . append ( name ) we can use .split() on a string to separate the words into individual strings and store them in a list. In [ ]: UN_string = \"Office of the United Nations\" print ( UN_string . split ()) #n_words = len(UN_string.split()) #print(\"Number of words: \" + str(n_words)); Even better: In [ ]: name_winners = [] for name in df . winner : # Check if we already encountered this name: if name in name_winners and len ( name . split ()) <= 2 : # if so, print the name print ( name ) else : # otherwise append the name to the list name_winners . append ( name ) How can we make this into a oneligner? List comprehension form: [f(x) for x in list] In [ ]: winners = [] [ print ( name ) if ( name in winners and len ( name . split ()) <= 2 ) else winners . append ( name ) for name in df . winner ]; In [ ]: HTML ( ' \\ \\ Marie Curie recieved the nobel prize in physics in 1903 and chemistry in 1911. \\ She is one of only four people to recieve two Nobel Prizes. \\ \\ ' ) Part 2: WEB SCRAPING The first step in web scraping is to look for structure in the html. Lets look at a real website: The official Nobel website has the data we want, but in 2018 and 2019 the physics prize was awarded to multiple groups so we will use an archived version of the web-page for an easier introduction to web scraping. The Internet Archive periodically crawls most of the Internet and saves what it finds. (That's a lot of data!) So let's grab the data from the Archive's \"Wayback Machine\" (great name!). We've just given you the direct URL, but at the very end you'll see how we can get it out of a JSON response from the Wayback Machine API. Let's take a look at the 2018 version of the Nobel website and to look at the underhood HTML: right-click and click on inspect . Try to find structure in the tree-structured HTML. Play around! (give floor to the students) In [ ]: ################################################### The first step of web scraping is to write down the structure of the web page Here some quick recap of HTML tags and what they do in the context of this notebook: HTML tags are opened and closed as follows: \\ some text \\<\\h3>. Here are a list of few tags, their definitions and what information they contain in our problem today: \\ : header 3 tag tag is a header size 3 tag (header 1 is the largest tag). This tag will contain the title and year of the nobel prize, which we will parse out. \\ : header 6 tag tag (smaller than header 3) will contain the prize recipients \\ : paragraph tag tags used for text, contains the prize motivation \\ \"Content Division element ( \\ ) is the generic container for flow content.\" What we care about here is the class attribute, which we will use with beautiful soup to quickly parse information which we want. The class attribute could be attatched to any tag. Paying attention to tags with class attributes is key to the homework. In [ ]: # here is what we will get after selecting using the class by year tag. einstein = HTML ( ' \\ \\ \\ \\ The Nobel Prize in Physics 1921 \\ \\ \\ \\ \\ Albert Einstein \\ \\ \\ \"for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect\" \\ \\ ' ) display ( einstein ) In [ ]: snapshot_url = 'http://web.archive.org/web/20180820111639/https://www.nobelprize.org/prizes/lists/all-nobel-prizes/' In [ ]: snapshot = requests . get ( snapshot_url ) snapshot Response [200] is a success status code. Let's google: response 200 meaning . All possible codes here . In [ ]: type ( snapshot ) Try to request \"www.xoogle.be\". What happens? In [ ]: snapshot_url2 = 'http://web.archive.org/web/20180820111639/https://www.xoogle.be' snapshot = requests . get ( snapshot_url2 ) snapshot Always remember to \"not to be evil\" when scraping with requests! If downloading multiple pages (like you will be on HW1), always put a delay between requests (e.g, time.sleep(1) , with the time library) so you don't unwittingly hammer someone's webserver and/or get blocked. In [ ]: snapshot = requests . get ( snapshot_url ) raw_html = snapshot . text print ( raw_html [ 500 :]) Regular Expressions You can find specific patterns or strings in text by using Regular Expressions: This is a pattern matching mechanism used throughout Computer Science and programming (it's not just specific to Python). Some great resources that we recommend, if you are interested in them (could be very useful for a homework problem): https://docs.python.org/3.3/library/re.html https://regexone.com https://docs.python.org/3/howto/regex.html . Specify a specific sequence with the help of regex special characters. Some examples: \\S : Matches any character which is not a Unicode whitespace character \\d : Matches any Unicode decimal digit * : Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. Let's find all the occurances of 'Marie' in our raw_html: In [ ]: import re In [ ]: re . findall ( r 'Marie' , raw_html ) Using \\S to match 'Marie' + ' ' + 'any character which is not a Unicode whitespace character': In [ ]: re . findall ( r 'Marie \\S' , raw_html ) How would we find the lastnames that come after Marie? ANSWER: the \\w character represents any alpha-numeric character. \\w* is greedy and gets a repeat of the characters until the next bit of whitespace. In [ ]: # Your code here last_names = re . findall ( r 'Marie \\w*' , raw_html ) display ( last_names ) Now, we have all our data in the notebook. Unfortunately, it is the form of one really long string, which is hard to work with directly. This is where BeautifulSoup comes in. This is an example of code that grabs the first title. Regex can quickly become complex, which motivates beautiful soup. In [ ]: first_title = re . findall ( r ' .*<\\/a><\\/h3>' , raw_html )[ 0 ] print ( first_title ) #you can do this via regex, but it gets complicated fast! This motivates Beautiful Soup. Parse the HTML with BeautifulSoup In [ ]: soup = BeautifulSoup ( raw_html , 'html.parser' ) Key BeautifulSoup functions we'll be using in this section: tag.prettify() : Returns cleaned-up version of raw HTML, useful for printing tag.select(selector) : Return a list of nodes matching a CSS selector tag.select_one(selector) : Return the first node matching a CSS selector tag.text/soup.get_text() : Returns visible text of a node (e.g.,\" Some text \" -> \"Some text\") tag.contents : A list of the immediate children of this node You can also use these functions to find nodes. tag.find_all(tag_name, attrs=attributes_dict) : Returns a list of matching nodes tag.find(tag_name, attrs=attributes_dict) : Returns first matching node BeautifulSoup is a very powerful library -- much more info here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ Let's practice some BeautifulSoup commands... Print a cleaned-up version of the raw HTML Which function should we use from above? In [ ]: pretty_soup = soup . prettify () print ( pretty_soup [: 500 ]) #what about negative indices? Find the first \"title\" object In [ ]: # Your code here soup . select ( \"h3 a\" ) Extract the text of first \"title\" object In [ ]: #Your code here Extracting award data Let's use the structure of the HTML document to extract the data we want. From inspecting the page in DevTools, we found that each award is in a div with a by_year class. Let's get all of them. In [ ]: award_nodes = soup . select ( '.by_year' ) # len ( award_nodes ) Let's pull out an example. In [ ]: award_node = award_nodes [ 200 ] In [ ]: HTML ( award_node . prettify ()) Magic commands: In [ ]: # show ls, tree, mkdir Let's practice getting data out of a BS Node The prize title In [ ]: award_node . select_one ( 'h3' ) . text How do we separate the year from the selected prize title? In [ ]: # %load solutions/sol2.py award_node . select_one ( 'h3' ) . text [:] How do we drop the year from the title? In [ ]: award_node . select_one ( 'h3' ) . text [:] . strip () Let's put them into functions: In [ ]: # %load solutions/sol_functions.py def get_award_title ( award_node ): return award_node . select_one ( 'h3' ) . text [: - 4 ] . strip () def get_award_year ( award_node ): return int ( award_node . select_one ( 'h3' ) . text [ - 4 :]) Make a list of titles for all awards In [ ]: #original code: list_awards = [] for award_node in award_nodes : list_awards . append ( get_award_title ( award_node )) list_awards Let's use list comprehension: In [ ]: # Your code here [ get_award_title ( award_node ) for award_node in award_nodes ] The recipients How do we handle there being more than one? In [ ]: award_node . select ( 'h6 a' ) In [ ]: [ node . text for node in award_node . select ( 'h6 a' )] We'll leave them as a list for now, to return to this later. This is how you would get the links: (Relevant for the homework) In [ ]: [ state_node . get ( \"href\" ) for state_node in award_node . select ( 'h6 a' )] The prize \"motivation\" How would you get the 'motivation'/reason of the prize from the following award_node ? In [ ]: award_node = award_nodes [ 200 ] award_node In [ ]: # Your code here print ( award_node . select ( 'p' )[ 0 ] . text ); Putting everything into functions: In [ ]: def get_award_motivation ( award_node ): award_node = award_node . select_one ( 'p' ) if not award_node : #0, [], None, and {} all default to False in a python conditional statement. return None return award_node . text Break Out Room 1: Practice with CSS selectors, Functions and list comprehension In [ ]: print ( award_nodes [ 200 ]) Exercise 1.1: complete the following function by assigning the proper CSS-selector so that it returns a list of nobel prize award recipients. Hint: you can specify multiple selectors separated by a space. To load the first exercise by deleting the \"#\" and typing shift-enter to run the cell clicking on \"cell\" -> \"run all above\" is also very helpful to run many cells of the notebook at once. In [ ]: # %load exercises/exercise1.py Exercise 1.2: Change the above function so it uses list comprehension. To load the execise simply delete the '#' in the code below and run the cell. In [ ]: # %load exercises/exercise2.py Don't look at this cell until you've given the exercise a go! It loads the correct solution. Exercise 1.2 solution (1.1 solution is contained herein as well) In [ ]: # %load solutions/breakoutsol1.py In [ ]: % run ./solutions/breakoutsol1.py Let's create a Pandas dataframe Now let's get all of the awards. In [ ]: awards = [] for award_node in soup . select ( '.by_year' ): recipients = get_recipients ( award_node ) #initialize the dictionary award = {} #{key: value} award [ 'title' ] = get_award_title ( award_node ) award [ 'year' ] = get_award_year ( award_node ) award [ 'recipients' ] = recipients award [ 'num_recipients' ] = len ( recipients ) award [ 'motivation' ] = get_award_motivation ( award_node ) awards . append ( award ) awards [ 0 : 2 ] In [ ]: df_awards_raw = pd . DataFrame ( awards ) In [ ]: #explain open brackets df_awards_raw Some quick EDA. In [ ]: df_awards_raw . info () In [ ]: df_awards_raw . year . min () What is going on with the recipients column? In [ ]: df_awards_raw . head () In [ ]: df_awards_raw . num_recipients . value_counts () Now lets take a look at num_recipients In [ ]: df_awards_raw . num_recipients == 0 In [ ]: df_awards_raw [ df_awards_raw . num_recipients == 0 ] Ok: 2018 awards have no recipients because this is a 2018 archived version of nobel prize webpage. Some past years lack awards because none were actually awarded that year. Let's keep only meaningful data: In [ ]: df_awards_past = df_awards_raw [ df_awards_raw . year != 2018 ] df_awards_past . info () Hm, motivation has a different number of items... why? In [ ]: df_awards_past [ df_awards_past . motivation . isnull ()] Looks like it's fine that those motivations were missing. Sort the awards by year. In [ ]: df_awards_past . sort_values ( 'year' ) . head () How many awards of each type were given? In [ ]: df_awards_past . title . value_counts () But wait, that includes the years the awards weren't offered. In [ ]: df_awards_actually_offered = df_awards_past [ df_awards_past . num_recipients > 0 ] df_awards_actually_offered . title . value_counts () When was each award first given? In [ ]: df_awards_actually_offered . groupby ( 'title' ) . year In [ ]: df_awards_actually_offered . groupby ( 'title' ) . year . describe () # we will use this information later! How many recipients per year? Let's include the years with missing awards; if we were to analyze further, we'd have to decide whether to include them. A good plot that clearly reveals patterns in the data is very important. Is this a good plot or not? In [ ]: df_awards_past . plot . scatter ( x = 'year' , y = 'num_recipients' ) #explain scatterplot It's hard to see a trend when there are multiple observations per year ( why? ). Let's try looking at total num recipients by year. Lets explore how important a good plot can be In [ ]: df_awards_past . groupby ( 'year' ) . num_recipients . sum () In [ ]: plt . figure ( figsize = [ 16 , 6 ]) plt . plot ( df_awards_past . groupby ( 'year' ) . num_recipients . mean (), 'b' , linewidth = '1' ) plt . title ( 'Total Nobel Awards per year' ) plt . xlabel ( 'Year' ) plt . ylabel ( 'Total recipients per prize' ) plt . grid ( 'on' ) plt . show () Check out the years 1940-43? Any comment? Any trends the last 25 years? In [ ]: set ( df_awards_past . title ) In [ ]: plt . figure ( figsize = [ 16 , 6 ]) i = 0 for award in set ( df_awards_past . title ): i += 1 year = df_awards_past [ df_awards_past [ 'title' ] == award ] . year recips = df_awards_past [ df_awards_past [ 'title' ] == award ] . num_recipients index = year > 2020 - 25 years_filtered = year [ index ] . values recips_filtered = recips [ index ] . values plt . subplot ( 2 , 3 , i ) plt . bar ( years_filtered , recips_filtered , color = 'b' , alpha = 0.7 ) plt . title ( award ) plt . xlabel ( 'Year' ) plt . ylabel ( 'Number of Recipients' ) plt . ylim ( 0 , 3 ) plt . tight_layout () A cleaner way to iterate and keep tabs: the enumerate( ) function 'How has the number of recipients per award changed over time?' In [ ]: # The enumerate function allows us to delete two lines of code # The number of years shown is increased to 75 so we can see the trend. plt . figure ( figsize = [ 16 , 6 ]) for i , award in enumerate ( set ( df_awards_past . title ), 1 ): ################### <--- enumerate year = df_awards_past [ df_awards_past [ 'title' ] == award ] . year recips = df_awards_past [ df_awards_past [ 'title' ] == award ] . num_recipients index = year > 2019 - 75 ########################### <--- extend the range years_filtered = year [ index ] . values recips_filtered = recips [ index ] . values #plot: plt . subplot ( 2 , 3 , i ) #arguments (nrows, ncols, index) plt . bar ( years_filtered , recips_filtered , color = 'b' , alpha = 0.7 ) plt . title ( award ) plt . xlabel ( 'Year' ) plt . ylabel ( 'Number of Recipients' ) plt . ylim ( 0 , 3 ) plt . tight_layout () End of Standard Section Break Out Room II: Dictionaries, dataframes, and Pyplot Exercise 2.1 (practice creating a dataframe): Build a dataframe of famous physicists from the following lists. Your dataframe should have the following columns: \"name\", \"year_prize_awarded\" and \"famous_for\". In [ ]: famous_award_winners = [ \"Marie Curie\" , \"Albert Einstein\" , \"James Chadwick\" , \"Werner Karl Heisenberg\" ] nobel_prize_dates = [ 1923 , 1937 , 1940 , 1934 ] famous_for = [ \"spontaneous radioactivity\" , \"general relativity\" , \"strong nuclear force\" , \"uncertainty principle\" ] In [ ]: #initialize dictionary famous_physicists = {} #TODO: build Pandas Dataframe Exercise 2.2: Make a bar plot of the total number of Nobel prizes awarded per field. Make sure to use the 'group by' function to achieve this. In [ ]: #create the figure: plt . figure ( figsize = [ 16 , 6 ]) #group by command: #TODO Solutions: Exercise 2.1 Solutions In [ ]: # %load solutions/exercise2.1sol Exercise 2.2 Solutions In [ ]: # %load solutions/exercise2.2sol_vanilla In [ ]: # %load solutions/exercise2.2sol_improved Food for thought: Is the prize in Economics more collaborative, or just more modern? Extra: Did anyone recieve the Nobel Prize more than once (based upon scraped data)? Here's where it bites us that our original DataFrame isn't \"tidy\". Let's make a tidy one. A great scientific article describing tidy data by Hadley Wickam: https://vita.had.co.nz/papers/tidy-data.pdf In [ ]: tidy_awards = [] for idx , row in df_awards_past . iterrows (): for recipient in row [ 'recipients' ]: tidy_awards . append ( dict ( recipient = recipient , year = row [ 'year' ])) tidy_awards_df = pd . DataFrame ( tidy_awards ) tidy_awards_df Now we can look at each recipient individually. In [ ]: tidy_awards_df . recipient . value_counts () End of Normal Section Optional Further Readings Harvard Professor Sean Eddy in the micro and chemical Biology department at Harvard teaches a great course called MCB-112: Biological Data Science . His course is difficult but a great complement to CS109a and is also taught in python. Here are a couple resources that he referenced early in his course that helped solidify my understanding of data science. 50 Years of Data Science by Dave Donoho (2017) Tidy data by Hadley Wickam (2014) Extra Material: Other structured data formats (JSON and CSV) CSV CSV is a lowest-common-denominator format for tabular data. In [ ]: df_awards_past . to_csv ( '../data/awards.csv' , index = False ) with open ( '../data/awards.csv' , 'r' ) as f : print ( f . read ()[: 1000 ]) It loses some info, though: the recipients list became a plain string, and the reader needs to guess whether each column is numeric or not. In [ ]: pd . read_csv ( '../data/awards.csv' ) . recipients . iloc [ 20 ] JSON JSON preserves structured data, but fewer data-science tools speak it. In [ ]: df_awards_past . to_json ( '../data/awards.json' , orient = 'records' ) with open ( '../data/awards.json' , 'r' ) as f : print ( f . read ()[: 1000 ]) Lists and other basic data types are preserved. (Custom data types aren't preserved, but you'll get an error when saving.) In [ ]: pd . read_json ( '../data/awards.json' ) . recipients . iloc [ 20 ] Extra: Pickle: handy for storing data For temporary data storage in a single version of Python, pickle s will preserve your data even more faithfully, even many custom data types. But don't count on it for exchanging data or long-term storage. (In fact, don't try to load untrusted pickle s -- they can run arbitrary code!) In [ ]: df_awards_past . to_pickle ( '../data/awards.pkl' ) with open ( '../data/awards.pkl' , 'r' , encoding = 'latin1' ) as f : print ( f . read ()[: 200 ]) Yup, lots of internal Python and Pandas stuff... In [ ]: pd . read_pickle ( '../data/awards.pkl' ) . recipients . iloc [ 20 ] Extra: Formatted data output Let's make a textual table of Physics laureates by year, earliest first: In [ ]: for idx , row in df_awards_past . sort_values ( 'year' ) . iterrows (): if 'Physics' in row [ 'title' ]: print ( ' {} : {} ' . format ( row [ 'year' ], ', ' . join ( row [ 'recipients' ]))) Extra: Parsing JSON to get the Wayback Machine URL We could go to http://archive.org , search for our URL, and get the URL for the archived version there. But since you'll often need to talk with APIs, let's take this opportunity to use the Wayback Machine's API . This will also give us a chance to practice working with JSON. In [ ]: url = \"https://www.nobelprize.org/prizes/lists/all-nobel-prizes/\" # All 3 of these do the same thing. The third is my (KCA's) favorite new feature of Python 3.6. wayback_query_url = 'http://archive.org/wayback/available?url= {} ' . format ( url ) wayback_query_url = 'http://archive.org/wayback/available?url= {url} ' . format ( url = url ) wayback_query_url = f 'http://archive.org/wayback/available?url= { url } ' r = requests . get ( wayback_query_url ) We got some kind of response... what is it? In [ ]: r . text Yay, JSON ! It's usually pretty easy to work with JSON, once we parse it. In [ ]: json . loads ( r . text ) Loading responses as JSON is so common that requests has a convenience method for it: In [ ]: response_json = r . json () response_json What kind of object is this? A little Python syntax review: How can we get the snapshot URL? In [ ]: snapshot_url = response_json [ 'archived_snapshots' ][ 'closest' ][ 'url' ] snapshot_url","tags":"sections","url":"sections/notebook/section1/"},{"title":"S-Section 01: Introduction to Web Scraping","text":"Jupyter Notebooks S-Section 1: Introduction to Web Scraping","tags":"sections","url":"sections/section1/"},{"title":"Lecture 3: Web Scraping + PANDAS","text":"CS109A Introduction to Data Science Lecture 3, Exercise 1: Web Scraping and Parsing Intro Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Title Exercise 1: Web Scraping and Parsing Intro Description OVERVIEW As we learned in class, the three most common sources of data used for Data Science are: files (e.g, .csv , .txt ) that already contain the dataset APIs (e.g., Twitter or Facebook) web scraping (e.g., Requests) Here, we get practice with web scraping by using Requests . Once we fetch the page contents, we will need to extract the information that we actually care about. We rely on BeautifulSoup to help with this. In [27]: import re import requests from bs4 import BeautifulSoup NOTE : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. For this exercise, we will be grabbing data (the Top News stories) from AP News , a not-for-profit news agency. In [ ]: # the URL of the webpage that has the desired info url = \"https://apnews.com/hub/ap-top-news\" Web Scraping (Graded) Let's use requests to fetch the contents. Specifically, the requests library has a .get() function that returns a Response object . A Response object contains the server's response to the HTTP request, and thus contains all the information that we could want from the page. Below, fill in the blank to fetch AP News' Top News website. In [ ]: ### edTest(test_a) ### home_page = requests . get ( ____ ) home_page . status_code You should have received a status code of 200, which means the page was successfully found on the server and sent to receiver (aka client/user/you). Again, you can click here for a full list of status codes. Recall that sometimes, while browsing the Internet, webpages will report a 404 error, possibly with an entertaining graphic to ease your pain. That 404 is the status code, just like we are using here! home_page is now a Response object . It contains many attributes, including the .text . Run the cell below and note that it's identical to if we were to visit the webpage in our browser and clicked 'View Page Source'. In [ ]: home_page . text Data Parsing Intro (Graded) The above .text property is atrocious to view and make sense of. Sure, we could write Regular Expressions to extract all of the contents that we're interested in. Instead, let's first use BeautifulSoup to parse the content into more manageable chunks. Below, fill in the blank to construct an HTML-parsed BeautifulSoup object from our website. In [ ]: ### edTest(test_b) ### soup = BeautifulSoup ( ____ , ____ ) soup You'll notice that the soup object is better formatted than just looking at the entire text. It's still dense, but it helps. Below, fill in the blank to set webpage_title equal to the text of the webpage's title (no HTML tags included). In [ ]: ### edTest(test_c) ### webpage_title = ____ Again, our BeautifulSoup object allows for quick, convenient searching and access to the web page contents. Data Parsing Examples (Not Graded) Anytime you wish to extract specific contents from a webpage, it is necessary to: Step 1 . While viewing the page in your browser, identify what contents of the page you're interested in. Step 2 . Look at the HTML returned from the BeautifulSoup object, and pinpoint the specific context that surrounds each of these items that you're interested in Step 3. Devise a pattern using BeautifulSoup and/or RegularExpressions to extract said contents. For example: Step 1: Let's say, for every news article found on the AP's Top News page, you want to extract the link and associated title. In this screenshot we can see one news article (there are many more below on the page). Its title is \"California fires bring more chopper rescues, power shutoffs\" and its link is to /c0aa17fff978e9c4768ee32679b8555c . Since the current page is stored at apnews.com, the article link's full address is apnews.com/c0aa17fff978e9c4768ee32679b8555c . Step 2: After printing the soup object, we saw a huge mess of all of the HTML still. So, let's drill down into certain sections. As illustrated in the official documentation here , we can retrieve all links by running the cell below: In [ ]: soup . find_all ( \"a\" ) This is still a ton of text (links). So, let's get more specific. I now search for the title text California fires bring more chopper rescues, power shutoffs within the output of the previous cell (the HTML of all links). I notice the following: California fires bring more chopper rescues, power shutoffs I also see that this is repeatable; every news article on the Top News page has such text! Great! Step 3: The pattern is that we want the value of the href attribute, along with the text of the link. There are many ways to get at this information. Below, I show just a few: In [19]: # EXAMPLE 1 # returns all `a` links that also contain `Component-headline-0-2-110` soup . find_all ( \"a\" , \"Component-headline-0-2-110\" ) # iterates over each link and extracts the href and title for link in soup . find_all ( \"a\" , \"Component-headline-0-2-110\" ): url = \"www.apnews.com\" + link [ 'href' ] title = link . text As mentioned in the official documentation here and here , a tag (such as a ) may have many attributes, and you can search them by putting your terms in a dictionary. In [ ]: # EXAMPLE 2 # this returns the same exact subset of links as the example above # so, we could iterate through the list just like above soup . find_all ( \"a\" , attrs = { \"data-key\" : \"card-headline\" }) Alternatively, we could use Regular Expressions if we were confident that our Regex pattern only matched on the relevant links. In [ ]: # EXAMPLE 3 # instead of using the BeautifulSoup, we are handling all of the parsing # ourselves, and working directly with the original Requests text re . findall ( \" \\\" Component-headline.*?href= \\\" (.+?) \\\" > (.+?) \" , home_page . text )","tags":"labs","url":"labs/lecture-3/notebook/"},{"title":"Lecture 3: Web Scraping + PANDAS","text":"CS109A Introduction to Data Science Lecture 3, Exercise 2: PANDAS Intro Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner NOTE : After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given. Title Exercise 2: PANDAS Intro Description As discussed in class, PANDAS is Python library that contains highly useful data structures, including DataFrames, which makes Exploratory Data Analysis (EDA) easy. Here, we get practice with some of the elementary functions. In [1]: import pandas as pd For this exercise, we will be working with the CS109 First Day survey results! In [2]: # import the CSV file df = pd . read_csv ( \"cs109a_student_survey.csv\" ) PANDAS Basics Let's get started with basic functionality of PANDAS! In the cell below, fill in the blank so that the variable cols stores the df 's column names. NOTE: Please keep the type of the data structure as a . Do not have to convert this to a list. In [31]: ### edTest(test_a) ### cols = ____ In the cell below, fill in the blank so that: num_cols stores the number of columns in df In [4]: ### edTest(test_b) ### num_rows = df . shape [ 0 ] num_cols = ____ In the cell below, fill in the blank so that sneak_peak is equal to the first 7 rows. ( HINT ) In [5]: ### edTest(test_c) ### sneak_peak = ____ In the cell below, fill in the blank so that the_end is equal to the last 4 rows. ( HINT ) In [6]: ### edTest(test_d) ### the_end = ____ In the cell below, fill in the blank so that the python_experiences variable stores a list of the 5 distinct values found within the Python experience column of df . In [27]: ### edTest(test_e) ### python_experiences = ________ In the cell below, fill in the blank so that the inventor variable stores the DataFrame row(s) that correspond to everyone who is an \"Inventor of Python\". In [8]: ### edTest(test_f) ### inventor = ________ In the cell below, fill in the blank so that the utc1 variable stores the DataFrame rows that correspond to everyone who has a Timezone value of UTC+1 (Most of mainland Europe) In [9]: ### edTest(test_g) ### utc1 = ____________ In the cell below, fill in the blank so that the row56 variable stores the 56th row of df . To be clear, imagine our DataFrame looked as follows: . Name Age \\ 0 Enrique 25 \\ 1 Sheila 67 \\ 2 Marcy 21 \\ 3 Utibe 33 We'd say the 1st row is the one with Enrique, the 2nd row is the one with Sheila, the 3rd row is the one w/ Marcy, etc. In [10]: ### edTest(test_h) ### row56 = ________ In the cell below, fill in the blank so that sorted_df now stores df after sorting it by the Name column in ascending order (A -> Z) In [11]: ### edTest(test_i) ### sorted_df = ________ In the cell below, fill in the blank so that sorted_row56 stores the 56th row of sorted_df . To be clear, imagine our sorted DataFrame looked as follows: . Name Age \\ 0 Enrique 25 \\ 2 Marcy 21 \\ 1 Sheila 67 \\ 3 Utibe 33 We'd say the 1st row is the one with Enrique, the 2nd row is the one with Marcy, the 3rd row is the one w/ Sheila, etc. In [12]: ### edTest(test_j) ### sorted_row56 = ________","tags":"labs","url":"labs/lecture-3/notebook-2/"},{"title":"Lecture 3: Web Scraping + PANDAS","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 3: Web Scraping and Parsing Intro [Notebook] Lecture 3: PANDAS Intro [Notebook]","tags":"lectures","url":"lectures/lecture03/"},{"title":"Lecture 1: Data Science Demo","text":"CS109A Introduction to Data Science Lecture 1: Example part 2 Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner In [1]: import sys import datetime import numpy as np import scipy as sp import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from math import radians , cos , sin , asin , sqrt from sklearn.linear_model import LinearRegression sns . set ( style = \"ticks\" ) % matplotlib inline In [2]: import os DATA_HOME = os . getcwd () if 'ED_USER_NAME' in os . environ : DATA_HOME = '/course/data' HUBWAY_STATIONS_FILE = os . path . join ( DATA_HOME , 'hubway_stations.csv' ) HUBWAY_TRIPS_FILE = os . path . join ( DATA_HOME , 'hubway_trips_sample.csv' ) In [3]: hubway_data = pd . read_csv ( HUBWAY_TRIPS_FILE , index_col = 0 , low_memory = False ) hubway_data . head () Out[3]: seq_id hubway_id status duration start_date strt_statn end_date end_statn bike_nr subsc_type zip_code birth_date gender 336967 336968 382289 Closed 1133 7/12/2012 16:59:00 64.0 7/12/2012 17:18:00 40.0 B00177 Registered '02339 1959.0 Male 1228655 1228656 1375284 Closed 720 9/1/2013 18:31:00 36.0 9/1/2013 18:43:00 54.0 B00130 Casual NaN NaN NaN 1018544 1018545 1149209 Closed 780 7/17/2013 19:44:00 81.0 7/17/2013 19:57:00 53.0 T01324 Registered '02115 NaN Male 139915 139916 157009 Closed 926 11/29/2011 14:17:00 57.0 11/29/2011 14:32:00 22.0 B00499 Casual NaN NaN NaN 1263342 1263343 1412424 Closed 900 9/9/2013 14:00:00 42.0 9/9/2013 14:15:00 55.0 B00076 Casual NaN NaN NaN Who? Who's using the bikes? Refine into specific hypotheses: More men or more women? Older or younger people? Subscribers or one time users? In [4]: # Let's do some cleaning first by removing empty cells or replacing them with NaN. # Pandas can do this. # we will learn a lot about pandas hubway_data [ 'gender' ] = hubway_data [ 'gender' ] . replace ( np . nan , 'NaN' , regex = True ) . values # we drop hubway_data [ 'birth_date' ] . dropna () age_col = 2020.0 - hubway_data [ 'birth_date' ] . values In [5]: # matplotlib can create a plot with two sub-plots. # we will learn a lot about matplotlib fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 6 )) # find all the unique value of the column gender # numpy can do this # we will learn a lot about numpy gender_counts = np . unique ( hubway_data [ 'gender' ] . values , return_counts = True ) ax [ 0 ] . bar ( range ( 3 ), gender_counts [ 1 ], align = 'center' , color = [ 'black' , 'green' , 'teal' ], alpha = 0.5 ) ax [ 0 ] . set_xticks ([ 0 , 1 , 2 ]) ax [ 0 ] . set_xticklabels ([ 'none' , 'male' , 'female' ]) ax [ 0 ] . set_title ( 'Users by Gender' ) age_col = 2020.0 - hubway_data [ 'birth_date' ] . dropna () . values age_counts = np . unique ( age_col , return_counts = True ) ax [ 1 ] . bar ( age_counts [ 0 ], age_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax [ 1 ] . axvline ( x = np . mean ( age_col ), color = 'red' , label = 'average age' ) ax [ 1 ] . axvline ( x = np . percentile ( age_col , 25 ), color = 'red' , linestyle = '--' , label = 'lower quartile' ) ax [ 1 ] . axvline ( x = np . percentile ( age_col , 75 ), color = 'red' , linestyle = '--' , label = 'upper quartile' ) ax [ 1 ] . set_xlim ([ 1 , 90 ]) ax [ 1 ] . set_xlabel ( 'Age' ) ax [ 1 ] . set_ylabel ( 'Number of Checkouts' ) ax [ 1 ] . legend () ax [ 1 ] . set_title ( 'Users by Age' ) plt . tight_layout () plt . savefig ( 'who.png' , dpi = 300 ) Challenge There is actually a mistake in the code above. Can you find it? Soon you will be skillful enough to answers many \"who\" questions Where? Where are bikes being checked out? Refine into specific hypotheses: More in Boston than Cambridge? More in commercial or residential? More around tourist attractions? In [6]: # using pandas again to read the station locations station_data = pd . read_csv ( HUBWAY_STATIONS_FILE , low_memory = False )[[ 'id' , 'lat' , 'lng' ]] station_data . head () Out[6]: id lat lng 0 3 42.340021 -71.100812 1 4 42.345392 -71.069616 2 5 42.341814 -71.090179 3 6 42.361285 -71.065140 4 7 42.353412 -71.044624 In [7]: # Sometimes the data is given to you in pieces and must be merged! # we want to combine the trips data with the station locations. pandas to the rescue... hubway_data_with_gps = hubway_data . join ( station_data . set_index ( 'id' ), on = 'strt_statn' ) hubway_data_with_gps . head () Out[7]: seq_id hubway_id status duration start_date strt_statn end_date end_statn bike_nr subsc_type zip_code birth_date gender lat lng 336967 336968 382289 Closed 1133 7/12/2012 16:59:00 64.0 7/12/2012 17:18:00 40.0 B00177 Registered '02339 1959.0 Male 42.351100 -71.049600 1228655 1228656 1375284 Closed 720 9/1/2013 18:31:00 36.0 9/1/2013 18:43:00 54.0 B00130 Casual NaN NaN NaN 42.349673 -71.077303 1018544 1018545 1149209 Closed 780 7/17/2013 19:44:00 81.0 7/17/2013 19:57:00 53.0 T01324 Registered '02115 NaN Male 42.352409 -71.062679 139915 139916 157009 Closed 926 11/29/2011 14:17:00 57.0 11/29/2011 14:32:00 22.0 B00499 Casual NaN NaN NaN 42.340799 -71.081572 1263342 1263343 1412424 Closed 900 9/9/2013 14:00:00 42.0 9/9/2013 14:15:00 55.0 B00076 Casual NaN NaN NaN 42.352096 -71.070378 OK - we cheated above and we skip some of the code which generated this plot. When? When are the bikes being checked out? Refine into specific hypotheses: More during the weekend than on the weekdays? More during rush hour? More during the summer than the fall? In [24]: # Sometimes the feature you want to explore doesn't exist in the data, and must be engineered! # to find the time of the day we will use the start_date column and extrat the hours. # we use list comprehension # we will be doing a lot of those check_out_hours = hubway_data [ 'start_date' ] . apply ( lambda s : int ( s [ - 8 : - 6 ])) In [25]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) check_out_counts = np . unique ( check_out_hours , return_counts = True ) ax . bar ( check_out_counts [ 0 ], check_out_counts [ 1 ], align = 'center' , width = 0.4 , alpha = 0.6 ) ax . set_xlim ([ - 1 , 24 ]) ax . set_xticks ( range ( 24 )) ax . set_xlabel ( 'Hour of Day' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Time of Day vs Checkouts' ) plt . show () Why? For what reasons/activities are people checking out bikes? Refine into specific hypotheses: More bikes are used for recreation than commute? More bikes are used for touristic purposes? Bikes are use to bypass traffic? Do we have the data to answer these questions with reasonable certainty? What data do we need to collect in order to answer these questions? How? Questions that combine variables. How does user demographics impact the duration the bikes are being used? Or where they are being checked out? How does weather or traffic conditions impact bike usage? How do the characteristics of the station location affect the number of bikes being checked out? How questions are about modeling relationships between different variables. In [1]: # Here we define the distance from a point as a python function. # We set Boston city center long and lat to be the default value. # you will become experts in building functions and using functions just like this def haversine ( pt , lat2 = 42.355589 , lon2 =- 71.060175 ): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) \"\"\" lon1 = pt [ 0 ] lat1 = pt [ 1 ] # convert decimal degrees to radians lon1 , lat1 , lon2 , lat2 = map ( radians , [ lon1 , lat1 , lon2 , lat2 ]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin ( dlat / 2 ) ** 2 + cos ( lat1 ) * cos ( lat2 ) * sin ( dlon / 2 ) ** 2 c = 2 * asin ( sqrt ( a )) r = 3956 # Radius of earth in miles return c * r In [27]: # use only the checkouts that we have gps location station_counts = np . unique ( hubway_data_with_gps [ 'strt_statn' ] . dropna (), return_counts = True ) counts_df = pd . DataFrame ({ 'id' : station_counts [ 0 ], 'checkouts' : station_counts [ 1 ]}) counts_df = counts_df . join ( station_data . set_index ( 'id' ), on = 'id' ) counts_df . head () In [28]: # add to the pandas dataframe the distance using the function we defined above and using map counts_df . loc [:, 'dist_to_center' ] = list ( map ( haversine , counts_df [[ 'lng' , 'lat' ]] . values )) counts_df . head () In [29]: # we will use sklearn to fit a linear regression model # we will learn a lot about modeling and using sklearn reg_line = LinearRegression () reg_line . fit ( counts_df [ 'dist_to_center' ] . values . reshape (( len ( counts_df [ 'dist_to_center' ]), 1 )), counts_df [ 'checkouts' ] . values ) # use the fitted model to predict distances = np . linspace ( counts_df [ 'dist_to_center' ] . min (), counts_df [ 'dist_to_center' ] . max (), 50 ) In [30]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 5 )) ax . scatter ( counts_df [ 'dist_to_center' ] . values , counts_df [ 'checkouts' ] . values , label = 'data' ) ax . plot ( distances , reg_line . predict ( distances . reshape (( len ( distances ), 1 ))), color = 'red' , label = 'Regression Line' ) ax . set_xlabel ( 'Distance to City Center (Miles)' ) ax . set_ylabel ( 'Number of Checkouts' ) ax . set_title ( 'Distance to City Center vs Checkouts' ) ax . legend () Notice all axis are labeled, we used legends and titles when necessary. Also notice we commented our code. In [31]: In [32]:","tags":"labs","url":"labs/lecture-1/notebook-2/"},{"title":"Lecture 1: Data Science Demo","text":"CS109A Introduction to Data Science Lecture 1: Example Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Title Hub data, part 1: Reading data, examining them and formulate questions. Description Introduction : Hubway was metro-Boston's public bike share program, with more than 1600 bikes at 160+ stations across the Greater Boston area. Hubway was owned by four municipalities in the area. By 2016, Hubway operated 185 stations and 1750 bicycles, with 5 million ride since launching in 2011. The Data : In April 2017, Hubway held a Data Visualization Challenge at the Microsoft NERD Center in Cambridge, releasing 5 years of trip data. The Question : What does the data tell us about the ride share program? The original question: ‘What does the data tell us about the ride share program?' is a reasonable slogan to promote a hackathon. It is not good for guiding scientific investigation. Before we can refine the question, we have to look at the data! Note: Here we switch the order of the \"data science process\" In [1]: import sys import zipfile import datetime import numpy as np import scipy as sp import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from math import radians , cos , sin , asin , sqrt from sklearn.linear_model import LinearRegression sns . set ( style = \"ticks\" ) % matplotlib inline Download the data from https://drive.google.com/open?id=0B28c493CP9GtMzN1emFoMkJNNlU In [13]: import os DATA_HOME = os . getcwd () if 'ED_USER_NAME' in os . environ : DATA_HOME = '/course/data' HUBWAY_STATIONS_FILE = os . path . join ( DATA_HOME , 'hubway_stations.csv' ) HUBWAY_TRIPS_FILE = os . path . join ( DATA_HOME , 'hubway_trips_sample.csv' ) In [14]: hubway_data = pd . read_csv ( HUBWAY_TRIPS_FILE , index_col = 0 , low_memory = False ) hubway_data . head () Basic Summaries In [15]: hubway_data . describe () What Type Of In [16]: hubway_data . dtypes Go to Part 1 quiz and enter your questions. Once you are done return to the main room. In [17]: In [ ]:","tags":"Lectures","url":"lectures/lecture01/notebook/"},{"title":"Lecture 2: Data + RegEx","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 2: Data + RegEx [Notebook]","tags":"lectures","url":"lectures/lecture02/"},{"title":"Lecture 2: Data + RegEx","text":"CS109A Introduction to Data Science Lecture 2, Exercise 1: RegEx Harvard University Fall 2020 Instructors : Pavlos Protopapas, Kevin Rader, and Chris Tanner Title Exercise 1 Description Introduction Regular Expressions (RegEx) are a mechanism that allows one to define a pattern to be searched for. That is, Regular Expressions aren't just a Python concept or library; they extend beyond the scope of any one programming language. Fortunately, many programming languages support them, though. This makes string processing (namely, parsing) incredibly convenient, as it's vastly easier than writing our own search query (e.g., via a series of if-statements) Many programming languages slightly differ in their syntax for creating RegEx. RegEx's can quickly get tedious at times, and it is not expected for any student to memorize all of the possible character sequences or to become a master at them. We do expect you to know what they are and to have basic exposure to them, so that you are aware of their incredible utility and can use them in the future when you want to parse through text. It is perfectly permissible -- and expected -- for students to reference the syntax guide while creating RegEx. You will quickly learn that there are a few main, powerful character sequences that tend to get used over and over, and it's these that you will likely find useful for committing to memory: any whitespace non-whitespace any character non-character digit non-digit occurs 0 or more times occurs 1 or more times start of a string end of a string the difference between greedy and non-greedy (aka reluctant) searching Resources You may find this \"cheatsheet\" useful. As part of your post-class assignment, you should look through the official Python documentation . It's okay to skim most sections (skip .split() and .sub() ), but you should pay particular attention to: the Performing Matches section ( .match() ) Grouping section ( .group() ) Greedy vs Non-Greedy For immediate, visual feedback as to if your regular expression is matching the way you want, I highly recommend visiting websites that provide Online Regular Expression testing, e.g.: Pythex <-- my favorite (it includes a cheatsheet button) Regex101 Exercise In this exercise, we want you to practice using RegularExpression. If you have used them many times in the past, this will likely be a breeze for you. If this is your first time, please do not be intimidated by the unusual-looking syntax. After a few correct RegEx, you will be a pro in no time. In [ ]: import re import requests For the three parts to this Exercise, you will be extracting contents from the sample_string provided below. In [ ]: # she was elected as State Representative of my district (Somerville) this week # (I am not blasting contact information of random people) sample_string = \"Hello, my name is Erika Uyterhoeven, and my email is erika@electerika.com!!\" Part A: Your first RegEx (or at least in CS109A) Write code (fill in the blank) that returns a list of all \"words\" in sample_string . NOTE: here, we consider a \"word\" to be any contiguous group of characters that are separated by whitespace. Thus, words include any attached punctuation. For example, the very first \"word\" is Hello, , and the very last word is erika@electerika.com!! In [ ]: ### edTest(test_a) ### words = re . findall ( '___' , sample_string ) words Part B: No punctuation In the cell below, write a very similar RegEx, but now return all words excluding any attached punctuation marks, even if it's more than 1 punctuation mark. For example, the first word is now Hello (without the , ) and the last word is erika@electerika.com (without !! ). NOTE: For this part, let's assume there are only 2 types of punctuations in the world, ! and , . Thus, do not worry about properly treating others (e.g., .;[] ) In [ ]: ### edTest(test_b) ### words = re . findall ( '___' , sample_string ) words Part C: E-mail only In the cell below, write a RegEx to extract just her e-mail address, excluding the exclamation points. Thus, you should return erika@electerika.com . To be clear, you do not need to write a robust RegEx that properly matches all patterns of ____@___.__ . It is fine to target just this one e-mail by assuming the template is ____@____ (without attached ! ) In [ ]: ### edTest(test_c) ### words = re . findall ( '___' , sample_string ) words","tags":"labs","url":"labs/lecture02/notebook/"},{"title":"Lecture 1: Introduction","text":"Slides Slides (PDF) Slides (PPTX) Exercises Lecture 1: Data Science Demo Exercises 1 [Notebook] Lecture 1: Data Science Demo Exercises 2 [Notebook]","tags":"lectures","url":"lectures/lecture01/"},{"title":"CS109a: Introduction to Data Science","text":"Fall 2020 Pavlos Protopapas , Kevin A. Rader , and Chris Tanner Additional Instructor: Eleni Kaxiras Welcome to CS109a/STAT121a/AC209a, also offered by the DCE as CSCI E-109a, Introduction to Data Science. This course is the first half of a one‐year course to data science. We will focus on the analysis of data to perform predictions using statistical and machine learning methods. Topics include data scraping, data management, data visualization, regression and classification methods, and deep neural networks. You will get ample practice through weekly homework assignments. The class material integrates the five key facets of an investigation using data: 1. data collection ‐ data wrangling, cleaning, and sampling to get a suitable data set 2. data management ‐ accessing data quickly and reliably 3. exploratory data analysis – generating hypotheses and building intuition 4. prediction or statistical learning 5. communication – summarizing results through visualization, stories, and interpretable summaries Only one of CS 109a, AC 209a, or Stat 121a can be taken for credit. Students who have previously taken CS 109, AC 209, or Stat 121 cannot take CS 109a, AC 209a, or Stat 121a for credit. Important Dates: Tuesday 9/8 - HW1 released Wednesday 9/9 - HW0 due at 11:59pm EST Thursday 9/10 - 'Study Break' at 8:30pm EST Friday 9/11 - 'Study Break' at 10:15am EST Friday 9/11 - Sections start (1:30pm EST) See syllabus/calendar for weekly times Helpline: cs109a2020@gmail.com Lectures: Mon , Wed , & Fri 9:00‐10:15 am & 3:00-4:14 pm (identical material in a single day) Sections: Fri 1:30-2:45 pm & Mon 8:30-9:45 pm (identical material) [starts 9/11] Advanced Sections: Wed 12pm [starts 9/23] Office Hours: TBD Course material can be viewed in the public GitHub repository . Previous Material 2019 2018 2017 2015 2014 . 2013","tags":"pages","url":"pages/cs109a-introduction-to-data-science/"}]}