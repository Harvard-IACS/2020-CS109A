{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title\n",
    "\n",
    "**Exercise: 3 - Hyper-parameter Tuning for Ridge Regression**\n",
    "\n",
    "# Description\n",
    "\n",
    "The goal of this exercise is to perform hyper-parameter tuning and produce a plot similar to the one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/IMAGE4.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "- Read the dataset \"polynomial50.csv\" as a dataframe.\n",
    "- Assign the predictor and response variables.\n",
    "- Visualize the dataset by making plots using the predictor and response variables along with the true function.\n",
    "- Split the data into train and validation sets using random_state=109.\n",
    "- For each value of alpha from a given list:\n",
    "    - Estimate a Ridge regression on the training data with the alpha value.\n",
    "    - Calculate the MSE of training and validation data. Append to separate lists appropriately.\n",
    "    - Use the given plot_functions function to plot the value of parameters.\n",
    "- Compute the best hyperparameter for this data based on the lowest MSE\n",
    "- Make a plot of the MSE values for each value of hyper-parameter alpha from the list above. It should look similar to the one given above.\n",
    "\n",
    "# Dataset Description:\n",
    "- The dataset has a total of 3 columns with names - x,y and f\n",
    "- \"x\" represents the predictor variable\n",
    "- \"y\" is the response variable\n",
    "- \"f\" denotes the true values of the underlying function\n",
    "\n",
    "\n",
    "# Hints\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\" target=\"_blank\">sklearn.Ridge()</a> : Linear least squares with L2 regularization\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\" target=\"_blank\">sklearn.train_test_split()</a> : Splits the data into random train and test subsets\n",
    "\n",
    "<a href=\"https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.axes.Axes.plot.html\" target=\"_blank\">ax.plot()</a> : Plot y versus x as lines and/or markers\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\" target=\"_blank\">sklearn.PolynomialFeatures()</a> : Generates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\" target=\"_blank\">sklearn.fit_transform()</a> : Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\" target=\"_blank\">sklearn.Ridge()</a> : Linear least squares with L2 regularization\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict\" target=\"_blank\">sklearn.predict()</a> : Predict using the linear modReturns the coefficient of the predictors in the model.\n",
    "\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\" target=\"_blank\">mean_squared_error()</a> : Mean squared error regression loss\n",
    "\n",
    "**Note: This exercise is auto-graded and you can try multiple attempts.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "plt.style.use('seaborn-white')\n",
    "\n",
    "# These are custom functions made to help you visualise your results\n",
    "from helper import plot_functions\n",
    "from helper import plot_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file 'polynomial50.csv' as a dataframe\n",
    "\n",
    "df = pd.read_csv('polynomial50.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>f</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.020408</td>\n",
       "      <td>1.039176</td>\n",
       "      <td>1.028283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.040816</td>\n",
       "      <td>1.075173</td>\n",
       "      <td>1.069739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061224</td>\n",
       "      <td>1.108144</td>\n",
       "      <td>1.077327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.081633</td>\n",
       "      <td>1.138242</td>\n",
       "      <td>1.105688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         f         y\n",
       "0  0.000000  1.000000  0.923951\n",
       "1  0.020408  1.039176  1.028283\n",
       "2  0.040816  1.075173  1.069739\n",
       "3  0.061224  1.108144  1.077327\n",
       "4  0.081633  1.138242  1.105688"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a quick look at the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the values of the 'x' column as the predictor\n",
    "\n",
    "x = df[['x']].values\n",
    "\n",
    "# Assign the values of the 'y' column as the response\n",
    "\n",
    "y = df['y'].values\n",
    "\n",
    "# Also assign the true value of the function (column 'f') to the variable f \n",
    "\n",
    "f = df['f'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the distribution of the x, y values & also the value of the true function f\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot x vs y values\n",
    "\n",
    "ax.plot(___,___, '.', label = 'Observed values',markersize=10)\n",
    "\n",
    "# Plot x vs true function value\n",
    "\n",
    "ax.plot(___,___, 'k-', label = 'Function description')\n",
    "\n",
    "# The code below is to annotate your plot\n",
    "\n",
    "ax.legend(loc = 'best');\n",
    "\n",
    "ax.set_xlabel('Predictor - $X$',fontsize=16)\n",
    "ax.set_ylabel('Response - $Y$',fontsize=16)\n",
    "ax.set_title('Predictor vs Response plot',fontsize=16);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f80e26f1ae15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Split the data into train and validation sets with training size 80% and random_state = 109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m109\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the data into train and validation sets with training size 80% and random_state = 109\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y, train_size = 0.8,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5ea6aac1c231>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### edTest(test_mse) ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Select the degree for polynomial features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "### edTest(test_mse) ### \n",
    "\n",
    "fig, rows = plt.subplots(6, 2, figsize=(16, 24))\n",
    "\n",
    "# Select the degree for polynomial features\n",
    "\n",
    "degree= __\n",
    "\n",
    "# List of hyper-parameter values \n",
    "\n",
    "alphas = [0.0, 1e-7,1e-5, 1e-3, 0.1,1]\n",
    "\n",
    "# Create two lists for training and validation error\n",
    "\n",
    "training_error, validation_error = [],[]\n",
    "\n",
    "# Compute the polynomial features train and validation sets\n",
    "\n",
    "x_poly_train = PolynomialFeatures(___).fit_transform(___)\n",
    "x_poly_val= PolynomialFeatures(___).fit_transform(___)\n",
    "\n",
    "for i, alpha in enumerate(alphas):\n",
    "    l,r=rows[i]\n",
    "    \n",
    "    # For each i, fit a ridge regression on training set\n",
    "    \n",
    "    ridge = Ridge(fit_intercept=False, alpha=___)\n",
    "    ridge.fit(___,___)\n",
    "    \n",
    "    # Predict on the validation set \n",
    "    \n",
    "    y_train_pred = ridge.predict(___)\n",
    "    y_val_pred = ridge.predict(___)\n",
    "    \n",
    "    # Compute the training and validation errors\n",
    "    \n",
    "    mse_train = mean_squared_error(___, ___) \n",
    "    mse_val = mean_squared_error(___, ___)\n",
    "    \n",
    "    # Add that value to the list \n",
    "    training_error.append(___)\n",
    "    validation_error.append(___)\n",
    "    \n",
    "    # Use helper functions plot_functions & plot_coefficients to visualise the plots\n",
    "    \n",
    "    plot_functions(degree, ridge, l, df, alpha, x_val, y_val, x_train, y_train)\n",
    "    plot_coefficients(ridge, r, alpha)\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper parameter value, alpha = \n"
     ]
    }
   ],
   "source": [
    "### edTest(test_hyper) ###\n",
    "# Find the best value of hyper parameter, which gives the least error on the validdata\n",
    "\n",
    "best_parameter = ___\n",
    "\n",
    "print(f'The best hyper parameter value, alpha = {best_parameter}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now make the MSE polots\n",
    "# Plot the errors as a function of increasing d value to visualise the training and validation errors\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12,8))\n",
    "\n",
    "# Plot the training errors for each alpha value\n",
    "\n",
    "ax.plot(___,___,'s--', label = 'Training error',color = 'Darkblue',linewidth=2)\n",
    "\n",
    "# Plot the validation errors for each alpha value\n",
    "\n",
    "ax.plot(___,___,'s-', label = 'validation error',color ='#9FC131FF',linewidth=2 )\n",
    "\n",
    "# Draw a vertical line at the best parameter\n",
    "\n",
    "ax.axvline(___, 0, 0.5, color = 'r', label = f'Min validation error at alpha = {best_parameter}')\n",
    "\n",
    "ax.set_xlabel('Value of Alpha',fontsize=15)\n",
    "ax.set_ylabel('Mean Squared Error',fontsize=15)\n",
    "ax.set_ylim([0,0.010])\n",
    "ax.legend(loc = 'upper left',fontsize=16)\n",
    "ax.set_title('Mean Squared Error',fontsize=20)\n",
    "ax.set_xscale('log')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune $\\alpha$ value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see above, the the best parameter value is at $\\alpha = 0.001$.\n",
    "However, you can further find a better solution by experimenting with the hyper-parameter values by narrowing the selection of `alphas`\n",
    "\n",
    "So **after marking the exercise**, Go back and remove 0, and 0.1 from the `alphas` list and replace with values close to 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
