{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "%matplotlib inline\n",
                "import sys\n",
                "import numpy as np\n",
                "import pylab as pl\n",
                "import pandas as pd\n",
                "import sklearn as sk\n",
                "import statsmodels.api as sm\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.decomposition import PCA\n",
                "import sklearn.metrics as met\n",
                "\n",
                "\n",
                "pd.set_option('display.width', 500)\n",
                "pd.set_option('display.max_columns', 100)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PCA"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 0: Reading the data \n",
                "\n",
                "In this notebook, we will be using the same Heart dataset from last lecture.  As a reminder the variables we will be using today include:\n",
                "\n",
                "- `AHD`: whether or not the patient presents atherosclerotic heart disease (a heart attack): `Yes` or `No`\n",
                "- `Sex`: a binary indicator for whether the patient is male (Sex=1) or female (Sex=0)\n",
                "- `Age`: age of patient, in years\n",
                "- `MaxHR`: the maximum heart rate of patient based on exercise testing\n",
                "- `RestBP`: the resting systolic blood pressure of the patient\n",
                "- `Chol`: the HDL cholesterol level of the patient\n",
                "- `Oldpeak`: ST depression induced by exercise relative to rest (on an ECG)\n",
                "- `Slope`: the slope of the peak exercise ST segment (1 = upsloping; 2 = flat; 3 = downsloping)\n",
                "- `Ca`: number of major vessels (0-3) colored by flourosopy\n",
                "\n",
                "For further information on the dataset, please see the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_heart = pd.read_csv('Heart.csv')\n",
                "\n",
                "# Force the response into a binary indicator:\n",
                "df_heart['AHD'] = 1*(df_heart['AHD'] == \"Yes\")\n",
                "\n",
                "print(df_heart.shape)\n",
                "df_heart.head()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 1: Principal Components Analysis (PCA) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q1.1** Just a sidebar (and a curiosity), what happens when two of the identical predictor is used in logistic regression?  Is an error created?  Should one be?  Investigate by predicting `AHD` from two copies of `Age`, and compare to the simple logistic regression model with `Age` alone."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "y = df_heart['AHD']\n",
                "\n",
                "logit1 = LogisticRegression(penalty=\"none\",solver=\"lbfgs\").fit(df_heart[['Age']],y)\n",
                "\n",
                "# investigating what happens when two identical predictors of 'Age' are used\n",
                "logit2 = LogisticRegression(penalty=\"none\",solver=\"lbfgs\").fit(___,y)\n",
                "\n",
                "print(\"The coef estimate for Age (when in the model once):\",logit1.coef_)\n",
                "print(\"The coef estimates for Age (when in the model twice):\",logit2.coef_)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*\n",
                "    \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will apply PCA to the heart dataset when there are just 4 predictors considered (remember: PCA is used when dimensionality is high (lots of predictors), but this will help us get our heads around what is going on):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For pedagogical purposes, let's simplify our lives and use just 7 predictors\n",
                "X = df_heart[['Age','RestBP','Chol','MaxHR','Sex','Oldpeak','Slope']]\n",
                "y = df_heart['AHD']\n",
                "X.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Here is the table of correlations between our predictors.  This will be useful later on\n",
                "X.corr()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q1.2** Is there any evidence of multicollinearity in the set of predictors?  How do you know?  How will PCA handle these correlations?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next we apply the [PCA transformation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) in a few steps, and show some of the results below:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# create/fit the 'full' pca transformation\n",
                "pca = PCA().fit(X)\n",
                "\n",
                "# apply the pca transformation to the full predictor set\n",
                "pcaX = pca.transform(X)\n",
                "\n",
                "# convert to a data frame\n",
                "pcaX_df = pd.DataFrame(pcaX, columns=[['PCA1' , 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7']])\n",
                "\n",
                "# here are the weighting (eigen-vectors) of the variables (first 2 at least)\n",
                "print(\"First PCA Component (w1):\",pca.components_[0,:])\n",
                "print(\"Second PCA Component (w2):\",pca.components_[1,:])\n",
                "\n",
                "# here is the variance explained:\n",
                "print(\"Variance explained by each component:\",pca.explained_variance_ratio_)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q1.3** Now try the PCA decompositon on the standardized version of X instead."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_pcaZ) ###\n",
                "\n",
                "# create/fit the standardized version of X\n",
                "Z = sk.preprocessing.StandardScaler().fit(___).transform(___)\n",
                "\n",
                "# create/fit the 'full' pca transformation on Z\n",
                "pca_standard = PCA().fit(___)\n",
                "pcaZ = pca_standard.transform(___)\n",
                "\n",
                "# convert to a data frame\n",
                "pcaZ_df = pd.DataFrame(___, columns=[['PCA1' , 'PCA2', 'PCA3', 'PCA4', 'PCA5', 'PCA6', 'PCA7']])\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# Let's look at them to see what they are comprised of:\n",
                "pd.DataFrame.from_dict({'Variable': X.columns,\n",
                "                        'PCA1': pca.components_[0],\n",
                "                        'PCA2': pca.components_[1],\n",
                "                        'PCA-Z1': pca_standard.components_[0],\n",
                "                        'PCA-Z2': pca_standard.components_[1]})"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q1.3** Interpret the results above.  What doss $w_1$ represent based on the untransformed data?  What doss $w_1$ represent based on the standardized data?  Which is a better representation of the data?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*\n",
                "\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "np.sum(pca.components_[0,:]**2)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q1.4** It is common for a model with high dimensional data (lots of predictors) to be plotted along the first 2 PCA components (with the classification boundaries added).  Below is the scatter plot for these data (without a classificaiton boundary, since we do not have a model yet) for the unstandardized PCA.  Repeat this for the standardized PCA:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the response over the first 2 PCA component vectors\n",
                "\n",
                "fig,(ax1,ax2) =  plt.subplots(1, 2, figsize = (12,5))\n",
                "\n",
                "ax1.scatter(pcaX_df[['PCA1']][y==0],pcaX_df[['PCA2']][y==0])\n",
                "ax1.scatter(pcaX_df[['PCA1']][y==1],pcaX_df[['PCA2']][y==1])\n",
                "ax1.legend([\"AHD = No\",\"AHD = Yes\"])\n",
                "ax1.set_xlabel(\"First PCA Component Vector\")\n",
                "ax1.set_ylabel(\"Second PCA Component Vector\");\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q2.4** Does there appear to be good potential here?  Which form of the data appear to provide better information as to *seperate* the classes in the response?  What at would a classification boundary look like if a logistic regression model were fit using the first 2 principal components as the predictors?  "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part 2: PCA in Regression (PCR) "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First let's fit the full logistic regression model to predict `AHD` from the 7 predictors above.\n",
                "\n",
                "Remember: PCA is an approach to handling the predictors (unsupervised), so it does not matter if we are using it for a regression or classification type problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#fit the 'full' model on the 7 predictors. and print out the coefficients\n",
                "logit_full = LogisticRegression(penalty=\"none\",solver=\"lbfgs\",max_iter=2000).fit(X,y)\n",
                "\n",
                "beta = logit_full.coef_[0]\n",
                "\n",
                "print(beta)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q2.1** Fit the logistic model on the first PCA vector below and get some relevant output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "### edTest(test_pcr1) ###\n",
                "\n",
                "logit_pcr1 = LogisticRegression(penalty = 'none', solver=\"lbfgs\").fit(___,y)\n",
                "\n",
                "print(\"Intercept from simple PCR-Logistic:\",logit_pcr1.intercept_)\n",
                "print(\"'Slope' from simple PCR-Logistic:\", logit_pcr1.coef_)\n",
                "\n",
                "print(\"First PCA Component (w1):\",pca.components_[0:1,:])\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q2.2** What does this PCR-1 model tell us about how the predictors relate to the response (aka, estimate the coefficient(s) in the original predictor space)?  Is it truly a simple logistic regression model in the original predictor space?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# your code here: do a multiplication of pcr_1's coefficients times the first component vector from PCA\n",
                "\n",
                "(logit_pcr1.coef_*pca.components_[0:1,:])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here is the above claculation fora few up to the 7th PCR logistic regression, and then plotted on a 'pretty' plot:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Fit the other 3 PCRs on the rest of the 7 predictors\n",
                "#pcaX_df.iloc[:,np.arange(0,5)].head()\n",
                "\n",
                "logit_pcr2 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2']],y)\n",
                "logit_pcr3 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2','PCA3']],y)\n",
                "logit_pcr4 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2','PCA3','PCA4']],y)\n",
                "logit_pcr5 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2','PCA3','PCA4','PCA5']],y)\n",
                "logit_pcr6 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df[['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6']],y)\n",
                "logit_pcr7 = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaX_df,y)\n",
                "\n",
                "pcr1=(logit_pcr1.coef_*np.transpose(pca.components_[0:1,:])).sum(axis=1)\n",
                "pcr2=(logit_pcr2.coef_*np.transpose(pca.components_[0:2,:])).sum(axis=1)\n",
                "pcr3=(logit_pcr3.coef_*np.transpose(pca.components_[0:3,:])).sum(axis=1)\n",
                "pcr4=(logit_pcr4.coef_*np.transpose(pca.components_[0:4,:])).sum(axis=1)\n",
                "pcr5=(logit_pcr5.coef_*np.transpose(pca.components_[0:5,:])).sum(axis=1)\n",
                "pcr6=(logit_pcr6.coef_*np.transpose(pca.components_[0:6,:])).sum(axis=1)\n",
                "pcr7=(logit_pcr7.coef_*np.transpose(pca.components_[0:7,:])).sum(axis=1)\n",
                "\n",
                "results = np.vstack((pcr1,pcr2,pcr3,pcr4,pcr5,pcr6,pcr7,beta))\n",
                "print(results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(['PCR1','PCR2', 'PCR3', 'PCR4','PCR5','PCR6','PCR7', 'Logistic'],results)\n",
                "\n",
                "plt.ylabel(\"Back-calculated Beta Coefficients\");\n",
                "\n",
                "#plt.legend(X.columns);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q2.3** Interpret the plot above.  Specifically, compare how each PCA vector \"contributes\" to the original logistic regression model using all 7 original predictors.  How Does PCR-4 compare to the original logistic regression model (in estimated coefficients)?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*\n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "All of this PCA work should have been done using the standardized versions of the predictors.  Below is the code that does exactly that:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "scaler = sk.preprocessing.StandardScaler()\n",
                "scaler.fit(X)\n",
                "Z = scaler.transform(X)\n",
                "pca = PCA(n_components=7).fit(Z)\n",
                "pcaZ = pca.transform(Z)\n",
                "pcaZ_df = pd.DataFrame(pcaZ, columns=[['PCA1' , 'PCA2', 'PCA3', 'PCA4','PCA5', 'PCA6', 'PCA7']])\n",
                "\n",
                "print(\"First PCA Component (w1):\",pca.components_[0,:])\n",
                "print(\"Second PCA Component (w2):\",pca.components_[1,:])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "#fit the 'full' model on the 4 predictors. and print out the coefficients\n",
                "logit_full = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(Z,y)\n",
                "\n",
                "\n",
                "betaZ = logit_full.coef_[0]\n",
                "\n",
                "print(\"Logistic coef. on standardized predictors:\",betaZ)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# Fit the PCR\n",
                "logit_pcr1Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1']],y)\n",
                "logit_pcr2Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1','PCA2']],y)\n",
                "logit_pcr3Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1','PCA2','PCA3']],y)\n",
                "logit_pcr4Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df[['PCA1','PCA2','PCA3','PCA4']],y)\n",
                "logit_pcr7Z = LogisticRegression(C=1000000,solver=\"lbfgs\").fit(pcaZ_df,y)\n",
                "\n",
                "pcr1Z=(logit_pcr1Z.coef_*np.transpose(pca.components_[0:1,:])).sum(axis=1)\n",
                "pcr2Z=(logit_pcr2Z.coef_*np.transpose(pca.components_[0:2,:])).sum(axis=1)\n",
                "pcr3Z=(logit_pcr3Z.coef_*np.transpose(pca.components_[0:3,:])).sum(axis=1)\n",
                "pcr4Z=(logit_pcr4Z.coef_*np.transpose(pca.components_[0:4,:])).sum(axis=1)\n",
                "pcr7Z=(logit_pcr7Z.coef_*np.transpose(pca.components_)).sum(axis=1)\n",
                "\n",
                "resultsZ = np.vstack((pcr1Z,pcr2Z,pcr3Z,pcr4Z,pcr7Z,betaZ))\n",
                "print(resultsZ)\n",
                "\n",
                "plt.plot(['PCR1-Z' , 'PCR2-Z', 'PCR3-Z', 'PCR4-Z', 'PCR7-Z', 'Logistic'],resultsZ)\n",
                "\n",
                "plt.ylabel(\"Back-calculated Beta Coefficients\");\n",
                "\n",
                "plt.legend(X.columns);"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q2.4** Compare this plot to the previous one; why does this plot make sense?.  What does this illustrate?  "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "*your answer here*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
