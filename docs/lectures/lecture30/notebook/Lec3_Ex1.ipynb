{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "%matplotlib inline"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here is our function of interest and its derivative."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def f(x):\n",
                "    return np.cos(3*np.pi*x)/x\n",
                "\n",
                "def der_f(x):\n",
                "    '''derivative of f(x)'''\n",
                "    return -(3*np.pi*x*np.sin(3*np.pi*x)+np.cos(3*np.pi*x))/x**2\n",
                "\n",
                "# the part of the function we will focus on\n",
                "FUNC_RANGE = (0.1, 3) \n",
                "\n",
                "x = np.linspace(*FUNC_RANGE, 200)\n",
                "fig, ax = plt.subplots(figsize=(4,3))\n",
                "plt.plot(x,f(x))\n",
                "plt.xlim(x.min(), x.max());\n",
                "plt.xlabel('x')\n",
                "plt.ylabel('y');"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "These 2 functions are just here to help us visualize the gradient descent. You should inspect them later to see how the work."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_tangent_line(x, x_range=.5):\n",
                "    '''returns information about the tangent line of f(x)\n",
                "       at a given x\n",
                "       Returns:\n",
                "       x: np.array - x-values in the tangent line segment\n",
                "       y: np.array - y-values in tangent line segment\n",
                "       m: float - slope of tangent line'''\n",
                "    y = f(x)\n",
                "    m = der_f(x)\n",
                "    # get tangent line points\n",
                "    # slope point form: y-y_1 = m(x-x_1)\n",
                "    # y = m(x-x_1)+y_1\n",
                "    x1, y1 = x, y\n",
                "    x = np.linspace(x1-x_range/2, x1+x_range/2, 50)\n",
                "    y = m*(x-x1)+y1\n",
                "    return x, y, m\n",
                "\n",
                "def plot_it(cur_x, title='', ax=plt):\n",
                "    '''plots the point cur_x on the curve f(x) as well as\n",
                "       the tangent line on f(x) where x=cur_x'''\n",
                "    y = f(x)\n",
                "    ax.plot(x,y)\n",
                "    ax.scatter(cur_x, f(cur_x), c='r', s=80, alpha=1);\n",
                "    x_tan, y_tan, der = get_tangent_line(cur_x)\n",
                "    ax.plot(x_tan, y_tan, ls='--', c='r')\n",
                "    # indicate if our location is outside the x range\n",
                "    if cur_x \u003e x.max():\n",
                "        ax.axvline(x.max(), c='r', lw=3)\n",
                "        ax.arrow(x.max()/1.6, y.max()/2, x.max()/5, 0, color='r', head_width=.25)\n",
                "    if cur_x \u003c x.min():\n",
                "        ax.axvline(x.min(), c='r', lw=3)\n",
                "        ax.arrow(x.max()/2.5, y.max()/2, -x.max()/5, 0, color='r', head_width=.25)\n",
                "    ax.set_xlim(x.min(), x.max())\n",
                "    ax.set_ylim(-3.5, 3.5)\n",
                "    ax.set_title(title)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Set a learning rate between 1 and 0.001. Then fill in the blanks to:\n",
                "1. Find the derivative, `delta`, of `f(x)` where `x = cur_x`\n",
                "2. Update the current value of x, `cur_x`\n",
                "3. Create the boolean expression `has_converged` that ends the algorithm if `True` \n",
                "\n",
                "You can experiment with how different values for `learning_rate` and `max_iter` affect your results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "### edTest(test_convergence) ###\n",
                "converged = False\n",
                "max_iter = __\n",
                "# Play with figsize=(25,20) to make more visible plots\n",
                "fig, axs = plt.subplots(max_iter//5,5, figsize=(25,20), sharey=True)\n",
                "\n",
                "cur_x = 0.75 # initial value of x\n",
                "\n",
                "learning_rate = __ # controls how large our update steps are\n",
                "epsilon = 0.0025 # minimum update magnitude\n",
                "\n",
                "for i, ax in enumerate(axs.ravel()):\n",
                "    plot_it(cur_x, title=f\"{i} step{'' if i == 1 else 's'}\", ax=ax)\n",
                "    \n",
                "    prev_x = cur_x # remember what x was\n",
                "    delta = __ # find derivative (Hint: use der_f())\n",
                "    cur_x = __ # update current x-value (Hint: use learning_rate \u0026 delta)\n",
                "    \n",
                "    # stop algorithm if we've converged\n",
                "    # boolean expression (Hint: last update size \u0026 epsilon)\n",
                "    has_converged = __  \n",
                "    if has_converged:\n",
                "        converged = True\n",
                "        # hide unused subplots\n",
                "        for ax in axs.ravel()[i+1:]:\n",
                "            ax.axis('off') \n",
                "        break\n",
                "plt.tight_layout()\n",
                "\n",
                "if not converged:\n",
                "    print('Did not converge!')\n",
                "else:\n",
                "    converged = True\n",
                "    print('Converged to a local minimum!')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Did you get $x$ to converge to a local minimum? Did it converge at all? If not, how would you describe the problem? What might we do to address this issue?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Mindchow üç≤\n",
                "\n",
                "Why does the algorithm not find the first local minimum which is actually the global minimum?\n",
                "\n",
                "Try changing the initial value of `cur_x` to see if you can have the function converge to this global minimum"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "your answer here"
            ]
        }
    ]
}
